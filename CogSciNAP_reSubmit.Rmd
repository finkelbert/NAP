---
title             : "The double life of language systems: Modelling sonority with complementary symbol and signal based models"
shorttitle        : "Modelling sonority with complementary models"

author: 
  - name          : "Anonymous"

affiliation:
  - institution   : "Affiliation"

authornote: |
  Author Note: In accordance with the Peer Reviewers' Openness Initiative (opennessinitiative.org), all materials and scripts associated with this manuscript were made available during the review process and will remain available (see anonymized link: https://tinyurl.com/5jxmjntx).

abstract: |
  *Sonority* is a fundamental notion in phonetics and phonology, central to many descriptions of the syllable and predictions in phonotactics.
  Although widely-accepted, sonority lacks a clear basis in speech articulation or perception, given that traditional formal principles in linguistic theory are often exclusively based on discrete units in symbolic representation, not commonly designed to be compatible with auditory perception, sensorimotor control, or general cognitive capacities. 
  On top of that, traditional sonority principles also exhibit systematic gaps in empirical coverage.
    Against this backdrop, we propose an incorporation of symbol-based and signal-based models to adequately account for sonority in a complementary manner.
  We claim that sonority is primarily a perceptual phenomenon related to pitch, driving the optimization of syllables as pitch-bearing units. We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we provide a novel principle that can account for syllabic well-formedness, the *Nucleus Attraction Principle* (NAP).
  We present perception experiments that test our two NAP-based models against four traditional sonority models and we use a Bayesian data analysis approach to test and compare them. Our symbolic NAP model outperforms all the other models we test, while our continuous bottom-up NAP model manages to reach the second place, along with the best traditional models.
  We interpret the results as providing strong support for our proposals:
  (i) the designation of periodic energy as sonority's correlate; 
  (ii) the incorporation of continuous entities in phonological models of perception; and 
  (iii) the dual-model strategy that separates and integrates symbol-based top-down processes and signal-based bottom-up processes in perception.

keywords          : "Sonority; Periodic energy; Bayesian data analysis; Phonetics and Phonology"
wordcount         : "X"

bibliography      : ["bibs/r-references.bib", "bibs/methods.bib", "bibs/phon.bib", "bibs/phon_sk.bib"]
appendix:
  - "./appendix_a.Rmd"
  - "./appendix_b.Rmd"
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : yes

documentclass     : "apa6"
classoption       : "doc"
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex
    includes:
      in_header: load.tex
keep_tex: yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r knitush, cache=FALSE,include=FALSE}
# global chunk options
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE,fig.path='figure/graphics-', fig.align='center')#, dev="cairo_pdf")
```

```{r libraries, message = FALSE}
library(R.matlab)
library(ggplot2)
library(dplyr)
library(Cairo)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores =  parallel::detectCores())
library(stringr)
library(readr)
library(purrr)
library(tidyr)
library(loo)
library(brms)
library(ggrepel)
```


```{r prepare-mat-per, include=FALSE}
# 60 x length of the audio file binned each 10 ms; 60 frequency bins with 10 ms for each column 
dir_mats <- c("data_tables/APPd_txt_matrices/AA/",
              "data_tables/APPd_txt_matrices/HN/")
files_mat <- list.files(path=dir_mats, pattern="*.txt",full.names=TRUE)

#creates a dataframe with 3 columns, the name of the syllable, syl, the time point, t, and the periodic energy, p
per_df <- map_dfr(files_mat, function(f){
#this is like a loop, it takes each file inside the f and does the following:
    # Read the file
    mat <- read_csv(f,col_names = FALSE,
                    col_types = cols(.default = col_double()))
    #Sum of every column of the matrix
   # vector_weights <- c(rep(1,8),rep(1.1,2),rep(1.3,3),rep(1.4,4),rep(1.5,5), rep(1.4,3), rep(1.3,5),rep(1.2,6),rep(1.1,10),rep(1,15))
    #mat <- mat * vector_weights
    per <- colSums(mat)
    #Extract the name of the syllable form the filename: looks for //(syllable).
    filename <- str_match(f,"//(.*?)_(.*?)\\.")
    syl <- filename[,2] 
    speaker <- filename[,3] 
    tibble(syl=syl,t=(0:(length(per)-1))*10,per=per,speaker=speaker)
})
# Periodic energy at every time point
per_df_full <-  per_df %>%  group_by(syl,speaker) %>% 
                mutate(smooth_per = unclass(smooth(per,"3RS3R"))) 
#head(per_df_full)

# ```
# 
# ```{r prepare-seg, include=FALSE}

dir_seg <- c("data_tables/praat_seg/AA/",
             "data_tables/praat_seg/HN/")

files_praat <- list.files(path=dir_seg, pattern="*.txt",full.names=T)
seg_df <- map_dfr(files_praat, function(f){  
#this is like a loop, it takes each file inside the f and does the following:

    # filename <- str_match(f,"/([^/]*?)_(.*?)_(.*?)\\.")
    filename <- str_match(f,"//(.*?)_(.*?)_(.*?)\\.")
    seg <- read_tsv(f, col_types =cols(
                          rowLabel = col_character(),
                          tmin = col_double(),
                          text = col_double(),
                          tmax = col_double()
                      )) %>% select(-rowLabel) %>%
        mutate(syl = filename[,2],
               speaker = filename[,3],
               text = str_extract_all(syl, ".")[[1]],
               position=row_number(),
               t = map2(tmin,tmax, ~ round(seq(.x,.y,.01)*1000))               ) %>%
        tidyr::unnest(cols = c(t)) %>%
        select(-tmin, -tmax)
})

syl_info <- left_join(per_df_full,seg_df,by = c("syl", "t", "speaker"))

## head(syl_info)

## syl       t   per speaker smooth_per text  position
## <chr> <dbl> <dbl> <chr>        <dbl> <chr>    <int>
## 1 cefal     0  0    AA            0    c            1
## 2 cefal    10  2.37 AA            2.37 c            1
## 3 cefal    20  4.97 AA            4.97 c            1
## 4 cefal    30  5.80 AA            4.99 c            1
## 5 cefal    40  4.99 AA            4.99 c            1
## 6 cefal    50  4.06 AA            4.06 c            1
# ```
# 
# ```{r log-transform, include=FALSE}

subset_voiceless_thresh <- filter(syl_info,
                                  #nchar(as.character(syl))>=4,
                                  position==1,
                                  syl %in% c("cfal","cpal","fsal","ftal","sfal","spal"))
per_thresh <- max(subset_voiceless_thresh$per)

syl_info <- group_by(syl_info,syl,speaker) %>%
    mutate(log_per = ifelse(smooth_per<per_thresh, 0,
                            10*log10(smooth_per/per_thresh)))

## print(syl_info,n=100)

# ```
# 
# ```{r, cog}

syl_info <- syl_info %>% group_by(syl, speaker) %>%
    mutate(com_syl = sum(log_per*t)/sum(log_per), # position of CoM of the whole syllable in time
           t_left_syl = ifelse(t <= com_syl,t,0),
           com_onset = sum(log_per*t_left_syl)/sum(log_per*(t_left_syl>0)),
           NAP_bu = -(com_syl - com_onset), #flipped sign
           NAP_bu_rel = -(com_syl - com_onset)/com_syl) %>%
    select(-t_left_syl)%>%
    select(NAP_bu, everything())
# ```
# 
# ```{r, monosyl, include=FALSE}

monosyl_info_AA <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "AA")

monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "ʃ"
# monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "U+0283"
# monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "\textesh"


monosyl_info_AA$syl <- as.factor(monosyl_info_AA$syl)
monosyl_info_AA <- mutate(group_by(monosyl_info_AA,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info_AA$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info_AA$smog_per[(monosyl_info_AA$smog_per<0)]=0

monosyl_info_AA <- mutate(group_by(monosyl_info_AA, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info_AA <- mutate(group_by(monosyl_info_AA, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```

```{r monosylHN, include=FALSE}

monosyl_info_HN <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "HN")

monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "ʃ"
# monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "U+0283"
# monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "\textesh"


monosyl_info_HN$syl <- as.factor(monosyl_info_HN$syl)
monosyl_info_HN <- mutate(group_by(monosyl_info_HN,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info_HN$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info_HN$smog_per[(monosyl_info_HN$smog_per<0)]=0

monosyl_info_HN <- mutate(group_by(monosyl_info_HN, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info_HN <- mutate(group_by(monosyl_info_HN, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```


<!-- ## Data analysis {#sec:datanlysis} -->

```{r setZeroRT, include=FALSE, message = FALSE}
syl_t <- filter(syl_info) %>%
  # Aviad: no idea why we are subsetting by "a"
    filter(text=="a" ) %>%
    group_by(NAP_bu, syl, speaker) %>%
    summarize(tmin = min(t))

other_scores <- read_tsv(file="data_tables/CCal_model_predictions_fix.tsv", 
                           col_types = cols(
                               syl = col_character(),
                               SSP = col_integer(),
                               SSP_obs = col_integer(),
                               MSD = col_integer(),
                               MSD_obs = col_integer(),
                               NAP_td = col_integer()
                           ))

scores_full <- left_join(syl_t, other_scores) %>% ungroup() %>%
    mutate(NAP_bu = ifelse(nchar(syl)<5,NAP_bu,NA_real_))
```

```{r openSesameFunction, include=FALSE}

read_list_opensesame <- function(list_files, speaker = "AA", scores_tbl= scores_full){

    GLIDES <- c("wlal","wnal","wzal","wsal","wtal","jmal","jval","jfal","jpal",               "welal","wenal","wezal","wesal","wetal","jemal","jeval","jefal","jepal")

    scores_speaker <- scores_tbl[scores_tbl$speaker == speaker,]

    map_dfr(list_files, ~{
        message(.x)
       suppressMessages (read_csv(.x)) %>%
            filter(practice =="no") %>%
           mutate(subj = str_match(logfile, '-([0-9]*)\\.csv')[,2],
                   RT = response_time /1000) %>%
            select(subj, stimulus,RT,correct) })%>%
        mutate(stimulus = str_replace_all(stimulus, 'ʃ','c')) %>% # ʃ
        filter(!stimulus %in% GLIDES) %>% left_join(scores_speaker,by=c("stimulus"="syl")) %>%
        mutate(corrRT = (RT - tmin/1000) * 1000, #in milliseconds
               type = factor(ifelse(nchar(stimulus)==4,"CCal",
                             ifelse(str_detect(stimulus,"^e.*" ),"eCCal","CeCal")),
                             levels=c("CeCal","eCCal","CCal")),
               response = case_when(RT>=3 ~ 0,
                                    correct ==1 & type == "CCal"  | correct ==0 & type != "CCal" ~ 1,
                                    TRUE ~ 2))
}
```

```{r readPilot, include=FALSE}

opensesame_pilot <- "data_tables/exploratry_results"

files_pilot <-
    c(list.files(path=paste0(opensesame_pilot,"/list1"), pattern="*.csv",full.names = TRUE),
      list.files(path=paste0(opensesame_pilot,"/list2"), pattern="*.csv",full.names = TRUE))

data_pilot_all <- read_list_opensesame(files_pilot, speaker="AA")

data_pilot <- data_pilot_all %>%
    filter(corrRT > 100)

N_below_100_pilot <- data_pilot_all %>%
    filter(corrRT < 100)
#0

# Sanity checks
N_trials_pilot <- 58
all(summarize(group_by(data_pilot,subj),N=n()) %>% pull(N)== N_trials_pilot)
summarize(group_by(data_pilot,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_pilot %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyPilot <- summarize(group_by(data_pilot,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyPilot %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyPilot %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)
```

```{r brmsPilot, include=FALSE}
library(brms)
run_brms <- function(data, chains = 4, iter = 3000, warmup=1000){
    data <- data %>% filter(type=="CCal", response ==1) %>%
        mutate_at(c("SSP","SSP_obs","MSD","MSD_obs","NAP_td"), ~ factor(., ordered = TRUE)) %>%
        mutate(sNAP_bu = NAP_bu -.5)
null_priors <- c(prior(normal(6, 2), class = Intercept),
                         prior(normal(.5, .2), class = sigma))
effect_priors <-  c(null_priors, prior(normal(0,1), class = b),
                    prior(normal(0,1), class = sd),
                    prior(lkj(2), class = cor))

    message("NAP_bu...")
    NAP_bu <- brm(corrRT ~ 1 +  sNAP_bu + (NAP_bu|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
               chains =chains, iter =iter, warmup = warmup)

    message("null...")
    null <- brm(corrRT ~ 1 + (1|subj), data=data,
               prior = null_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
    chains =chains, iter =iter, warmup = warmup)

message("SSP...")
    SSP <- brm(corrRT ~ 1 +  mo(SSP) +(mo(SSP)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("SSP_obs...")
    SSP_obs <- brm(corrRT ~ 1 +  mo(SSP_obs) +(mo(SSP_obs)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD...")
    MSD <- brm(corrRT ~ 1 +  mo(MSD) +(mo(MSD)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD_obs...")
    MSD_obs <- brm(corrRT ~ 1 +  mo(MSD_obs) +(mo(MSD_obs)|subj), data=data,
                   prior = effect_priors,
                   family =  lognormal(), 
               control = list(adapt_delta=.9995,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("NAP_td...")
    NAP_td <- brm(corrRT ~ 1 +  mo(NAP_td) +(mo(NAP_td)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

list(data = data, models = list(NAP_bu = NAP_bu, null = null, SSP=SSP, SSP_obs =SSP_obs, MSD = MSD, MSD_obs =MSD_obs, NAP_td = NAP_td))
}
```

```{r mpilot, include=FALSE}

 if(file.exists("data_tables/RDS/m_pilot.RDS")){
     m_pilot <- readRDS("data_tables/RDS/m_pilot.RDS")
 } else {
     m_pilot <- run_brms(data_pilot)
     saveRDS(m_pilot, file = "data_tables/RDS/m_pilot.RDS")
 }
 if(file.exists("data_tables/RDS/kfold_pilot.RDS")){
     
     kfold_pilot <- readRDS("data_tables/RDS/kfold_pilot.RDS")
 } else {
     kfold_pilot <- map(m_pilot$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_pilot, file = "data_tables/RDS/kfold_pilot.RDS")
 }

## loo::compare(x=kfold_pilot)

```

<!-- real data -->

```{r readGer, include=FALSE}
opensesame_german <- "data_tables/confirmatory_results"

files_german <-
    list.files(path=opensesame_german,pattern="*.csv",full.names = TRUE)

data_german_all <- read_list_opensesame(files_german, speaker="AA")

data_german <- data_german_all %>%
    filter(corrRT > 100)

N_below_100 <- data_german_all %>%
    filter(corrRT < 100)

## Sanity checks
N_trials_german <- 232 
all(summarize(group_by(data_german,subj),N=n()) %>% pull(N)== N_trials_german)
summarize(group_by(data_german,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_german %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyGer <- summarize(group_by(data_german,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyGer %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyGer %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracyGer %>% filter(nchar==4)
bi_acc <- subj_accuracyGer %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracyGer %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# 1 bad subject

data_german <- data_german %>% filter(!subj %in% bad_subj)

```

```{r brms-models, include=FALSE}

if(file.exists("data_tables/RDS/m_german.RDS")){
    m_german <- readRDS("data_tables/RDS//m_german.RDS")
 } else {
     m_german <- run_brms(data_german, iter=4000, warmup=2000)
     saveRDS(m_german, file = "data_tables/RDS//m_german.RDS")
 }
# 
```

```{r loo-models, include=FALSE}
if(file.exists("data_tables/RDS/kfold_german.RDS")){
    kfold_german <- readRDS("data_tables/RDS/kfold_german.RDS")
} else {
     kfold_german <- map(m_german$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_german, file = "data_tables/RDS/kfold_german.RDS")
    }
```

<!-- ```{r weigths-models, include=FALSE} -->

<!-- ## ## loo::compare(x=loo_pilot) -->
<!-- ## loo::compare(x=kfold_german) -->
<!-- ## loo::compare(x=kfold_german[-1]) -->
<!-- ## loo::compare(x=kfold_german[-1][-6]) -->
<!-- ## loo::compare(x=kfold_german[c("MSD_obs","SSP_obs","null")]) -->


<!-- ## loo::loo_model_weights(x=loo_german) -->
<!-- ## compare(loo_german$NAP_td, loo_german$SSP_obs) -->
<!-- ## if(file.exists("data_tables/RDS/weights.RDS")){ -->
<!-- ##     weights <- readRDS("data_tables/RDS/weights.RDS") -->
<!-- ## } else { -->
<!--     ## weights <-loo_model_weights(,kfold_german$MSD_obs) -->

<!--         ## xx <- ll_matrix[,c(2,3,4,5,6)] -->
<!--     ## wxx <- loo::stacking_weights(xx) -->
<!--     ## names(wxx) <-  colnames(xx) -->

<!--     ## saveRDS(weights, file = "data_tables/RDS/weights.RDS") -->
<!-- ## } -->
<!-- ``` -->

```{r loo-proc, include=FALSE}

## w_a <- model_weights(m_german$models$MSD,
##                    m_german$models$MSD_obs,
##                    m_german$models$SSP,
##                   ##  m_german$models$SSP_obs,
##                   ## m_german$models$NAP_bu,
##                   ## m_german$models$NAP_td,
##                   ## m_german$models$null,
##                    weights = "loo")


data_german_s <- m_german$data


## loos <- loo_german %>% map_dfc( ~
##     .x$pointwise[,"elpd_loo"]
##     ) %>% {setNames(.,paste0("elpd_",colnames(.)))}

## data_german_s <-  data_german_s %>% bind_cols(loos)


## data_g_summary <- data_german_s %>%
##     group_by(stimulus, NAP_bu, NAP_td) %>%
##     summarize_at(vars(starts_with("elpd")), mean)


## loo_model_weights(loo_german)

predictions <- m_german$models %>% map(~
                     predict(.x,summary=FALSE) %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_german_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots, eval =TRUE, warning=FALSE, include=FALSE}
data_RT <- data_german_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```


```{r readHeb, include=FALSE}
opensesame_hebrew <- "data_tables/confirmatory_results_Heb"

files_hebrew <-
    list.files(path=opensesame_hebrew,pattern="*.csv",full.names = TRUE)

data_hebrew_all <- read_list_opensesame(files_hebrew, speaker="HN")

data_hebrew <- data_hebrew_all %>%
    filter(corrRT > 100)

N_below_100 <- data_hebrew_all %>%
    filter(corrRT < 100)

## Sanity checks

data_hebrew %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyHeb <- summarize(group_by(data_hebrew,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyHeb %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyHeb %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracyHeb %>% filter(nchar==4)
bi_acc <- subj_accuracyHeb %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracyHeb %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# NO bad subject!

data_hebrew <- data_hebrew %>% filter(!subj %in% bad_subj)
# saveRDS(data_hebrew, "data_hebrew.RDS")
```

```{r brms-models-heb, include=FALSE}

if(file.exists("data_tables/RDS/m_hebrew.RDS")){
    m_hebrew <- readRDS("data_tables/RDS//m_hebrew.RDS")
 } else {
     m_hebrew <- run_brms(data_hebrew, iter=4000, warmup=2000)
     saveRDS(m_hebrew, file = "data_tables/RDS//m_hebrew.RDS")
 }
# 
```

```{r loo-models-heb, include=FALSE}
if(file.exists("data_tables/RDS/kfold_hebrew.RDS")){
    kfold_hebrew <- readRDS("data_tables/RDS/kfold_hebrew.RDS")
} else {
     kfold_hebrew <- map(m_hebrew$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_hebrew, file = "data_tables/RDS/kfold_hebrew.RDS")
    }
```


```{r loo-proc_heb, include=FALSE}

data_hebrew_s <- m_hebrew$data


predictions_heb <- m_hebrew$models %>% map(~
                     predict(.x,summary=FALSE,ndraws = 2000)  %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_hebrew_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots_heb, eval =TRUE, warning=FALSE, include=FALSE}

data_RT_heb <- data_hebrew_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```

<!-- ```{r prepareParticipants, include=FALSE, fig.show="hide"}
require(ggplot2) -->

<!-- # READ dataframe Hebrew subjects -->
<!-- partHeb_dir_tables <- "data_tables/confirmatory_results_Heb/" -->
<!-- files <- list.files(path=partHeb_dir_tables, pattern="subject*") -->
<!-- subjects_dfHeb <- plyr::ldply(files, function(f){ -->
<!--   subject <- read.csv(paste0(partHeb_dir_tables,"//",f), nrows = 1, as.is = T) -->
<!--   subject <- subset(subject,select = c(subject_nr, subject_age, subject_gender, subject_handedness, subject_education, subject_hebrew_native, subject_other_native_lang, subject_language_other)) -->
<!--   subject -->
<!-- }) -->

<!-- #fix one missing input -->
<!-- subjects_dfHeb$subject_education[subjects_dfHeb$subject_education=="None"] <- 2 -->

<!-- #map numerals to choices -->
<!-- subjects_dfHeb <- mutate(group_by(subjects_dfHeb, subject_nr), -->
<!--                       subject_age = ifelse(subject_age==1, "40-59", ifelse(subject_age==2, "30-39", ifelse(subject_age==3, "19-29", "13-18"))), -->
<!--                       subject_gender = ifelse(subject_gender==2, "male", ifelse(subject_gender=="1", "female", "other")), -->
<!--                       subject_handedness = ifelse(subject_handedness==0, "left-handed", "right-handed"), -->
<!--                       subject_education = ifelse(subject_education==0, "graduate", ifelse(subject_education==1, "undergrad", ifelse(subject_education==2, "high-school", "elementry"))), -->
<!--                       subject_hebrew_native = ifelse(subject_hebrew_native==1, "multilingual", "monolingual") -->
<!--                       ) -->

<!-- #change to factors -->
<!-- subjects_dfHeb$subject_nr <- as.factor(subjects_dfHeb$subject_nr) -->
<!-- subjects_dfHeb$subject_age <- as.factor(subjects_dfHeb$subject_age) -->
<!-- subjects_dfHeb$subject_gender <- as.factor(subjects_dfHeb$subject_gender) -->
<!-- subjects_dfHeb$subject_handedness <- as.factor(subjects_dfHeb$subject_handedness) -->
<!-- subjects_dfHeb$subject_education <- as.factor(subjects_dfHeb$subject_education) -->
<!-- subjects_dfHeb$subject_hebrew_native <- as.factor(subjects_dfHeb$subject_hebrew_native) -->

<!-- ## Write the main data file -->
<!-- write.csv(subjects_dfHeb, "data_tables/subjects/subjects_heb.csv") -->

<!-- ``` -->

<!-- participantsStuff -->
```{r participantsRead, include=FALSE}

## pilot
subjects_pilot <- read.csv("data_tables/subjects/subjects_pilot.csv") %>% select(-X)

## Ger (main)
subjects_main <- read.csv("data_tables/subjects/subjects_main.csv") %>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

subjects_main$subject_education <- as.character(subjects_main$subject_education)
subjects_main$subject_education[subjects_main$subject_education=="undergrad"] <- "undergraduate"
subjects_main$subject_education <- as.factor(subjects_main$subject_education)

## Heb
subjects_heb <- read.csv("data_tables/subjects/subjects_heb.csv") %>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

subjects_heb$subject_education <- as.character(subjects_heb$subject_education)
subjects_heb$subject_education[subjects_heb$subject_education=="high-school"] <- "school"
subjects_heb$subject_education[subjects_heb$subject_education=="undergrad"] <- "undergraduate"
subjects_heb$subject_education <- as.factor(subjects_heb$subject_education)

```


# Introduction

<!-- ## tmp {-} -->
The following work models the contribution of *sonority* to phonology in a manner that attempts to be compatible with general auditory perception and cognition, as well as with linguistic theory. This is in contrast to many of the traditional formal principles in linguistic theory that do not tend to specify how they interact with general systems of human capacity, such as auditory perception or sensorimotor control. Traditional linguistic principles are mostly generalizations that explain linguistic typologies as they have been depicted in symbolic writing systems. 

The prevailing sonority-related principles such as the *Sonority Sequencing Principle* (SSP) and its derivatives are, indeed, generalizations of the latter type.
Our novel proposals in this study achieve a better empirical coverage compared to different versions of the SSP that we test in a set of perception tasks, while, at the same time, provide a more comprehensive explanation than common formal linguistic principles to the notion of sonority in phonology and phonetics.

We assume here that a proper model of speech perception involves a bottom-up route and a related, yet separate, top-down route. While both inference routes are able to select between discrete alternatives as the goal of their process (e.g., select different consonants or syllabic parses), they arrive there in two very different ways: bottom-up processes are based on continuous events and top-down processes are based on existing sets of discrete symbolic entities. In that sense, top-down models are more reminiscent of traditional formal principles in phonology, that, more often than not, cover processes that start and end with discrete symbols.

As two complementary inference routes, the top-down and bottom-up modes should not be considered equal. The bottom-up route is the source of learned linguistic distinctions, it is functionally motivated by the laws of physics and the limitations of the perceptual and cognitive systems of the communicating agents. In contrast, the top-down route is based on linguistic experience and superficial inferences that reflect the history of the symbols in the system (i.e., the distributional probabilities of recurring patterns). In other words, top-down inferences reflect functionally motivated behaviors only indirectly, as the outcome of learning the superficial expressions of functionally-motivated (bottom-up) dynamics.

Sonority in our account is strongly related to pitch perception and the major role of pitch in  all language systems, whether it's lexical *tone* in tone languages or the post-lexical *tunes* of intonation systems in all the known human languages. We claim that sonority is related to the strength and quality of pitch perception, serving as a measure of *pitch intelligibility* that acts as a universal drive to optimize the pitch-bearing ability of syllabic units (see Section \@ref(sec:sonPitch)). 

We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we propose a novel principle that accounts for syllabic well-formedness based on general principles of competition in real-time, the *Nucleus Attraction Principle* (NAP). We implement NAP with two complementary models (see Section \@ref(sec:modelimp)): (i) a bottom-up model that directly analyzes continuous acoustic signals; and (ii) a top-down model that is based on discrete segments such as consonants and vowels. 

We present a series of syllable count tasks in Section \@ref(sec:experiments), 
designed in order to test our two NAP-based models (applying NAP with bottom-up and top-down approaches) against four traditional sonority models, considering two types of common sonority hierarchies combined with two types of common sonority principles---the *Sonority Sequencing Principle* (SSP) and the *Minimum Sonority Distance* (MSD).

We use a Bayesian data analysis approach to test and compare the six different sonority models. Whereas all the different models are found to be capable of predicting the experimental results to a good extent, the symbolic top-down version of NAP is shown to be the superior model. The bottom-up model of NAP comes in second alongside a few of the traditional models. Interestingly, some of the results suggest a relatively high degree of complementarity between the two NAP models, even though they represent the same principle. This is a desirable result for our framework, which advocates the need for two complementary models, both dynamic and symbolic.

Our set of proposals has many advantages over traditional sonority accounts, including methodological aspects, theoretical perspectives, and, essentially, a better empirical coverage. In Subsection \@ref(sec:discussionResults) we interpret the results of the  confirmatory experiments and in Section \@ref(sec:genDiscussion) we address some relevant implications, where we discuss the division of labor between sonority and other phonotactic factors, demonstrated with a holistic account of the phenomenon of /s/-stop clusters (Subsection \@ref(sec:division)). 
We also discuss the contribution of this work with relation to previous theoretical efforts to incorporate continuous entities in phonology (Subsection \@ref(sec:lingMod)).
Finally, we also discuss the debate about the universality of sonority in light of our work (Subsection \@ref(sec:projection)), before we present our conclusions in Section \@ref(sec:conclusions).

In the remainder of this Introduction, we briefly present the relevant background on traditional sonority hierarchies and principles, to cover the basics of their rationale and common application.
<!-- , and on the tension between discrete and symbolic entities on the one hand, and continous dynamic events on the other hand, in liguistic models of phonology (Subsection \@ref(sec:lingMod)). -->

<!-- emphasizing their rationale, their application, and their inherent flaws (Subsections \@ref(sec:hierarchies)-\@ref(sec:failures)).  -->

## Sonority Background {#sec:sonback}

### Sonority hierarchies {#sec:hierarchies}

A sonority hierarchy is a single scale on which all consonants and vowels can be ranked relative to each other. Early versions of current sonority hierarchies often date back to @sievers1893grundzugesk; @jespersen1899fonetik; and @whitney1865relation, while @ohala1992alternatives even goes as far back as @debrosses1765traite.

While the phonetic basis of sonority hierarchies remains controversial, phonological sonority hierarchies have been primarily based on repeated observations that revealed systematic behaviors of segmental distribution and syllabic organization within and across languages. The general consensus regarding the phonological sonority hierarchy thus stems from attested cross-linguistic phonotactic behaviors of different segmental classes, such as, for instance, the relatively high frequency of stop-liquid sequences in the onset of complex syllables (e.g., /kl/ in the English word ***cl**ean*) and the rarity of the opposite liquid-stop sequences at such onset positions, but not at the mirror-image coda position of syllables (e.g., /lk/ in the English word *mi**lk***). 

Although there are many different proposals for sonority hierarchies [@parker2002quantifying found more than 100 distinct sonority hierarchies in the literature], a very basic hierarchy that seems to reach a considerable consensus, and is often cited in relation to Clements's [-@clements1990role] seminal paper is given in (\@ref(ex:scale)).[^cf-liquid]

\begin{exe} 
\ex \emph{Obstruents} $<$ \emph{Nasals} $<$ \emph{Liquids} $<$ \emph{Glides} $<$ \emph{Vowels}  \label{ex:scale} 
\end{exe} 

The ordering of different speech sounds along the sonority hierarchy is assumed to be universal, in line with the common assumption that sonority has a phonetic basis in perception and/or articulation, yet the patterning of segmental classes as distinct groups along the scale is considered to be language-specific, i.e., based on phonological categorization. 
For example, voiceless stops may be considered universally lower than voiced fricatives on the sonority hierarchy, yet for some languages and analyses they may constitute a single level of *obstruents*. Classes along the sonority hierarchy are most commonly modeled as a series of integers (often referred to as sonority indices) reflecting the ordinal nature of phonological interpretations of the sonority hierarchy. 

(ref:hierarchy-caption) (\#tab:hierarchy) Traditional phonological sonority hierarchies
(ref:hierarchy-caption2) Index values reflect the ordinal ranking of categories in sonority hierarchies. The obstruents in *H~col~* are collapsed into one category (bottom four rows = 1), while in *H~exp~* they are expanded into four distinct levels.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:hierarchy-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
\multicolumn{2}{c}{\textbf{Sonority index values}} & \multicolumn{1}{l}{\textbf{Segmental classes}} & \multicolumn{1}{l}{\textbf{Phonemic examples}}\\
\multicolumn{1}{c}{\emph{H\textsubscript{col}} hierarchy} & \multicolumn{1}{c}{\emph{H\textsubscript{exp}} hierarchy} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}\\
\midrule
5 & 8 & Vowels & \multicolumn{1}{l}{/u, i, o, e, a/}\\
4 & 7 & Glides & \multicolumn{1}{l}{/w, j/}\\
3 & 6 & Liquids & \multicolumn{1}{l}{/l, r/}\\
2 & 5 & Nasals & \multicolumn{1}{l}{/m, n/}\\
\textbf{1} & \textbf{4} & Voiced Fricatives & \multicolumn{1}{l}{/v, z/}\\
\textbf{1}& \textbf{3} & Voiced Stops & \multicolumn{1}{l}{/b, d, g/}\\
\textbf{1}& \textbf{2} & Voiceless Fricatives & \multicolumn{1}{l}{/f, s/}\\
\textbf{1}&\textbf{1} & Voiceless Stops & \multicolumn{1}{l}{/p, t, k/}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:hierarchy-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}


The main differences that result from variation of the basic hierarchy in (\@ref(ex:scale)) concern the class of obstruents, which may contain voiced and voiceless variants of stops and fricatives (to mention just the most prominent distinctions).
Note that vowels are often also commonly divided into subgroups along the sonority hierarchy [see @gordon2012sonority], but these distinctions will be irrelevant in the context of this paper.
It is therefore not uncommon to expand the class of obstruents, whereby stops are lower than fricatives and voiceless consonants are lower than voiced ones. 

The two variants of the sonority index values given in Table \@ref(tab:hierarchy) thus reflect two ends of a spectrum of common sonority hierarchies, ranging from hierarchies that collapse all obstruents together into a single class (resulting in the same sonority index value for all obstruents), to hierarchies that expand the class of obstruents by employing voicing distinctions as well as distinctions between stops and fricatives (resulting in multiple sonority index values within obstruents). In what follows we will refer to these two versions of the sonority hierarchy as *H~col~* for the sonority hierarchy with *collapsed* obstruents, and *H~exp~* for the sonority hierarchy that exhibits an *expanded* class of obstruent.

### Sonority principles {#sec:principles}

Sequencing principles can be understood as a mapping scheme between the ranks of a sonority hierarchy and the linear order of symbolic speech segments.
Modern formulations of such principles, which use the ordinal sonority hierarchy to generalize over the phonotactics of consonantal sequences in terms of *sonority slopes* were developed mainly throughout the seventies and eighties of the twentieth century in seminal works such as @hooper1976introduction; @steriade1982greek; @selkirk1984majorsk; @harris1983syllablesk; @mohanan1986theory, and @clements1990role.
<!-- [^cf-sequencingclassics] -->

(ref:slopes-pl-lp) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *lpV*. The red line denotes the sonority slope of the onset cluster (i.e., the two onset consonants), while the grey line denotes the slope between the second consonant and the vowel at the nucleus position (always a rise in these cases). The angle of the red lines reflects the well-formed rising sonority slope of the onset cluster in *plV* and the ill-formed falling sonority slope of the onset cluster in *lpv*. Image taken from Anonymous (in press).
```{r slopes-pl-lp, fig.cap = "(ref:slopes-pl-lp)", fig.asp = .45, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","l","p","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority slopes: different types",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="well-formed",y=9.5,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset rise",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="ill-formed",y=9.5,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset fall",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8, "b",
        5,6, "c",
        6,1, "c",
        6,1, "d",
        7,8, "d") %>%
  slopes_plot()
```

The most basic and widely used sonority-based principle that derives phonotactic predictions in terms of syllabic well-formedness is the *Sonority Sequencing Principle* (SSP). The SSP is a simple yet powerful generalization about phonotactics that has been used in countless theoretical accounts. It identifies three distinct types of slopes---*rises*, *falls*, and *plateaus*---such that sequences of segments should rise in sonority from the consonant(s) in the syllabic onset to the syllable's nucleus (most often a vowel) and fall from the nucleus to the consonant(s) in the syllabic coda. 

In this paper we focus on syllable-initial onset consonant clusters that precede a vowel, whereby a rising sonority slope (e.g., *plV*) is considered well-formed, a falling sonority slope (e.g., *lpV*) is considered ill-formed, and sonority plateaus (e.g., *pkV*) fare in between, giving way to various interpretations, depending on the language and analysis, such that plateaus may pattern as ill- or well-formed [e.g., @blevins1995syllable], although they are generally interpreted as denoting a third mid-level of well-formedness, as we will treat them here.

The *Minimum Sonority Distance* [MSD; @steriade1982greek; @selkirk1984majorsk] is a well-known elaboration on the preferred angle of sonority slopes compared to basic applications of the SSP, given that the SSP makes no distinction between different angles of rising or falling slopes. The MSD was designed to prefer onset rises with steep slopes over onset rises with shallow slopes, under the assumption that consonantal sequences in the onset are preferred with a larger sonority distance between them. For instance, *plV* has a steeper rise compared to *bnV* and it is therefore better-formed according to the MSD (see Figure \@ref(fig:slopes-pl-bn)).[^cf-sdp] 

(ref:slopes-pl-bn) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *bnV* (the red solid line denotes the sonority slope of the onset clusters). The angle of the red lines reflects a steeper rise for *plV* (left) compared with *bnV* (right), due to the larger sonority distance between the consonants in *plV*. Image taken from Anonymous (in press).
```{r slopes-pl-bn, fig.cap = "(ref:slopes-pl-bn)", fig.asp = .4, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","b","n","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority rises: different slopes",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="steep rise",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="shallow rise",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8,"b",
        5,3,"c",
        6,5,"c",
        6,5,"d",
        7,8,"d") %>%
  slopes_plot()
```

#     Sonority, Pitch and the Nucleus Attraction Principle (NAP) {#sec:sonPitch}

##		Sonority and Pitch Intelligibility {#sec:pitchintelligibility}

The observation that sonority summarizes an essential quality that is related to vowels and their propensity to deliver a relatively steady harmonic structure, highlighting pitch and formant information, is by no means new. Previous proposals already defined sonority as either relating to vowels in some general way, or more specifically relating to voicing or vibration of the glottal folds, or to the clarity/strength of the formants.[^cf-list] A few previous accounts went even further, by addressing the function of this vowel-centric feature, suggesting that sonority may be related to periodic energy or pitch/tone [@lass1988phonology; @nathan1989preliminaries; @puppel1992sonority; @ladefoged1997linguistic; @heselwood1998unusual]. What all these proposals share, explicitly or implicitly, is a recurring insight about a strong link between the preferred type of segmental material in syllabic nuclei and a set of features that conspire to optimize pitch intelligibility, a property which characterizes vowels more than consonants. 

Pitch is an inseparable communicative dimension of all linguistic sound systems [@bolinger1978intonation; @house1990tonal; @cutler1997prosody; @roettger2019tune], whether it is lexically determined as in linguistic *tone*, 
or post-lexically employed to convey intonation, i.e., the linguistic *tune* [e.g., @pike1945intonationsk; @tHart1990perceptual; @jun2005prosodicsk; @jun2015prosodicsk]. 
Tones are used to distinguish lexical items while tunes are used to demarcate units as well as to modulate semantics (e.g., information structure and sentence modality) and to express a vast array of non-propositional meanings (e.g., discourse-pragmatic intention, emotional state, socio-indexical identity, and attitudinal stance). The importance of pitch to human communication cannot be overstated.

Crucially, linguistic pitch events are commonly assumed to target syllable-sized units as their "docking site", regardless of the type of pitch event, whether it is a lexical tone or a post-lexical tune [@lehiste1977suprasegmentals]. 
These linguistic pitch events associate with either syllables or *moras*.[^cf-mora] 
For example, intonation pitch contours that highlight and modulate whole words and phrases essentially target privileged syllables---stressed syllables and/or syllables at the edges of prosodic words and phrases---to achieve their communicative goal with texts of various sizes.
This tone-bearing role of syllables is the hallmark of many prominent theories regarding tone and intonation like Autosegmental and Autosegmental-Metrical Phonology [e.g., @liberman1975intonationalsk; @goldsmith1976autosegmental; @pierrehumbert1980phoneticssk; @ladd2008intonational].

The functionally motivated conclusion that emerges with respect to sonority is therefore that syllables require a pitch-bearing nucleus and sonority is a scalar measure of the ability to bear pitch. In other words, sonority is, most likely, a measure of *pitch intelligibility*. 
This hypothesis comes with an underlying assumption that syllables have followed an evolutionary trajectory that shaped them to optimally carry pitch in their nuclei. Sonority, according to this description, serves as the tool that governs the requirement for intelligible pitch as a fundamental characteristic in the design of the building blocks of prosody.

<!-- [^cf-perception] -->
It is important to note that this view of sonority is explicitly and exclusively based on perception, rather than articulation of speech. However, it does not exclude articulation-based description of syllables under the assumption that restrictions on syllabic structure must be derived from both perception and articulation of speech. A case in point is the *Articulatory Phonology* framework (see Section \@ref(sec:synthesis)), with its valuable descriptions of temporal coordination and phase relations between motor gestures, in manners that can be effectively linked to syllabic organization [see, e.g., @goldstein2007syllablesk; @goldstein2009coupled; @shaw2009syllabificationsk; @gafos2014stochastic; @hermes2017variabilitysk]. 

###     Pitch intelligibility and periodic energy {#sec:periodicenergy}

Pitch is a psychophysical phenomenon based on perception and cognition [see @plomp1976aspects; @plack2005psychophysics]. We can extract perception-related measurements from acoustics, i.e., not directly from the perceived sensation of a human subject but from the digitally-analyzed description of the physical sound in space. Using acoustics to cover auditory psychophysical phenomena is not a straight-forward task though. It requires a consistent and reliable association between acoustics on the one hand, and perception and cognition on the other hand.
This task is potentially complicated further with a complex phenomenon like pitch, that is evidently sensitive to various aspects of the rich acoustic signal [see, e.g., @houtsma1995pitch; @shepard2001pitch; @moore2013anintro 203], as well as to our top-down expectations with regards to learned regularities of pitch behavior in the speech signal [@mcpherson2018diversity]. 

Fortunately, there are strong links that hold between pitch and measurable acoustic markers. This is evident from the extensive use of acoustic F0 measurements to estimate perceived pitch height. 
Pitch estimations from F0 measurements is in fact quite reliable when dealing with specific types of audio such as speech, whereby the bulk of the pitch information is coming from a single source (i.e., one speaker) within a limited range of fundamental frequencies (mostly between 75--400 Hz, rarely below 50Hz or above 600Hz).

To estimate perceived pitch intelligibility from acoustic signals, we need to obtain a measure of *periodic energy*, which is a measurement of the acoustic power of the periodic components in the signal. It may be helpful to think of it as a measurement of general intensity that excludes the contribution of aperiodic noise and transient bursts.
Measurements of periodic energy are not much different from widely-used F0 measurements that are commonly based on abilities to detect periodic components in the complex signal. Roughly speaking, rather than resolving the harmonic denominator of detected periodic components in order to estimate F0, a periodic energy meter needs to sum over their power.

To conclude, our ability to detect periodicity in acoustic signals allows us to extract good estimates of F0 and periodic energy from speech data. We stand on firm grounds when we map these acoustic markers to perception in terms of pitch height and pitch intelligibility (respectively).
Given a causal link between perceived pitch intelligibility and linguistic sonority, we assume by transitivity that acoustic periodic energy maintains a causal link with the linguistically-loaded notion of sonority.  


##   The Problem of Intensity-Based Accounts {#sec:correlusions}
<!-- ##   Sonority "correlusions"  {#sec:correlusions} -->

Although no strong consensus ever formed with respect to the phonetic basis of sonority, acoustic *intensity* is perhaps the most widely assumed correlate of linguistic sonority. This can be evident in the many influential studies on sonority that consider acoustic intensity as its phonetic correlate [e.g., @sievers1893grundzugesk; @heffner1969generalsk; @ladefoged1975acourse; @clements1990role; @blevins1995syllable; @parker2008sound; and @gordon2012sonority, to name just a few prominent examples].[^cf-parkerIntensity]

[^cf-parkerIntensity]: In his overview of existing literature, @parker2002quantifying found close to a 100 different proposals for correlates of sonority in the literature, and he tested five leading proposals in laboratory conditions: *intensity*, *intraoral air pressure*, *F~1~ frequency*, *total air flow*, and *duration*. In his study, the tightest correlations with sonority classes were obtained for acoustic intensity measurements, a conclusion that was repeated and elaborated upon in @parker2008sound. 
<!-- Parker's conclusions about the role of acoustic intensity are very much in line with many influential studies in phonetics and phonology that also target acoustic intensity as the phonetic correlate of sonority [e.g., @sievers1893grundzugesk; @heffner1969generalsk; @ladefoged1975acourse; @clements1990role; @blevins1995syllable; and @gordon2012sonority, to name just a few prominent examples]. -->
<!-- [^cf-articultory]  -->

The main problem with intensity-based accounts is related to the distinction between *causation* and *correlation*. Establishing causation from acoustic signals necessitates a theory that can reliably link between acoustic markers and consistent operations or processes in sensorimotor speech articulation and/or auditory speech perception. The problem with accounts that are based on acoustic intensity is that the general acoustic intensity of the signal does not consistently map to any aspect of human auditory perception, not even perceived loudness, as detailed below. 

###  Acoustic intensity $≠$ perceived loudness {#sec:intensity}

The acoustic signal has certain physical qualities contributing to its overall power, but they have different effects on the perceptual system of the human hearer. This discrepancy between acoustic intensity and perceived loudness is a well-known problem, playing a role at different dimensions of the mapping between acoustics and perception.
The prominent points of departure between acoustic intensity and perceived loudness include different frequency bands that lead to different loudness perception [e.g., @fletcher1933loudness; @plack1995loudness; @suzuki2004equal], different signal durations that have different loudness effects [e.g., @turk1996processing; @olsen2010loudness; @moore2013anintro 143], and different periodic structures, where the difference between harmonic structures vs. random noise, and their respective spectral bandwidths, was also found to influence perception of loudness [e.g., @hellman1972asymmetry; @stevens1972perceivedsk; @bao2010psychoacousticsk].

Acoustic intensity is therefore a physical description of sound waves in space which does not consistently relate to how loud we perceive them, or to any other perceptual phenomenon for that matter.

###  Loudness is not a good candidate for sonority {#sec:loudness}

Note also that the relevance of perceived loudness to syllabic organization requires some sort of functional explanation, which seems to be lacking. 
The systematic differences in intensity of adjacent speech sounds imply that these differences are neutralized in perception, as it should make sense to assume that the different sounds that compose coherent speech are perceived as having comparable loudness. 
The literature on perceived loudness supports this assumption given that speech portions with relatively low acoustic intensity, like voiceless fricatives, appear in speech next to portions with relatively high acoustic intensity, like that of a vowel.
Our auditory system perceives the aperiodic high-mid frequencies of many obstruents as exceptionally loud compared to the periodic low-mid frequency ranges of vowel sounds, thus compensating in perception for physical differences in acoustic intensity.

Given the above, we should anticipate that perceived loudness will not be a good candidate for the acoustic correlate of sonority hierarchies, as a measure of perceived loudness would bring all speech sounds closer together by diminishing the distinctions provided by acoustic intensity. Indeed, although good approximations of perceived loudness from acoustic signals are available [e.g., @seshadri2009perceived; @skovenborg2012loudnesssk; @lund2014loudnesssk; @itu2015algorithmssk], we are unaware of attempts to employ such measures for sonority. 

Instead of attempts to map acoustic intensity with perception in terms of perceived loudness, most successful endeavors that use intensity-based measures as correlates of sonority do it by essentially enhancing the intensity-loudness discrepancy, targeting certain frequency bands to---roughly speaking---discriminate against energy at the higher frequencies, that are more characteristic of obstruents, in favor of energy at low-mid ranges of the spectrum, that are more characteristic of sonorants and particularly vowels [e.g., @Pfitzinger1996syllablesk; @port1996dynamicsk; @fant2000source; @galves2002sonoritysk; @wang2007robust; @tilsen2013speech; @patha2016syllablesk; @nakajima2017english; @rasanen2018pre]. 
The relative success of such metrics is not commonly motivated on perceptual grounds. However, they are often tightly linked to the perceptual quality that is identified with sonority in this work---the capacity to perceive pitch.


##      The Nucleus Attraction Principle {#sec:nap}

At the heart of all sonority-based principles lies the idea that the most sonorous segment in a sequence is contained within the nucleus of the syllable. This idea in fact postulates a link between the amount of sonority and the nucleus position of the syllable. We adopt this fundamental insight that guides all other sonority principles in the development of the Nucleus Attraction Principle, but instead of adding further formal assumptions about non-overlapping segments with fixed sonority values and corresponding sonority slopes in symbolic time, the link between sonority and the syllabic nucleus is conceived of as a dynamic process in real time, whereby all the portions of the speech signal compete against each other for available nuclei. 
<!-- (*available nuclei* can be thought of as the canonical syllables of words in the *mental lexicon*).  -->

Sonority is therefore the quality that is capable of *attracting* the nucleus. The varying quantities of this quality, which temporally fluctuate along the stream of speech, determine which portions of speech are prone to succeed in attracting nuclei given their superior local sonority *mass*. The speech portions that fall between those successful attractors are syllabified in the margins of syllables, at onset, and coda positions.[^cf-attraction]

[^cf-attraction]: This notion of prosodic *attraction* is, in fact, well-established in phonological theory, with descriptions of *weight sensitivity* in the stress systems of many unrelated languages, in which the stress is said to be attracted to heavy syllables.
Heaviness is mainly the product of a longer vowel in the nucleus, and in some languages heaviness may also result from a (preferably sonorant) consonant in the coda [e.g., @mccarthy1979formalsk; @hayes1980metrical; @prince1990quantitative; @gordon2006syllableweight]. There are also analyses whereby 
vowel qualities that are considered more sonorous can contribute to heaviness and attract the stress [@zec1995sonority; @zec2003prosodic; @kenstowicz1997quality; @delacy2002formal; @gordon2012sonority].
Viewed with NAP in mind, attraction of stress in weight sensitive systems is simply the special case of a regular procedure, whereby weight---i.e., sonority---attracts syllabic nuclei. 

Crucially, NAP treats the postulated link between sonority peaks and syllabic nuclei as the result of a perceptual-cognitive process in real time, rather than describing a formal geometric state of affairs with discrete symbolic machinery. 
By doing so, no further theoretical postulates like sonority slopes are required to determine well-formedness in NAP. 
The degree of nucleus competition that a given syllabic parse incurs is positively correlated with syllabic ill-formedness in NAP-based models.
<!-- By modelling the link between sonority and the syllabic nucleus in dynamic terms it is not necessary to add further theoretical postulates about sonority slopes, nor discrete segmental categories of consonants and vowels, to determine well-formedness of syllabic structures. Syllabic ill-formedness in NAP-based models is positively correlated with the degree of nucleus competition that a given syllabic parse incurs.  -->

It is important to note that the informativeness of NAP-based models is not derived from identifying the winner of the nucleus competition (which is always the only vowel in the structures under discussion), but from quantifying the degree of competition within different portions of speech that stand for potential syllabic parses. 
NAP-based models can analyze speech parts that are parsed together as a single syllabic unit in order to estimate the degree of competition they give rise to when they compete for a single nucleus. 
In discrete terms, NAP-based models can quantify different sequences of segments to reflect how strongly they compete for a single nucleus. 
The higher the degree of internal competition, the more worse-formed syllable is predicted to result from this parse.
To simplify this further with respect to the subset of instances discussed in this work (i.e., syllables with complex consonantal onset clusters), it is possible to say that
<!-- the winner of the nucleus competition is always the only vowel in the structure.  -->
the determination of ill-formedness in these cases is based on quantifying the amount of competition that the winning vowel has to withstand given different consonantal clusters in the onset of the same syllable.

It should be also useful to note that we do not expect serious competition to arise from a consonant adjacent to the vowel in the same syllable.
Nucleus competition, much like sonority slopes, has a limited impact on syllables with simple onsets and/or codas (e.g., CVC). Principles like SSP and NAP play a role chiefly when sequences of consonants are syllabified within a single syllable as complex onset or coda clusters (e.g., **C**CVC**C**). The phonotactics of these possible sequences are determined, to a large extent, by sonority principles. We interpret this aspect of cluster phonotactics such that sequences within syllables are avoided the more they increase the potential competition for the nucleus in the process of syllabifying/parsing the stream of speech.

###     Schematic NAP sketches {#sec:NAPsketch}

(ref:nap-depictions) Schematic depictions of competition scenarios with symbolic CCV structures. The nucleus competition can be understood as the competition between the blue and the purple areas under the sonority curve. The two examples at the top row---*plV* and *lpV*---suggest replication of successful traditional predictions, while the three examples at the bottom row---*spV*, *sfV* and *nmV*---suggest a divergence from inherent failures of SSP-type models (see text for more details). Image taken from Anonymous (in press).
```{r nap-depictions, fig.cap = "(ref:nap-depictions)", out.width = '100%', fig.align = 'center'}

knitr::include_graphics(rep(c("extrenal_figures/napComb150.png")))

```


To understand the rationale of NAP, a series of schematic sketches are presented in Figure \@ref(fig:nap-depictions), accompanied by an impressionistic description. These will eventually be implemented within formal models that are described in detail in Chapter \@ref(sec:modelimp).
The five examples with specified consonantal clusters exhibit their related sonorant energy depicted as the *area under the curve*, whereby the curve itself is an idealized depiction of schematic sonority.
The purple area in each syllable in Figure \@ref(fig:nap-depictions) denotes the sonorant energy of the winning vowel in the nucleus position while the blue area denotes the sonorant energy of the losing portions in the onset. 
Consider for example the pair *plV* and *lpV*, with schematic NAP-related depictions in the top row of Figure \@ref(fig:nap-depictions) (and with more traditional sonority slopes in Figure \@ref(fig:slopes-pl-lp)). A consonantal onset cluster with a putatively well-formed rising sonority slope like *plV* should be also considered well-formed under NAP due to the very low potential of competition between the marginal minimally-sonorous onset consonant /p/ and the non-adjacent vowel that wins the competition for the nucleus. The intervening /l/ in this case only promotes a continuous rise in sonority from /p/ to V. Likewise, a consonantal onset cluster with a putatively ill-formed falling sonority slope like *lpV* should be also considered ill-formed under NAP due to the strong potential for competition between the marginal sonorous onset consonant /l/ and the non-adjacent vowel, especially given intervening /p/ that leads to discontinuity in the sonority trajectory between /l/ and V.

Unlike the examples above, where the rationale of NAP is expected to replicate successful predictions of the SSP with cases like *plV* and *lpV*, NAP is also expected to diverge from traditional sonority sequencing principles in those cases where traditional principles 
consistently fail. Consider the examples in the bottom row of Figure \@ref(fig:nap-depictions).
<!-- , which were also depicted with traditional sonority slopes in Figures \@ref(fig:slopes-sp-sp)-\@ref(fig:slopes-nm-sf). -->
Under NAP, neither */s/-stop* clusters like *spV* nor voiceless obstruent plateaus like *sfV* are expected to incur a strong competition syllable-internally due to the low potential for competition between the minimally-sonorous onset consonant /s/ and the non-adjacent vowel that wins the competition (here, the intervening voiceless obstruents /p/ and /f/ retain a minimally sonorous trajectory throughout the whole onset).
At the same time, a strong degree of competition is predicted under NAP for nasal plateaus like *nmV* when compared to obstruent plateaus like *sfV*. This should be expected given the strong potential for competition between the marginal sonorous onset consonant /n/ and the non-adjacent winning vowel (here, the intervening nasal retains a relatively leveled sonorous trajectory throughout the onset).

The impressionistic descriptions of NAP, as provided in this section, are implemented within formal models that we describe in detail in Section \@ref(sec:modelimp).


#     NAP Implementations {#sec:modelimp}

##      Complementary NAP Models {#sec:complementary}

NAP essentially describes a bottom-up process, illustrating the parsing of the stream of speech into syllables as the end of a process that starts with continuous events in perception.
A bottom-up perspective on modelling NAP is therefore relatively straight-forward as it requires a similar approach to the process NAP describes: the analysis of continuous acoustic data at the input, which results in well-formedness predictions at the output.

A bottom-up approach for NAP models has no capacity to exploit the power of abstraction so it essentially has no "memory". It is a mechanistic dynamic model that contains discrete symbolic entities only at the (meta-)linguistic target of the task, at the end of the process that determines syllabic well-formedness.
This means that a bottom-up model can be only designed to analyze concrete speech tokens. Unlike traditional sonority principles and their models, a bottom-up model of NAP cannot determine the well-formedness of an abstract syllable as it is depicted in symbolic form, and it will give slightly different scores to different renditions of the same syllable, even by the same speaker.

A NAP-based model that operates on abstracted symbolic units is a top-down model that is taken to be a separate model, complementary to the model of the bottom-up route. Top-down inferences are based on learned regularities and categorical abstractions that reflect linguistic experience. To that end, knowledge about consonantal inventories and the probabilities of consonantal distribution with respect to position in the syllable has to be learned and stored in abstract symbolic forms which are available for top-down inferences. In that sense, top-down inferences in perception are based on the distributional probability of recognized symbols, given their history in the system.

The above description of top-down inferences, which are detached from the functional aspects of the bottom-up route, echo models of the language user as a *statistical learner* [see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing]
and, more specifically, they are very much in line with models of *phonotactic learners* [see, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @jarosz2017inputsk; @mayer2019phonotacticsk].

That said, the current project does not explore the statistical nature of top-down inferences. Instead, we operationalize the rationale behind NAP with symbolic machinery to present what can be understood as the symbolic model of NAP, which is used to estimate NAP-based top-down inferences. 
This choice allows the presentation of a top-down model with a stronger explanatory value with regards to NAP as it uses a similar architecture to that of standard sonority principles, helping to elucidate NAP's core ideas with familiar vocabulary (see Subsection \@ref(sec:naptdmodel)). 

Moreover, since a top-down model in this framework is based on the distributional patterns of recognizable symbols, these distributions are, in fact, "blind" to their various bottom-up sources, which include a host of universal and idiosyncratic phonotactic pressures. A true top-down statistical learner is thus inherently "contaminated" by all the different sources that contribute to phonotactics in a given system, without a clear distinction between sonority and other factors. Thus, it remains an open question whether top-down inferences that target only sonority-based phonotactics can be modeled in a more direct and principled way than the one presented here with the symbolic model of NAP.

Finally, the symbolic NAP model is necessary for the application of NAP in typological, diachronic, and many traditional and current studies, where speech data is transcribed into strings of discrete symbols (see Anonymous, in press). 

##     Model Implementations in Dynamic and Symbolic Terms {#sec:modelimpOLD}

In order to compare the different proposals, four types of traditional sonority models are considered alongside the two NAP models.
For traditional models we use the two types of sonority hierarchies that were presented in Subsection \@ref(sec:hierarchies), where the class of obstruents is either *collapsed* (*H~col~*) into a single level or *expanded* (*H~exp~*) to include distinctions between voiced and voiceless obstruents, and between stops and fricatives. 
Both hierarchies are applied with each of the two main variants of traditional sonority principles, the *Sonority Sequencing Principle*, SSP, and the *Minimum Sonority Distance*, MSD (see Subsection \@ref(sec:principles)). 
The four traditional sonority models under discussion are therefore a combination of a sonority principle (either SSP or MSD) and a sonority hierarchy (either *H~col~* or *H~exp~*). Accordingly, they bear the notation *SSP~col~* , *SSP~exp~*, *MSD~col~*, and *MSD~exp~*.

The two NAP models use periodic energy as the correlate of sonority, and they apply it either continuously with acoustics (bottom-up model), or in a discrete manner using symbols (top-down model). We refer to these two NAP models with the notation *NAP~td~* for the top-down model and *NAP~bu~* for the bottom-up one.

To test the different sonority models this study focuses on complex onset clusters of the general form CCV, where C denotes consonants in onset position and V denotes a vowel in nucleus position. Traditional sonority models look at the sonority slope of the onset cluster to determine well-formedness of CCV syllables, while NAP-based models apply the notion of *competition* to determine well-formedness.

In the following sections we elaborate on the methods for obtaining well-formedness scores, starting with the ordinal scores obtained from the four traditional sonority models (Subsection \@ref(sec:traditionalmodels)), and the symbolic NAP model, *NAP~td~* (Subsection \@ref(sec:naptdmodel)). 
The implementation of the continuous model, *NAP~bu~* follows in Subsection \@ref(sec:napbu). 
<!-- Finally, this chapter is concluded with a short overview of some of the key advantages of NAP over traditional sonority models (Section \@ref(sec:advantages)). -->

It is very important to note that the implementations of NAP in the following subsections present procedures for estimated results, based on the rationale of NAP. These procedures are not, in and of themselves, supposed to stand for any perceptual or cognitive mechanism, neither top-down nor bottom-up.

### Traditional sonority models {#sec:traditionalmodels}

Implementation of traditional sonority principles like the SSP is based on a calculation of the sonority slope with a given sequence of segments. Speech segments in these frameworks have fixed index values on the sonority hierarchy, based on their class membership, as in the *H~col~* and *H~exp~* hierarchies (see Table \@ref(tab:hierarchy)). These sonority index values are usually expressed in terms of integers since they reflect an ordinal scale, and, for this reason, the mathematical operations that these models employ should be restricted to basic arithmetic functions of addition and subtraction. Sonority slopes can be therefore obtained straight-forwardly by a subtraction between the corresponding sonority indices of two adjacent consonants. In onset clusters with two consonants (CCV) this can be simply achieved by the formula $C_2 – C_1$, which yields positive results for rising sonority slopes, negative results for falling sonority slopes, or a zero for plateaus. This is applied to the two SSP models, *SSP~col~* and *SSP~exp~* (see examples in Table \@ref(tab:ordinalscores)).

The same formula is also used to obtain scores for the Minimum Sonority Distance models, *MSD~col~* and *MSD~exp~*, which elaborate on the well-formedness of onset rises. 
With the MSD, higher positive scores are preferred over lower positive scores to reflect the preference for a larger sonority distance (or a steeper slope) in a rising onset configuration (see examples in Table \@ref(tab:ordinalscores)). 

### The top-down symbolic NAP model {#sec:naptdmodel}

The symbolic version of NAP, which is used to derive predictions for the top-down NAP (*NAP~td~*), shares a similar architecture with common SSP-based models, yet it reflects the novelties of the current proposal, both in terms of the sonority hierarchy it assumes, and in terms of the design of the sonority principle. *NAP~td~* uses a sonority hierarchy that is based on the periodic energy potential of different phoneme classes as the basis of distinct categorical patterning (see following Subsection \@ref(sec:snaphierarchy)). Furthermore, *NAP~td~* models syllabic well-formedness with the notion of nucleus competition in mind rather than the formal notion of sonority slopes, as in traditional SSP-type models (see Subsection \@ref(sec:snapimplementation)).

#### The sonority hierarchy in NAP~td~ {#sec:snaphierarchy}

The symbolic sonority hierarchy in NAP uses the basic ratio between periodic and aperiodic energy in the speech signal to divide all speech sounds into three distinct groups, reflecting the coarse, yet reliable differences in potential periodic energy mass of different abstract speech sound categories. To achieve that, we rely on the following set of general characteristics: (i) the main source of periodic energy in speech stems from the vocal fold vibration when voicing occurs; (ii) aperiodic energy in speech is mostly the result of the turbulent airflow resulting from articulatory friction (i.e., fricatives), and from articulatory closure in oral stops, which often result in transient bursts when released [see @rosen1992temporal].

Thus, the ratio between periodic and aperiodic components in speech sounds readily yields the following three distinct groups: (i) voiceless obstruents that consist of mostly aperiodic energy are the least sonorous type of speech sounds; (ii) sonorant consonants and vowels that consist of mostly periodic energy are the most sonorous type of speech sounds; and (iii) voiced obstruents that consist of both periodic and aperiodic energy belong in the middle of this 3-place scale (see \@ref(ex:napscale))

\begin{exe}
\ex \emph{Voiceless Obstruents} $<$ \emph{Voiced Obstruents} $<$ \emph{Sonorants}  \label{ex:napscale}
\end{exe}

A further distinction in NAP's sonority hierarchy is based on the general presence or absence of articulatory contact, which can be referred to as the distinction between *contoids* and *vocoids* [@pike1943phonetics].
A free and open vocal tract contributes to a potentially stronger and longer vocalic signal that can qualitatively enhance the potential periodic energy mass.
This distinction effectively separates the sonorants into *sonorant vocoids* (glides and vowels) and *sonorant contoids* (nasals and liquids).
<!-- [^cfrhotics]  -->
See Table \@ref(tab:napscale) for the full sonority hierarchy in the symbolic model of NAP.
<!-- [^cf-reconciliation] -->

<!-- [^cfrhotics]: Note that some rhotics, which are traditionally considered liquids, may in fact belong with the vocoid consonants (e.g., most of the English rhotics, especially in coda position). However, we can ignore this issue here since as rhotics are not included in this paper. -->

(ref:napscale-caption) (\#tab:napscale) The symbolic sonority hierarchy in *NAP~td~*
(ref:napscale-caption2) Index values reflect the ordinal ranking of categories in the sonority hierarchy. The distinctions between categories in the symbolic NAP hierarchy are based on the characteristic ratio between periodic and aperiodic energy, and on articulatory contact, both taken to reflect the potential of the periodic energy mass, i.e., the potential for nucleus attraction.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:napscale-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
Sonority index & \multicolumn{1}{c}{Segmental classes} & \multicolumn{1}{c}{Periodic:Aperiodic} & \multicolumn{1}{c}{Articulatory contact}\\
\midrule
4 & \textbf{Sonorant Vocoids} & \multicolumn{1}{c}{1:0} & $-$\\
 & (\emph{glides}, \emph{vowels}) &  & \\
3 & \textbf{Sonorant Contoids} & \multicolumn{1}{c}{1:0} & $+$\\
 & (\emph{nasals}, \emph{liquids}) &  & \\
2 & \textbf{Voiced Obstruents} & \multicolumn{1}{c}{1:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
1 & \textbf{Voiceless Obstruents} & \multicolumn{1}{c}{0:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:napscale-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The complete 4-place sonority hierarchy of *NAP~td~* in Table \@ref(tab:napscale) also reflects a basic typology of nucleus types, which supports the use of this scale as a qualitative measure for nucleus attraction potentials. Sonorant vocoids like glides and vowels can attract the nucleus in all languages we know (a glide is considered a vowel when syllabified in the nucleus position), while sonorant contoids like nasals or liquids can be syllabic (i.e., attract the nucleus) only in a subset of languages, of which a smaller subset may allow obstruents to attract nuclei [but see @easterday2019highly for some divergent patterns with syllabic obstruents relative to syllabic liquids].

#### NAP~td~ implementation {#sec:snapimplementation}

In order to account for C~1~C~2~V syllables with the NAP framework, we essentially want to measure the competition potential between C~1~ and V given C~2~. In and of itself, C~2~ should not be considered as a strong competitor due to its proximity to the vowel, as discussed in Subsection (\@ref(sec:nap)).
The issue of competition may be therefore expressed by the following questions:
(i) what is the potential periodic energy mass of C~1~ (i.e., how sonorous is C~1~, or what is the intercept of the cluster that determines the starting point of the slope);
(ii) how much of the energy in C~1~ is potentially lost, gained or maintained in C~2~, before peaking at the vowel (i.e., what is the sonority slope).
Assessing this relationship between C~1~ and V given C~2~ can be achieved by the combination of two subtraction formulas: 
(i) a calculation of the difference between C~1~ and the non-adjacent vowel, to reflect the potential strength of C~1~ in terms of the difference between the intercept and the nucleus; 
(ii) a calculation of the slope between adjacent C~1~ and C~2~, just like in the SSP-based models, to reflect the trend of the energy's trajectory towards the peak. 
This can be summarized with the formula in \@ref(eq:naptdeq), see examples in Table (\@ref(tab:ordinalscores)).
<!-- [^cf-angle] -->

\begin{equation}
  (V - C_1) + (C_2 - C_1)  \label{eq:naptdeq}
\end{equation}

###   Ordinal sonority scores {#sec:ordinalscores}

Table \@ref(tab:ordinalscores) demonstrates and compares the scores of the five ordinal models (2 $\times$ SSP, 2 $\times$ MSD and *NAP~td~*) with different CCV cluster types. It shows that the main difference between the two sonority hierarchies, *H~exp~* and *H~col~*, concerns fricative-stop clusters like the */s/-stop* cluster *spV*, which are considered as either an onset fall (with the *H~exp~* hierarchy) or an onset plateau (with the *H~col~* hierarchy). When the MSD is applied, the two sonority hierarchies also show differences in ranking within onset rises, given their different treatment of obstruents: in models that use the *H~exp~* hierarchy there are four levels of obstruents (voiced and voiceless stops and fricatives) which are collapsed into one level in models that use the *H~col~* hierarchy. 
This results in five distinct sonority rise scores in *MSD~exp~* model, but only two in the *MSD~col~* model, where also some of the trends differ (e.g., *smV* vs. *vlV* in the two MSD-based models).

(ref:ordinalscores-caption) (\#tab:ordinalscores) Ordinal sonority scores
(ref:ordinalscores-caption2) Well-formedness scores with ordinal models. The table demonstrates the predictions we obtain using the two traditional sonority hierarchies, *H~col~* and *H~exp~*, with each of the two traditional sonority principles, SSP and MSD. Numbers in brackets next to "Rise" reflect MSD's ranking of onset rises by distance---higher values indicate better-formed rises. The scores derived from *NAP~td~* on the right column are taken to directly reflect the nucleus competition potential, where higher scores are better-formed.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:ordinalscores-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Traditional sonority principles} & \multicolumn{1}{c}{Symbolic NAP}\\

\multicolumn{1}{l}{Onset} & \multicolumn{2}{c}{\emph{exp} hierarchy} & \multicolumn{2}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\

\multicolumn{1}{l}{clusters} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{exp}}}} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{col}}}} & \multicolumn{1}{c}{$(V-C_1)+(C_2-C_1)$}\\

\midrule
\multicolumn{1}{l}{\textbf{pl}V} & 6$-$1 $=$ 5 & \multicolumn{1}{c}{\textbf{Rise (5)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{fl}V} & 6$-$2 $=$ 4 & \multicolumn{1}{c}{\textbf{Rise (4)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{sm}V} & 5$-$2 $=$ 3 & \multicolumn{1}{c}{\textbf{Rise (3)}} & 2$-$1 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{vl}V} & 6$-$4 $=$ 2 & \multicolumn{1}{c}{\textbf{Rise (2)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$2)$+$(3$-$2) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{ml}V} & 6$-$5 $=$ 1 & \multicolumn{1}{c}{\textbf{Rise (1)}} & 3$-$2 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sf}V} & 2$-$2 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{zv}V} & 3$-$3 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$2)$+$(2$-$2) $=$ \textbf{2}}\\
\multicolumn{1}{l}{\textbf{nm}V} & 5$-$5 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 2$-$2 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sp}V} & 1$-$2 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{lm}V} & 5$-$6 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$3 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{mz}V} & 4$-$5 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{lv}V} & 4$-$6 $=$ $-$2 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$4 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{ms}V} & 2$-$5 $=$ $-$3 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{np}V} & 1$-$5 $=$ $-$4 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{lp}V} & 1$-$6 $=$ $-$5 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$3 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:ordinalscores-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

Unlike traditional models, the predictions of *NAP~td~* are not grouped into levels that reflect the rough angle of the sonority slope in terms of falls, rises and plateaus. The raw score of the *NAP~td~* formula is taken as reflective of the nucleus competition potential such that higher scores denote weaker competition and are thus better-formed. 
The top-down NAP model allows scores within a range that goes from -$\mathit{3}$ for the most ill-formed syllable up to $\mathit{6}$ for the most well-formed, although a more relevant range to consider, given that glides are excluded from this set, is between -$\mathit{1}$ and $\mathit{5}$. These scores are not immediately comparable to the traditional model scores, yet some interesting departures from the traditional models can be observed in Table \@ref(tab:ordinalscores). For example, *NAP~td~* considers the onset rise in the sonorous cluster *mlV* as ill-formed as the inverse fall *lmV*, and both clusters pattern in *NAP~td~* with nasal plateaus (e.g., *nmV*), where they all receive the same relatively low value of $\mathit{1}$. At the same time, voiceless clusters pattern in *NAP~td~* with well-formed combinations (scoring $\mathit{3}$) although they may include sonority plateaus (e.g., *sfV*) or sonority falls (e.g., *spV*) in traditional model terms.

###   The bottom-up NAP model {#sec:napbu}

There are various ways to calculate an estimation of the nucleus competition potential within syllables based on the periodic energy data of the acoustic signal. The method that we present here has the advantage of not relying on segmental landmarks that are categorical abstractions of the type that is assumed in the top-down model---they are not supposed to be available in the bottom-up route.

(ref:com-4examples-1) Smoothed periodic energy curve (black) of the four syllables from the experimental stimuli---*lpal*, *nmal*, *vlal*, and *smal*. Red vertical line denotes the center of periodic mass of the entire syllable (*CoM~syllable~*), blue vertical line denotes the center of periodic mass of the left portion (*CoM~onset~*). Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). The distance between the two CoM landmarks is indicative of the energy displacement away from the syllabic center, reflecting the nucleus competition potential within the syllable (see text for details).
```{r com-4examples-1, fig.cap = "(ref:com-4examples-1)", fig.width=7, fig.asp=.6, warning=FALSE, dev="cairo_pdf"}
# CoM_ons <-  expression(CoM[ons])# %>% as_label()
monosyl_examp_1 <- filter(monosyl_info_AA,
                          syl %in% c("smal","vlal","nmal","lpal"))
ordered_syl_abs <- unique(monosyl_examp_1[order(monosyl_examp_1$NAP_bu),]$syl)
monosyl_examp_1$syl <- factor(monosyl_examp_1$syl, levels=ordered_syl_abs)
monosyl_examp_1_plot <-
  ggplot(monosyl_examp_1, aes(x=t)) +
  xlim(0,475) + ylim(-3,19) + #ggtitle("") +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-1, yend=13), color="royalblue1", size=1.7, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-1, yend=13),color="red", size=1.7, alpha=.6, linetype = "solid", lineend = "round") +
  # geom_text(aes(x=com_onset, y=15, label=deparse(CoM_ons)), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=com_onset,y=15,label="CoM:onset"), nudge_x = -30, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=x_com_syl,y=15,label="CoM:syllable"), nudge_x = 30, color="red", alpha=.9, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-.75, yend=-.75), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.04, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2,y=-2.5,label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=19), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=18,label=text), size=6, family = "Charis SIL", check_overlap=T) + 
  #
  facet_wrap(~syl, ncol=2) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Charis SIL"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Charis SIL"), strip.text = element_blank())
print(monosyl_examp_1_plot)
```

The periodic energy data that is extracted from acoustic recordings of speech is viewed in terms of a *mass*, i.e., the area under the periodic energy curve, integrating duration and power as the two linked dimensions of quantity in sound [see @turk1996processing on interactions between duration and intensity in linguistic perception contexts]. 
<!-- This is using a summing strategy, as opposed to averaging or peak extraction, to represent the quantity of sonority. -->
<!-- Although less commonly used compared to averaging or peak detection [see meta-analysis in @gordon2017acoustic], summing was used in @beckman1986stress to measure stress in terms of *total amplitude*. A related measurement, *total perceptual energy* is presented in @gordon2004syllable (combining normalized RMS with duration) to also account for syllabic weight. -->
<!-- Summing takes measurements from the whole duration of a unit, allowing it to accumulate the individual measurements at each time point, and, in contrast to averaging, to consider the contribution of duration at the final calculation (rather than normalize over it). -->
Summing is therefore essentially different from averaging, as well as from peak extraction, in that it is capable of uncovering the quantitative difference between two periodic sounds that have similar amplitude envelopes yet they differ in duration.

The contribution of duration to sonority was convincingly illustrated in the seminal work of @price1980sonority, whereby the perception of disyllabic English words like *polite* /pəlʌɪt/ was shown to take place when the duration of the sonorant /l/ in the superficially related monosyllabic word *plight* /plʌɪt/ was manipulated, essentially leading to the perception of another syllable when the duration of the sonorant was increased [see also @berent2007we; @wilson2013bayesian; @wilson2014effects]. Importantly, the periodic energy mass, which is the integral of power and duration, is the only measurement of the three basic alternatives for continuous curve measurement---sum, average or peak---that is capable of accounting for the contribution of duration to perception of sonority.

It is therefore useful to locate the *center of mass* within regions of interest as a measurement that is sensitive to the two axes of the periodic energy mass---duration (x-axis) and power (y-axis). The center of mass can be viewed as the point in time in which the area under the curve is split into two equal parts. The location of the center of mass in time (x-axis) is attracted to the location of the peak of the curve (on the y-axis), where it is expected to be found with a perfectly symmetrical shape. However, the center of mass most often diverges from the peak of the main rise-fall fluctuations so as to reflect asymmetries in the overall distribution of the mass, either leftward or rightward. The center of mass of the periodic energy curve (henceforth CoM) follows a methodology that was introduced with the *Tonal Center of Gravity* [@barnes2012tonal] by calculating a weighted average time point that uses a continuous time series as the weighting term. The equation in \@ref(eq:com) is used to locate the average point in time (*t*), weighted by continuous periodic energy (*per*) at discrete time points:

\begin{equation}
  CoM = \frac{\sum_i per_i\cdot t_i}{\sum_i per_i}  \label{eq:com}
\end{equation}

<!-- \begin{equation} -->
<!-- \frac{\sum_i per_i\times t_i}{\sum_i per_i}  \label{eq:comasd} -->
<!-- \end{equation} -->

The location of the center of periodic energy mass of the entire syllable (henceforth *CoM~syllable~*) guides us to the point in time where the periodic mass of all the competing forces within that syllable are split into two equal parts. Once we obtain this reference point we can repeat this process within the resulting left-side portion, i.e., from the beginning of the syllable up to *CoM~syllable~*, to focus on the onset position (henceforth *CoM~onset~*).
We therefore measure the center of mass twice---first for the entire syllable (resulting in *CoM~syllable~*) and then for the left portion of the first measurement (resulting in *CoM~onset~*). 
The distance between *CoM~syllable~* and *CoM~onset~* is indicative of the amount of displacement of energy away from the center of the syllable, which is reflective of the degree of nucleus competition (see Figure \@ref(fig:com-4examples-1)).
<!-- [^cf-loess] -->

The center of mass is capable of capturing both components of a two-dimensional mass, considering the non-linear shape of the periodic energy curve. 
The leftward displacement of *CoM~onset~* relative to *CoM~syllable~* is affected by the distance, the amplitude, and the amount of discontinuity between the periodic energy at the onset and the center of mass of the entire syllable.
Any increase in the above results in a larger distance between the two centers of mass as Figure \@ref(fig:com-4examples-1) demonstrates.


#		Experimental Assessments {#sec:experiments}

In order to be able to assess NAP's predictions in situations where both bottom-up and top-down inferences contribute to incoming speech processing, an experimental procedure was designed to collect behavioral responses using a perception task. In what follows we present a series of perception experiments: (i) an exploratory pilot study with 12 German-speaking subjects; (ii) a confirmatory study with 51 German-speaking subjects; and (iii) a second confirmatory study with 33 Hebrew-speaking subjects (each group of subjects listened to stimuli of a native speaker from the other language to retain the effects of a foreign accent).
<!-- [^cf-helsinki]  -->

<!-- Note that all the media and original code that was used in this study are available at the following link on *Open Science Framework*: https://tinyurl.com/y3l4avap. -->
<!-- (anonymized link): https://tinyurl.com/y4gvobyp  -->

##		Rationale {#sec:rationale}

The goal of the experimental procedure is to tap into the cognitive cost that syllabification of different sequences incurs. To that end, we devised a forced-choice task that allows us to systematically compare response times of forced categorical decisions. Response times are linked with cognitive cost, which, in the context of the task, is understood as the result of syllabic ill-formedness. The working assumption is that worse-formed structures are harder to parse as a single syllable, which is reflected in a slower process altogether.

This design uses nonce words to test different consonantal combinations in comparable structures. This type of experimental design is reminiscent of many of the experiments on sonority effects that Iris Berent and her colleagues have been publishing, starting with the seminal @berent2007we.[^cf-berents] 
In their studies, the misperception and confusion between alternatives (e.g., *lbV*/*lə.bV*) is expected to be greater with worse-formed sonority clusters (e.g., greater confusion between *lbV*/*lə.bV* compared to *blV*/*bə.lV*), leading to a drop in categorical accuracy (e.g., a drop in correct identification of syllable number, or correct detection of similarity in a same/different task) accompanied by a gradual increase in response time.

[^cf-berents]: Examples of further publications by Berent et al. with various experimental settings that test sonority effects in perception with behavioral data include: @berent2008language; @berent2010phonological; @berent2011syllable; @berent2012language; @berent2013phnological; @tamasi2014sensitivity; @zhao2015universalsk; @lennertz2015onthesonority. The following examples include also neurological data: @berent2014languagesk; @gomez2014language; @berent2015role. 

In a NAP-based model we assume that syllabic well-formedness is tightly related to the nucleus competition between different portions of a syllable, such that response times will reflect the degree of nucleus competition within syllables (more competition = slower responses = worse-formed sequence). With traditional sonority models we interpret the processing cost as related to well-formedness in terms of sonority slopes, such that worse-formed clusters are more likely to be misperceived and take longer to process [e.g., @berent2007we; @berent2008language; @berent2009listeners; @berent2012language; @lennertz2010people; @maionchi2015sonoritysk; @sung2016perceptionsk; @young2017markednesssk]. 

To test the different predictions of the six sonority models (2$\times$NAP, 2$\times$SSP, 2$\times$MSD), we designed a perception task that prompts meta-linguistic syllable count judgement with 29 experimental target items. 
Participants were presented with a collection of speech items that were produced with one or two vowels, systematically for each combination of consonants in our set. 
Only the single-vowel productions were considered as targets, and an accurate response to our targets is always the monosyllabic option (note that the term "accuracy" is used here to describe participants' responses with respect to predictions).
By focusing on the response time of "correct" responses to the target words we, in fact, measure the time it took participants to decide that a given single-vowel stimulus is monosyllabic. 
We can therefore interpret the reaction times of monosyllabic responses to single-vowel targets as reflective of the processing cost of assigning one nucleus to a given target stimulus with one vowel.

As detailed below, the experiments were designed as syllable count tasks, i.e., a meta-linguistic forced-choice task. We assume that this design activates top-down inferences which urge subjects to think about possible syllables in their language. However, since we want to also tap into subjects' bottom-up inferences we present these nonce words as real words in another unknown language, uttered by a native speaker of that foreign language. For that matter, the German-speaking subjects listened to a stimuli set that was comprised of recordings of a native Hebrew-speaking speaker, AA (the first author). Likewise, the Hebrew-speaking subjects listened to a stimuli set that was comprised of recordings of a native German-speaking speaker, HN (a phonetically trained volunteer).

Since the bottom-up predictions of NAP are derived via measurements of acoustic signals of particular productions rather than from fixed symbolic predictions, the assumption that all things other than the controlled variable are equal in the experimental stimuli should hold also for a large degree of variation that occurs in natural speech. Thus, if a certain segment in one item is slightly longer, shorter, louder or softer than in other comparable tokens, bottom-up NAP is designed to directly account for this variation, while the other symbol-based ordinal models essentially assume that such variation is mostly negligible as it cannot survive the abstraction into phonemes. This allows us to opt for a slightly more ecologically valid experimental paradigm, by using natural speech recordings that were designed and selected to sound as similar as possible, rather than using synthesized speech, which would have allowed a higher degree of similarity between tokens.


##		Materials {#sec:materials}

The experimental design is focused on onset consonantal clusters with two members. These CC combinations are composed from a set of consonants with *coronal* and *labial* places of articulation to avoid articulatory effects that may arise from *homorganic* sequences (i.e., adjacent consonants that share the same place of articulation) by exploiting both directions of each combination---coronal-labial (back-to-front) and labial-coronal (front-to-back). The consonantal classes in this experiment include *stops*, *fricatives*, *nasals*, and *liquids* to reflect the main classes in traditional sonority hierarchies, with the exclusion of *glides*.
See Appendix \@ref(appendix:a) for a list of considerations and criteria that we used in constructing the experimental stimulus set, and see the full stimulus set in Table \@ref(tab:targetlist).

(ref:targetlist-caption) (\#tab:targetlist) Experimental stimulus set: Onset cluster types in the experiment
(ref:targetlist-caption2) cor = coronal; lab = labial; * = voicing disagreement between obstruents; ** = no labial liquid; *** = dorsal stop /k/ (see list in Appendix \@ref(appendix:a))
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:targetlist-caption)}
\begin{tabular}{cclcclcclcclcclcclcclcclccl}
\toprule
\multicolumn{1}{r}{\textbf{C1}} & \multicolumn{2}{c}{\textbf{Voiceless}} & \multicolumn{2}{c}{\textbf{Voiced}} & \multicolumn{2}{c}{\textbf{Nasals}} & \multicolumn{2}{c}{\textbf{Liquids}}\\
\multicolumn{1}{l}{\textbf{}} & \multicolumn{2}{c}{\textbf{Fricatives}} & \multicolumn{2}{c}{\textbf{Fricatives}} & & & &\\
\multicolumn{1}{l}{\textbf{C2}} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor}\\
\midrule
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sp}, \textbf{ʃp}} & \multicolumn{1}{c}{\textbf{ft}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{np}} & \multicolumn{1}{c}{\textbf{mt}} & \multicolumn{1}{c}{\textbf{lp}} & \multicolumn{1}{c}{\textbf{lk}***}\\
\multicolumn{1}{l}{\textbf{Stops}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sf}, \textbf{ʃf}} & \multicolumn{1}{c}{\textbf{fs}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{nf}} & \multicolumn{1}{c}{\textbf{ms}} & \multicolumn{1}{c}{\textbf{lf}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiced}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{zv}} & \multicolumn{1}{c}{\textbf{vz}} & \multicolumn{1}{c}{\textbf{nv}} & \multicolumn{1}{c}{\textbf{mz}} & \multicolumn{1}{c}{\textbf{lv}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Nasals}} & \multicolumn{1}{c}{\textbf{sm}, \textbf{ʃm}} & \multicolumn{1}{c}{\textbf{fn}} & \multicolumn{1}{c}{\textbf{zm}} & \multicolumn{1}{c}{\textbf{vn}} & \multicolumn{1}{c}{\textbf{nm}} & \multicolumn{1}{c}{\textbf{mn}} & \multicolumn{1}{c}{\textbf{lm}} & \multicolumn{1}{c}{**}\\
& & & & & & & &\\
& & & & & & & &\\
\multicolumn{1}{l}{\textbf{Liquids}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{fl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{vl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{**}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:targetlist-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

Table \@ref(tab:targetlist) presents 29 CC sequence types that reflect 16 different combinations of classes (16 unique cells in Table \@ref(tab:targetlist), excluding differences in place of articulation), of which 7-8 are considered onset falls, 3-4 are considered onset plateaus (11 total), and 5 are considered onset rises.[^cf-plateaufall] 

Of the 29 different clusters, only 3 clusters regularly occur in German words, /ʃp, ʃm, fl/, 6 clusters are attested to some degree in German loanwords, /sp, sf, sm, vl, zv, ml/ [@van2012sonority], and one cluster, /ʃf/, may be considered as similar to German licit clusters with a voiced obstruent following a voiceless one (i.e., /ʃv/ and /t͡sv/).
Thus, the experimental set contains 19 cluster types that are unattested in German words. These unattested CC types appear in 13 of the 16 unique combinations, excluding the three rising sonority clusters with a liquid in *C*~2~ (e.g., /fl, vl, ml/), that are all attested in German complex onsets to some degree (yet only marginally so in the case of /vl/ and /ml/).

Out of the 29 different cluster types in Table \@ref(tab:targetlist), more clusters occur regularly in Modern Hebrew [see @asherov2019syllablesk]. These include all of the eight sibilant-initial clusters, /sp, ʃp, sf ʃf, sm, ʃm, zm, zv/, and the liquid-second clusters, /fl, vl/. The voiceless cluster /ft/ and the /m/-initial clusters /ml, mn/ are marginally attested in Modern Hebrew [@asherov2019syllablesk 75, 86].
Thus, the experimental set contains 16 cluster types that are unattested in Hebrew words.
These unattested CC types appear in 12 of the 16 unique combinations in Table \@ref(tab:targetlist), excluding the three rising sonority clusters with a liquid in *C*~2~ (e.g., /fl, vl, ml/), and the *fricative*-*stop* clusters (including /ft/), although note that /ft/ and /ml/ are only marginally attested in Hebrew complex onsets.

The different CC sequences were embedded within a /CCal/ word-like frame, with a recurring *-al* rime. These /CCal/ tokens were produced with a single vowel, intended to yield monosyllabic items that resemble typical content words [i.e., prosodically heavier than a single light syllable; see, e.g., @demuth1996prosodic]. Two disyllabic counterparts were prepared for each CC type---one with an epenthetic vowel, /CəCal/, and another with a prothetic vowel, /əCCal/ (a more accurate annotation should be /(ʔ)əCCal/, given that the presence of an initial glottal stop was not controlled for).
Note that the schwa in this case is produced as a weak (unstressed) central vowel, not necessarily a schwa.
The entire word set therefore includes 29 single-vowel target types and 58 associated bi-vocalic filler types, adding up to 87 different word-like stimuli.

##		Predictions {#sec:predictions}

(ref:OrdinalTargetPreds-caption) (\#tab:OrdinalTargetPreds) Well-formedness scores for the 29 experimental items using the five ordinal models that are based on symbolic phonemes: SSP~col/exp~, MSD~col/exp~, and NAP~td~
(ref:OrdinalTargetPreds-caption2) Higher values predict better-formed onset clusters in an ordinal scale (i.e., magnitude of differences between values cannot be inferred from these models).
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:OrdinalTargetPreds-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{Onset cluster types} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{exp}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{exp}}}} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\
\midrule
\multicolumn{1}{l}{\textbf{fl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{4 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{sm}, \textbf{ʃm}, \textbf{fn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{3 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{vl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zm}, \textbf{vn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{ml}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sf}, \textbf{ʃf}, \textbf{fs}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zv}, \textbf{vz}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{2}\\

\multicolumn{1}{l}{\textbf{nm}, \textbf{mn}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sp}, \textbf{ʃp}, \textbf{ft}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{lm}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{mz}, \textbf{nv}, \textbf{lv}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{0}\\

\multicolumn{1}{l}{\textbf{ms}, \textbf{nf}, \textbf{np}, \textbf{mt}, \textbf{lf}, \textbf{lp}, \textbf{lk}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{-1}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:OrdinalTargetPreds-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The full set of predictions for the 29 experimental targets is presented for all the symbol-based ordinal models (2$\times$SSP, 2$\times$MSD, and *NAP~td~*) in Table \@ref(tab:OrdinalTargetPreds), and for the signal-based continuous model (*NAP~bu~*) in Figure \@ref(fig:com-monosyl). Note that the scores of *NAP~bu~* are presented on a continuous ratio scale, with specific predictions for each token and consequential intervals between scores. The scores in *NAP~bu~* are not a generalization, rather, they are extracted from the specific set of recordings we measured, and they are expected to vary to some extents when measuring different tokens. We present the *NAP~bu~* scores for the two sets of stimuli used in the experiments: (i) A set spoken by a native Hebrew speaker (first author; see Figure \@ref(fig:com-monosyl)) and a set spoken by a native German speaker who is a trained phonetician (see Figure \@ref(fig:com-monosylHeb)).

(ref:com-monosyl) AA set (Hebrew speaker). Well-formedness scores in the continuous *NAP~bu~* model shown in terms of the distance between the center of mass of the entire syllable, *CoM~syllable~* (red vertical lines), and the center of mass of the left portion, *CoM~onset~* (blue vertical lines). Periodic energy is represented by the black curve. Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). Items are ordered by score (from worse- to better-formed), going from left-to-right and from top-to-bottom.
```{r com-monosyl, fig.cap = "(ref:com-monosyl)", fig.width=7, fig.asp=.8, warning=FALSE, dev="cairo_pdf"}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info_AA[order(monosyl_info_AA$NAP_bu),]$syl)
monosyl_info_AA$syl <- factor(monosyl_info_AA$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info_AA, aes(x=t)) + ylim(-5.5,20) +
  xlab("Time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  # geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, check_overlap=T) + 
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) +
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  # geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, check_overlap=T) +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Charis SIL", check_overlap=T) + #family = "Charis SIL",
  #
  facet_wrap(~syl, ncol=5) +
  theme(text = element_text(family = "Charis SIL"), panel.background = element_blank(), axis.title = element_text(size = 12), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8), strip.text = element_blank())
  # theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="sans"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="sans"), strip.text = element_blank())
print(monosyl_plot_abs)

```

(ref:com-monosylHeb) HN set (German speaker). See previous figure (Figure \@ref(fig:com-monosyl)) for plot details.
```{r com-monosylHeb, fig.cap = "(ref:com-monosylHeb)", fig.width=7, fig.asp=.8, warning=FALSE, dev="cairo_pdf"}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info_HN[order(monosyl_info_HN$NAP_bu),]$syl)
monosyl_info_HN$syl <- factor(monosyl_info_HN$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info_HN, aes(x=t)) + ylim(-5.5,20) +
  xlab("Time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  # geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, check_overlap=T) + 
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) +
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  # geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, check_overlap=T) +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Charis SIL", check_overlap=T) + #family = "Charis SIL",
  #
  facet_wrap(~syl, ncol=5) +
  theme(text = element_text(family = "Charis SIL"), panel.background = element_blank(), axis.title = element_text(size = 12), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8), strip.text = element_blank())
  # theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="sans"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="sans"), strip.text = element_blank())
print(monosyl_plot_abs)

```

##  Designs {#sec:designs}

The details in the following analyses address three separate experiments: 
(i) *Experiment 1*, an exploratory pilot experiment with 12 German speaking subjects listening to stimulus set AA (Hebrew speaker); (ii) *Experiment 2*, a confirmatory experiment with 51 German speaking subjects listening to stimulus set AA; and (iii) *Experiment 3*, a confirmatory experiment with 33 Hebrew speaking subjects listening to stimulus set HN (German speaker).

<!-- --- -->
<!-- ### Designing the exploratory study  -->

Given the various novelties in our proposal, the methodologies for data collection, data extraction, and model implementation were first tested on a small body of real data that we collected before finalizing our methodologies (namely the model implementations in Section \@ref(sec:modelimp) and the various procedural details in 
Appendix \@ref(appendix:a)).
<!-- Section \@ref(sec:methods).  -->
We used this exploratory study to test our methodologies and to explore the possibilities for properly estimating nucleus competition in each of the NAP models. 

The exploratory pilot study was administered in two versions, each with half of the fillers and all of the targets in one block, yielding a total of 58 data points per subject (29 fillers + 29 targets, no repetitions).
<!-- [^cf-pilot]  -->
The two different versions were evenly split between participants (each version was presented to six participants).

<!-- ####  Participants (Experiment 1) -->

<!-- The exploratory study consisted of 12 subjects (two males and ten females), all native German-speaking students from the Technische Hochschule Köln, who volunteered to participate in the study. The experiment was administered in a quiet room at the institute's facility in Cologne. The mean age of participants in the exploratory study was 25 (21--30). -->

<!-- ### Designing the confirmatory studies -->

Experiments 2 and 3 are the main confirmatory studies conducted after finalizing our hypotheses and methodologies with the data from Experiment 1. Each experimental block in Experiments 2-3 consisted of two repetitions of the target words (2 $\times$ 29 $=$ 58) and one trial of each filler word (1 $\times$ 58). The experiment consisted of two blocks with randomized trials, generating altogether four repetitions of the target words (4 $\times$ 29 $=$ 116) and two repetitions of the filler words (2 $\times$ 58 $=$ 116), yielding a total of 232 data points per subject. 

The difference between Experiment 2 and 3 concerns the native language of the subjects, and, as a consequence, the stimulus set in use. To promote bottom-up inferences (see Subsection \@ref(sec:rationale)), Experiment 2 tested German speaking subjects on stimulus set AA, featuring a Hebrew speaker, while Experiment 3 tested Hebrew speaking subjects on stimulus set HN, featuring a German speaker. 
In theory, we assume that speakers of any language will exhibit the general trends that are predicted by sonority-based models. We therefore assume that small differences between the groups should be reflective of their language-specific experience, which is related to their overall top-down phonotactic knowledge (not necessarily to sonority per se).
<!-- reflective of the differences in their language-specific top-down inferences. -->
<!-- with respect to sonority-related phonotactics -->
<!-- Using speakers of different languages  -->

##  Participants 

```{r participantsPil-tests, fig.show="hide"}

pilot_mean_age <- round(mean(subjects_pilot$subject_age))
pilot_min_age <- round(min(subjects_pilot$subject_age))
pilot_max_age <- round(max(subjects_pilot$subject_age))
pilot_N <- length(subjects_pilot$subject_nr)
pilot_male <- length(which(subjects_pilot$subject_gender=="male"))

```


```{r participantsHeb-tests, fig.show="hide"}

ggplot(subjects_heb, aes(x="", fill=subject_age)) +
  geom_bar(stat = "count") + #color="black",
  geom_text(aes(label = subject_age), size = 3, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Age")) +
  coord_polar("y", start = 0) +
  scale_fill_brewer(palette = "Blues") +
  ggtitle("Age", subtitle = "Experiment 3 (N = 33)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold")
        )


# which(subjects_heb$subject_age == subject_age)

```


### Experiment 1

The exploratory pilot study consisted of 12 subjects (two males and ten females), all native German-speaking students from the Technische Hochschule Köln, who volunteered to participate in the study. The experiment was administered in a quiet room at the institute's facility. The mean age of participants in the pilot study was 25 (21--30).

### Experiment 2

<!-- out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.4,.4,.4,.4,.4,.4), fig.show="hold", fig.align = "default"} -->
(ref:participantsGer) Participants in Experiment 2. Education categories refer to academic achievements ("school" = not yet acquired an academic degree).
```{r participantsGer, fig.cap = "(ref:participantsGer)", fig.show="hold", fig.align = "default", out.width=c("33%","33%","33%"), fig.width=c(2.3,2.3,2.3)}

ggplot(subjects_main, aes(x="", fill=subject_age)) + 
  geom_bar(stat = "count") + #color="black", 
  geom_text(aes(label = subject_age), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Age")) +
  coord_polar("y", start = 0.4, direction = -1) + 
  scale_fill_brewer(palette = "Blues") +
  ggtitle("Age", subtitle = "Experiment 2 (N = 51)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

ggplot(subjects_main, aes(x="", fill=subject_gender)) +
  geom_bar(stat = "count") + #color="black",
  geom_text(aes(label = subject_gender), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Gender")) +
  coord_polar("y", start = 0) +
  scale_fill_brewer(palette = "Greens") +
  ggtitle("Gender", subtitle = "Experiment 2 (N = 51)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

ggplot(subjects_main, aes(x="", fill=subject_education)) +
  geom_bar(stat = "count") + #color="black",
  geom_text(aes(label = subject_education), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Education")) +
  coord_polar("y", start = 0.2) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("Education", subtitle = "Experiment 2 (N = 51)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

# ggplot(subjects_main, aes(x="", fill=subject_handedness)) +
#   geom_bar(stat = "count") + #color="black",
#   geom_text(aes(label = subject_handedness), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
#   guides(fill = guide_legend(title = "Handedness")) +
#   coord_polar("y", start = 0) +
#   scale_fill_brewer(palette = "Greys") +
#   ggtitle("Handedness", subtitle = "Experiment 2 (N = 51)") +
#   theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
#         )
# 
# ggplot(subjects_main, aes(x="", fill=subject_german_native)) +
#   geom_bar(stat = "count") + #color="black",
#   geom_text(aes(label = subject_german_native), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
#   guides(fill = guide_legend(title = "Language background")) +
#   coord_polar("y", start = 0) +
#   scale_fill_brewer(palette = "Purples") +
#   ggtitle("Language background", subtitle = "Experiment 2 (N = 51)") +
#   theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
#         )

```

Fifty-one native German speakers (who did not participate in the exploratory pilot study) participated in Experiment 2, of which `r length(which(subjects_main$subject_german_native=="monolingual"))` were monolingual (the `r length(which(subjects_main$subject_german_native=="multilingual"))` bilingual speakers had Polish, Low German, and Hebrew as their heritage language). `r length(which(subjects_main$subject_handedness =="right-handed"))` participants were right-handed. See more details on age, gender and education of participants in Figure \@ref(fig:participantsGer).
<!-- Fifty-one native German speakers (`r length(which(subjects_main$subject_gender=="female"))` females, `r length(which(subjects_main$subject_gender=="male"))` males, and `r length(which(subjects_main$subject_gender=="other"))` "others") participated in Experiment 2, of which `r length(which(subjects_main$subject_german_native=="monolingual"))` were monolingual (the `r length(which(subjects_main$subject_german_native=="multilingual"))` bilingual speakers had Polish, Low German, and Hebrew as their heritage language). The vast majority (`r length(which(subjects_main$subject_age =="19-29"))` out of 51) were between the ages 19-29. Eight participants were between 30-39, and `r length(which(subjects_main$subject_age =="40-59"))` participants were between 40-59 years old. There were also two younger participants, between 13 and 18 years old. The vast majority (`r length(which(subjects_main$subject_handedness =="right-handed"))` of 51 participants) were right-handed.  -->
<!-- `r length(which(subjects_main$subject_education =="school"))` of the 51 participants had not yet acquired an academic degree at the time of the task (designated "school" in Figure \@ref(fig:participantsGer)), `r length(which(subjects_main$subject_education =="undergraduate"))` finished an *undergraduate* (*bachelor*) degree and `r length(which(subjects_main$subject_education =="graduate"))` acquired advanced *graduate* academic degrees (*master* and above). -->

Of the 51 participants, 34 were students at a major German university who took part in the experiment at the sound attenuated booth of the phonetics laboratory. The other 17 participants took part in the experiment at three different locations---all small quiet rooms within private apartments. All subjects were paid five Euros for their participation. 

We excluded the responses from one participant who failed in our participant inclusion criterion requiring accuracy of at least 75% with bi-vocalic fillers. The bi-vocalic fillers of the forms /CəCal/ and /əCCal/ link correct responses to the disyllabic choice ("2"), and we expect relatively few monosyllabic choices ("1") in response to stimuli with two separate vowels. 
Indeed, the overall average accuracy of all the 51 participants, when responding to bi-vocalic filler stimuli, was 96%. The excluded participant achieved a much lower accuracy score for bi-vocalic fillers, nearing chance-level with 65%.

### Experiment 3 {#sec:participants3}
<!-- TBA. -->
<!-- test if theis, `r length(which(subjects_heb$subject_gender=="male"))`, is this: 17 -->


(ref:participantsHeb) Participants in Experiment 3 (plot details are the same as above).
```{r participantsHeb, fig.cap = "(ref:participantsHeb)", fig.show="hold", fig.align = "default", out.width=c("33%","33%","33%"), fig.width=c(2.3,2.3,2.3)}

ggplot(subjects_heb, aes(x="", fill=subject_age)) + 
  geom_bar(stat = "count") + #color="black", 
  geom_text(aes(label = subject_age), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Age")) +
  coord_polar("y", start = 0, direction = -1) + 
  scale_fill_brewer(palette = "Blues") +
  ggtitle("Age", subtitle = "Experiment 3 (N = 33)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

ggplot(subjects_heb, aes(x="", fill=subject_gender)) +
  geom_bar(stat = "count") + #color="black",
  geom_text(aes(label = subject_gender), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Gender")) +
  coord_polar("y", start = 0) +
  scale_fill_brewer(palette = "Greens") +
  ggtitle("Gender", subtitle = "Experiment 3 (N = 33)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

ggplot(subjects_heb, aes(x="", fill=subject_education)) +
  geom_bar(stat = "count") + #color="black",
  geom_text(aes(label = subject_education), size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  guides(fill = guide_legend(title = "Education")) +
  coord_polar("y", start = 0.5) +
  scale_fill_brewer(palette = "Oranges") +
  ggtitle("Education", subtitle = "Experiment 3 (N = 33)") +
  theme(legend.position="none", panel.background = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), axis.ticks = element_blank(), plot.title = element_text(size=14, face="bold"), plot.subtitle = element_text(size=10)
        )

```
<!-- Thirty-three native Hebrew speakers (`r length(which(subjects_heb$subject_gender=="female"))` females, `r length(which(subjects_heb$subject_gender=="male"))` males, and `r length(which(subjects_heb$subject_gender=="other"))` "others") participated in Experiment 3, of which `r length(which(subjects_heb$subject_hebrew_native=="monolingual"))` were monolingual (the five native bilinguals were also native speakers of English, Russian and Spanish). -->
<!-- `r length(which(subjects_heb$subject_age =="19-29"))` out of the 33 subjects in Experiment 3 were between the ages 19-29 and `r length(which(subjects_heb$subject_age =="40-59"))` participants were between 40-59 years old. `r length(which(subjects_heb$subject_age =="30-39"))` participants were between 30-39, and there were also `r length(which(subjects_heb$subject_age =="13-18"))` younger participants, between 13 and 18 years old. The vast majority (`r length(which(subjects_heb$subject_handedness =="right-handed"))` of 33 participants) were right-handed. -->
<!-- `r length(which(subjects_heb$subject_education =="school"))` of the 33 participants had not yet acquired an academic degree at the time of the task (designated "school" in Figure \@ref(fig:participantsHeb)), `r length(which(subjects_heb$subject_education =="undergraduate"))` finished an *undergraduate* (*bachelor*) degree and `r length(which(subjects_heb$subject_education =="graduate"))` acquired advanced *graduate* academic degrees (*master* and above). -->

Thirty-three native Hebrew speakers participated in Experiment 3, of which `r length(which(subjects_heb$subject_hebrew_native=="monolingual"))` were monolingual (the five native bilinguals were also native speakers of English, Russian and Spanish). `r length(which(subjects_heb$subject_handedness =="right-handed"))` participants were right-handed. See more details on age, gender and education of participants in Figure \@ref(fig:participantsHeb).

<!-- `r length(which(subjects_heb$subject_education =="school"))` of the 33 participants had no to very little academic background, `r length(which(subjects_heb$subject_education =="undergraduate"))` finished an *undergraduate* (*bachelor*) academic degree and `r length(which(subjects_heb$subject_education =="graduate"))` finished advanced *graduate* academic degrees (*master* and above). -->
<!-- Of the 33 participants, 34 were students at the University of Cologne who took part in the experiment at the sound attenuated booth of the phonetics laboratory. The other 17 participants took part in the experiment at three different locations---all small quiet rooms within private apartments. All subjects were paid five Euros for their participation.  -->

The data collection in Experiment 3 was more diverse, and, perhaps more "noisy" than Experiment 2 as a result.
The first endeavor, which took place in 2019, managed to collect data from student volunteers in Tel Aviv University and The Hebrew University of Jerusalem. The second data collection effort occurred already during the early phases of the global COVID-19 pandemic, which resulted in fewer overall participants, at many different ad-hoc and suboptimal locations which were used to administer the experiment.
<!-- and more heterogenous population in some respects.  -->
<!-- (the results confirm this with a wider standard error for the intercepts of the different subjets in the statistical models).  -->

## Data Analysis {#sec:datanlysis}

We use a Bayesian data analysis approach implemented in the probabilistic programming language  *Stan* [@Stan2018] using the model wrapper package *brms* [@R-brms_a; @R-brms_b] in *R* [@R-base].^[The complete list of *R* packages that we used is:  `r cite_r("bibs/r-references.bib")`.] An important motivation for using the Bayesian approach is that it    is easy to fit fully hierarchical models with a "maximal random effect structure", which provide the most conservative estimates of uncertainty [@SchielzethForstmeier2009]. In all our models, we use regularizing priors, which we detail below. These priors are minimally informative and have the objective of yielding stable inferences [@chung2013weakly; @gelman2008weakly; @GelmanEtAl2017]. @NicenboimVasishth2016 and @VasishthEtAl2017EDAPS discuss the Bayesian approach in detail in the context of psycholinguistic and phonetic sciences research. We fit the models with four chains and 4000 iterations each, of which 1000 iterations were the burn-in or warm-up phase. In order to assess convergence, we verify that there are no divergent transitions, that all the $\hat{R}$  (the between- to within-chain variances) are close to one,  that the number of effective sample size are at least 10\% of the number of post-warmup samples, and we visually inspect the chains.

For the statistical models, we take into account that the traditional sonority models and the top-down version of NAP (i.e., *SSP~col~* , *SSP~exp~*, *MSD~col~*, *MSD~exp~*, and *NAP~td~*) are ordinal models, while the bottom-up version of NAP (*NAP~bu~*) is a continuous model. The ordinal models predict that certain groups of onset clusters will be better or worse-formed than other group depending on an ordinal score, but they do  not assume that the score will be equidistant with respect to its effect on the response variable, log-transformed response times. For this reason, the discrete scores of these models are assumed to have a monotonic effect on the log-response time in our task, that is, having a monotonically increasing or decreasing relationship with the log-response time, while  the distance between groups are estimated from the data [@burknerModelingMonotonicEffects2018]. 

In contrast,  *NAP~bu~* is modeled with a continuous predictor which is assumed to have a linear relationship with the log-response times. Finally, as baseline, we fitted a "null" model which assumes no relationship between the stimuli and the response times. 

All the models, included a random intercept and slope by subjects (except for the null model that included only a random intercept) and the following weakly regularizing priors: $Normal(6, 2)$ for the intercept, $Normal(0, 1)$ for the slope, $Normal_+(0,1)$ for the variance components, and $lkj(2)$ for the correlation between by-participant adjustments. The ordinal models have as well a Dirichlet prior for the simplex vector that represents the distance between the categories set to one for each of its parameters. 

We evaluate the models in three different ways: (i) estimation, (ii) descriptive adequacy, and (iii) model comparison.

### (i) Estimation {-}
We report mean estimates and 95\% quantile-based Bayesian credible intervals. A 95\% Bayesian credible interval has the following interpretation: it is an interval containing the true value with 95% probability given the data and the model [see, for example, @Jaynes1976; @MoreyEtAl2015]. 

### (ii) Descriptive adequacy {-}
We use posterior predictive checking to examine the descriptive adequacy or "fit" of the models [@shiffrinSurveyModelEvaluation2008]: the observed data should look plausible under the posterior predictive distribution of the models. The posterior predictive distribution of each model is composed of simulated datasets generated based on the posterior distributions of its parameters. Given the posterior of the parameters of the model, the posterior predictive distribution shows how other data may look like. Achieving descriptive adequacy means that the current data could have been predicted with the model. It is important to notice that a good fit, that is, passing a test of descriptive adequacy, is not strong evidence in favor of a model; in contrast, a major failure in descriptive adequacy can be interpreted as strong evidence against a model [@shiffrinSurveyModelEvaluation2008]. Thus, we use posterior predictive checks to assess whether the model behavior is reasonable and in which situations is not [see @gelmanBayesianDataAnalysis2013  for further discussion].

### (iii) Model comparison {-}
For model comparison, we examine the out-of-sample predictive accuracy of the different models using k-fold (k=15) cross validation stratified by subjects.^[Pareto smoothed importance sampling approximation to leave-one-out cross validation  [implemented in the package `loo`; @vehtariPracticalBayesianModel2017; @vehtariParetoSmoothedImportance2015] failed to yield stable estimates.] Cross validation evaluates the different models with respect to their predictive accuracy, that is, how well the models generalize to new data.

##  Results {#sec:results}

###  Estimations
<!-- ###  Results: Estimations -->

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp_pil <- results_txt(m_pilot$models$SSP, "moSSP")
est_MSD_exp_pil <- results_txt(m_pilot$models$MSD, "moMSD")
est_SSP_col_pil <- results_txt(m_pilot$models$SSP_obs, "moSSP_obs")
est_MSD_col_pil <- results_txt(m_pilot$models$MSD_obs, "moMSD_obs")
est_NAP_td_pil <- results_txt(m_pilot$models$NAP_td, "moNAP_td")
est_NAP_bu_pil <- results_txt(m_pilot$models$NAP_bu, "sNAP_bu")

```

```{r}
scores <- data_pilot %>% filter(!is.na(NAP_bu)) %>% 
  distinct(stimulus, NAP_bu) %>% 
  arrange(NAP_bu)

lpal_nap <- scores %>% filter(stimulus == "lpal") %>% 
  pull(NAP_bu) %>% round(0)
lkal_nap <- scores %>% filter(stimulus == "lkal") %>% 
  pull(NAP_bu)%>% round(0)
spal_nap <- scores %>% filter(stimulus == "spal") %>% 
  pull(NAP_bu)%>% round(0)

```

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp <- results_txt(m_german$models$SSP, "moSSP")
est_MSD_exp <- results_txt(m_german$models$MSD, "moMSD")
est_SSP_col <- results_txt(m_german$models$SSP_obs, "moSSP_obs")
est_MSD_col <- results_txt(m_german$models$MSD_obs, "moMSD_obs")
est_NAP_td <- results_txt(m_german$models$NAP_td, "moNAP_td")
est_NAP_bu <- results_txt(m_german$models$NAP_bu, "sNAP_bu")

```

<!-- ### Experiment 3: Estimation {#sec:results3} -->

```{r}
estHeb_SSP_exp <- results_txt(m_hebrew$models$SSP, "moSSP")
estHeb_MSD_exp <- results_txt(m_hebrew$models$MSD, "moMSD")
estHeb_SSP_col <- results_txt(m_hebrew$models$SSP_obs, "moSSP_obs")
estHeb_MSD_col <- results_txt(m_hebrew$models$MSD_obs, "moMSD_obs")
estHeb_NAP_td <- results_txt(m_hebrew$models$NAP_td, "moNAP_td")
estHeb_NAP_bu <- results_txt(m_hebrew$models$NAP_bu, "sNAP_bu")
```

<!-- #### Estimation {#sec:estimation1} -->
For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

**Experiment 1**

<!-- ### Experiment 1 {-} -->
* For *SSP~col~*: `r est_SSP_col_pil`.
* For *SSP~exp~*: `r est_SSP_exp_pil`.
* For *MSD~col~*: `r est_MSD_col_pil`.
* For *MSD~exp~*: `r est_MSD_exp_pil`.
* For *NAP~td~*: `r est_NAP_td_pil`.
* For *NAP~bu~*: `r est_NAP_bu_pil`.

**Experiment 2**

<!-- ### Experiment 2 {-} -->
* For *SSP~col~*: `r est_SSP_col`.
* For *SSP~exp~*: `r est_SSP_exp`.
* For *MSD~col~*: `r est_MSD_col`.
* For *MSD~exp~*: `r est_MSD_exp`.
* For *NAP~td~*: `r est_NAP_td`.
* For *NAP~bu~*: `r est_NAP_bu`.

**Experiment 3**

<!-- ### Experiment 3 {-} -->
* For *SSP~col~*: `r estHeb_SSP_col`.
* For *SSP~exp~*: `r estHeb_SSP_exp`.
* For *MSD~col~*: `r estHeb_MSD_col`.
* For *MSD~exp~*: `r estHeb_MSD_exp`.
* For *NAP~td~*: `r estHeb_NAP_td`.
* For *NAP~bu~*: `r estHeb_NAP_bu`.

Notice that the posterior of the effect of well-formedness, $\hat\beta$, is not comparable across models. For the ordinal models, it represents  the average increase (or decrease) in the dependent variable associated with two neighboring factor levels, or in other words, $\hat\beta$ multiplied by the number of categories minus one represents the increase in log-scale between the first and the last category. This means that it is highly affected by the number of categories. For the continuous bottom-up model, $NAP_{bu}$, $\beta$ represents, the increase in log-scale for one unit in the well-formedness scale. To make it concrete, between, /lpal/ and /lkal/ there are `r lkal_nap -lpal_nap` units (`r lkal_nap` and `r lpal_nap` respectively); and between /lkal/ and /spal/ there are `r spal_nap  -lkal_nap` units (since their NAP scores are `r spal_nap` and `r lkal_nap` respectively). However, for all the models, $\hat\beta$ is negative, indicating that well-formedness is associated with faster responses. See Appendix \@ref(appendix:b) for the complete output of the models.

The results shown here reflect the final state of the models in the exploratory stage, which is the same as the state of the models in the confirmatory stage. Importantly, the results of the confirmatory studies, Experiments 2-3, which are statistically much more robust, remain consistent with those of Experiment 1, which had a relatively small number of observations. As such, Experiment 1 was not designed to distinguish between the models and it will not be shown here further in the presentation of the results. 

<!-- Note also that the effects in Experiment 3 are somewhat smaller. This is suggested to be reflective of the noisier settings -->

###  Descriptive adequacy
<!-- ##  Results: Descriptive adequacy -->
<!-- ### Experiment 2: Descriptive adequacy -->
<!-- ### (ii) Descriptive adequacy {-} -->
<!-- ####  Descriptive adequacy {#sec:posterior2} -->

The model fits of the different models are shown in Figures \@ref(fig:NullFit)--\@ref(fig:NAPbuFit) below. The plots in these figures present the dispersion of the average response time results, depicted as red points for related CC clusters, vis-à-vis each models' predictions in the form of distributions, depicted with blue violins. The order of the stimuli, from left to right, follows from the models' scores such that predictions for better-formed clusters appear further to the right. Recall that scores in the *NAP~bu~* model yield slightly different predictions for each stimulus set (AA vs. HN).

#### Null models

The null models are shown in Figure \@ref(fig:NullFit) as baselines in the respective experiments (the order of stimuli along the x-axis follows the *NAP~bu~* scores, yet in a forced ordinal scale, with equidistant intervals). The slight differences in predictions for different clusters are due to individual differences in the accuracy. Recall that we subset the response times conditional on the monosyllabic response ('1') to the forced-choice task. This means that when participants give more monosyllabic answers for a specific cluster, their adjusted intercept will have a greater influence on the predictions of the model for that cluster. In addition, clusters with fewer monosyllabic responses show more variability in their predictions (e.g., /lf/ vs. /fl/ in the AA set, on the left side of Figure \@ref(fig:NullFit)).

(ref:NullFit) Null model fit. Observed mean log-transformed response times are depicted with red points, distribution of simulated means based on the null model are depicted with blue violins. 
<!-- Stimuli ordered from left to right according to their ascending well-formedness score in *NAP~bu~* (here in a forced-ordinal scale for exposition purposes). -->
```{r NullFit,fig.cap = "(ref:NullFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r NullFit,fig.cap = "(ref:NullFit)", warning=FALSE, fig.width=7, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}
predictions$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .5, size =.8) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) + 
    scale_x_discrete("Stimuli (AA set: Experiment 2)", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("Response Time") +
    ylim(600,1200) +
    coord_trans(y="log") +
  theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
    # ggtitle("Null model fit")

predictions_heb$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .5, size =.8) +
    geom_text_repel(data= data_RT_heb, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) + 
    scale_x_discrete("Stimuli (HN set: Experiment 3)", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("RT (Experiment 3)") +
    ylim(600,1200) +
    coord_trans(y="log") +
  theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())


```

<!-- ### (ii.a) Traditional sonority models fit {-} -->
#### SSP and MSD models {#sec:traditionalModelfit}

We consider a good fit in the case of the ordinal models to be roughly characterized by the following three criteria:
(i) the data is contained within the predictions, i.e., the red points appear within the respective violins;
(ii) the data is consistent within each predicted level, i.e., the vertical dispersion of red points pattern together around the same area within each level (preferably in the middle of the distribution); and 
(iii) the model predictors are not redundant, i.e., the violins of the different model levels show little overlap between them.

(ref:SSPcolFit) *SSP~col~* model fit. Stimuli ordered from left to right according to their score in the model in ascending well-formedness (other details are the same as above).
<!-- Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness. -->
```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .5, size =.8) +
    xlab(bquote(italic(SSP[col])~scores~(Experiment~2))) + ylab("Response Time") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(SSP[col])~fit))
   }

predictions_heb$SSP_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .5, size =.8) +
    xlab(bquote(italic(SSP[col])~scores~(Experiment~3))) + ylab("RT (Experiment 3)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(SSP[col])~fit))
   }

```

(ref:SSPexpFit) *SSP~exp~* model fit (plot details are the same as above).
```{r SSPexpFit,fig.cap = "(ref:SSPexpFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r SSPexpFit,fig.cap = "(ref:SSPexpFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(SSP[exp])~scores~(Experiment~2))) +
       ylab("Response Times") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(SSP[exp])~fit))
   }

predictions_heb$SSP %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(SSP[exp])~scores~(Experiment~3))) +
       ylab("RT ") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(SSP[exp])~fit))
   }

```

<!-- ### MSD models {#sec:MSDModelfit} -->

(ref:MSDcolFit) *MSD~col~* model fit (plot details are the same as above).
```{r MSDcolFit,fig.cap = "(ref:MSDcolFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r MSDcolFit,fig.cap = "(ref:MSDcolFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[col])~scores~(Experiment~2))) + ylab("Response Ttime") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(MSD[col])~fit))
   }

predictions_heb$MSD_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[col])~scores~(Experiment~3))) + ylab("RT") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(MSD[col])~fit))
   }

```


(ref:MSDexpFit) *MSD~exp~* model fit (plot details are the same as above).
```{r MSDexpFit,fig.cap = "(ref:MSDexpFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r MSDexpFit,fig.cap = "(ref:MSDexpFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[exp])~scores~(Experiment~2))) + ylab("Response Time") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(MSD[exp])~fit))
   }

predictions_heb$MSD %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[exp])~scores~(Experiment~3))) + ylab("RT") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
        # ggtitle(bquote(italic(MSD[exp])~fit))
   }

```

A quick observation at the four plots of Experiment 2, on the left side of Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) reveals a common failure of all the traditional sonority models with respect to the German speaking subjects, to contain the nasal plateaus (/mn/ and more so /nm/) within their predicted distribution alongside all the other plateaus ('0' model score in all these figures). Furthermore, the data within the '0' plateau levels appears to be widely dispersed for the German-speaking subjects in Experiment 2 (plots on the left side) but quite well centered for the Hebrew-speaking subjects in Experiment 3 (right plots).

A comparison of the left-most violin in all the plots in Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) highlights some differences between the two sonority hierarchies---*H~col~* (*SSP/MSD~col~*) and *H~exp~* (*SSP/MSD~exp~*).
The left-most violins reflect the onset fall levels, of these SSP and MSD models.
For the Hebrew-speaking subjects in Experiment 3 (right plots), there was no big difference between the two sonority hierarchies and a similiar wide distribution appears in all fits of sonority falls. In contrast, the German-speaking subjects in Experiment 2 exhibit a bimodal distribution in the falling onsets of sonority models that use the *H~exp~* hierarchy (*SSP/MSD~exp~*), whereby *fricative-stop* clusters /ʃp, sp, ft/ are considered to be highly ill-formed onset falls.

<!-- A closer observation at the two left-most columns of all the plots in Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit), which reflect the onset fall and onset plateau levels, presents a comparison between the two sonority hierarchies---*H~col~* (*SSP/MSD~col~*) and *H~exp~* (*SSP/MSD~exp~*). The difference between these two sonority hierarchies results in different allocation of the fricative-stop clusters /ʃp, sp, ft/, as plateaus in the *H~col~* hierarchy ('0' in Figures \@ref(fig:SSPcolFit) and \@ref(fig:MSDcolFit)) and as falls in the *H~exp~* hierarchy ('-1' in Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). The latter case of the *H~exp~* hierarchy, whereby the fricative-stop clusters are treated alongside the most ill-formed onset falls, leads to two distinct patterns in the vertical dispersion of data at the left-most column of onset falls (Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)).  -->
This suggests that the *H~col~* hierarchy (where all obstruents are grouped into one class on the sonority hierarchy such that fricative-stop clusters are considered plateaus) is better than the *H~exp~* hierarchy in treating fricative-stop clusters, as reflected in the better model fits for onset falls and plateaus when the *H~col~* hierarchy is applied (*SSP/MSD~col~* vs. *SSP/MSD~exp~*). However, the two sonority hierarchies also lead to differences in the grouping of onset rises when the MSD models are taken into account, as we observe next.

The right side of these plots, i.e., the violins that reflect well-formed onset rises with positive model scores, present three types of grouping across the four models. The two SSP models (*SSP~col/exp~*) make identical predictions with respect to onset rises, lumping all rises into one category ('1' in Figures \@ref(fig:SSPcolFit)--\@ref(fig:SSPexpFit)). 
This, again, results in a wider distribution for the German-speaking subjects in Experiment 2 (left plots) compared to the Hebrew-speaking subjects in Experiment 3 (right plots).

The MSD models present multiple levels of well-formedness for onset rises. *MSD~col~* exhibits two levels of rises ('1--2' in Figure \@ref(fig:MSDcolFit)) while *MSD~exp~* exhibits four levels of rises ('1--4' in Figure \@ref(fig:MSDexpFit)).
This elaboration seems to be beneficial in fitting the scores of the German-speaking subjects to the 4 rise levels of *MSD~exp~*, but less so for *MSD~col~*. Furthermore, the additional levels of the MSD are redundant, or even worse, for the fits of the scores of the Hebrew-speaking subjects in Experiment 3 (right plots).

<!-- The vertical dispersion of data in the right-most column of the SSP models appears very wide, with voiced-initial clusters like /ml, vn, vl/ appearing to pattern separately from voiceless-initial clusters like /ʃm, sm, fl/. -->
<!-- In contrast, the MSD models present multiple levels of well-formedness for onset rises. *MSD~col~* exhibits two levels of rises ('1--2' in Figure \@ref(fig:MSDcolFit)) while *MSD~exp~* exhibits four levels of rises ('1--4' in Figure \@ref(fig:MSDexpFit)). The grouping in *MSD~col~* appears to result in a relatively high overlap between the distributions of the three right-most columns (including onset plateaus), suggesting redundancy in the model. In comparison, the grouping in *MSD~exp~* seems to capture the distinct patterns of onset rises more accurately, with more levels yet less overlap. Therefore, it appears that the combination of the *H~exp~* hierarchy with the MSD (*MSD~exp~*) has the best fit with respect to onset rises. -->

To conclude, an observation of the model fits of the four traditional sonority models in the two confirmatory studies brings up a mixed picture: the *H~col~* hierarchy (*SSP/MSD~col~*) appears to result in a better fit with onset falls and plateaus, especially for the German-speaking subjects.
The competing *H~exp~* hierarchy appears to be advantageous when fitting the scores of the rising onset slopes, but mostly with the MSD (i.e., *MSD~exp~*), and only for the German-speaking subjects in Experiment 2, where sonority falls exhibit an undesirable bimodal distribution (as explained above).

<!-- while *MSD~exp~* seems to have the best fit with onset rises. The advantage of *H~col~* with respect to onset falls and plateaus is related to the treatment of voiceless fricative-stop clusters as better-formed (onset plateau rather than of onset fall). The advantage of *MSD~exp~* with respect to onset rises is related to its ability to separate the voiceless-initial from the voiced-initial onset rises, such that voiceless-initial rises are better-formed. These selective and partial advantages, as we detail next, are built-in into the logic of the NAP models. -->

<!-- ### (ii.b) NAP~td~ model fit {-} -->
#### NAP models {#sec:NAPtdModelfit}

Although *NAP~td~* is an ordinal model like all the traditional sonority models, it follows a different rationale (see Subsection \@ref(sec:naptdmodel)), whereby the scores of the model estimate nucleus competition to reflect well-formedness. 

(ref:NAPtdFit) *NAP~td~* model fit (plot details are the same as above).
```{r NAPtdFit,fig.cap = "(ref:NAPtdFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r NAPtdFit,fig.cap = "(ref:NAPtdFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha=.5, size =.8)+
  xlab(bquote(italic(NAP[td])~scores~(Experiment~2))) + ylab("Response Time") +
      ylim(600,1200) +
      coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
      # ggtitle(bquote(italic(NAP[td])~fit))
  }

predictions_heb$NAP_td %>% 
  left_join(data_RT_heb) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha=.5, size =.8)+
  xlab(bquote(italic(NAP[td])~scores~(Experiment~3))) + ylab("RT") +
      ylim(600,1200) +
      coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
      # ggtitle(bquote(italic(NAP[td])~fit))
  }

```

Figure \@ref(fig:NAPtdFit) shows that *NAP~td~* succeeds in containing all the data (points) within the respective predictions (blue violins) in both experiments, making *NAP~td~* the only model to achieve that coverage among the fits of Experiment 2. 
<!-- A comparison with Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) shows that *NAP~td~* is the only model to achieve this. -->
*NAP~td~* appears to exhibit some redundancy, as suggested by the relatively large degrees of overlap between some of the predictive distributions of the model. This is apparent from the overlap between violins in the left side (worse-formed) of the model fit with Experiment 2 (left plot), and between violins in the right side (better-formed) of the model fit with Experiment 3 (right plot) in Figure \@ref(fig:NAPtdFit).

<!-- ### (ii.c) NAP~bu~ model fit {-} -->
<!-- ### NAP~bu~ models {#sec:NAPbuModelfit} -->
(ref:NAPbuFit) *NAP~bu~* model fit (plot details are the same as above).
```{r NAPbuFit,fig.cap = "(ref:NAPbuFit)", warning=FALSE, out.width=c("50%","50%"), fig.width=c(3.5,3.5), fig.asp=c(.5,.5), fig.show="hold", fig.align = "default", dev="cairo_pdf"}
# ```{r NAPbuFit,fig.cap = "(ref:NAPbuFit)", warning=FALSE, fig.width=7, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .5, size =.8) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    scale_x_discrete(bquote(italic(NAP[bu])~scores~(AA~set:~Experiment~2)), breaks=NULL) +
    ylab("Response Time") +
    ylim(600,1200) +
    coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_text(size=8, family = "Charis SIL"), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_text(size=10, family = "Charis SIL"), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
    # ggtitle(bquote(italic(NAP[bu])~fit))

predictions_heb$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .5, size =.8) +
    geom_text_repel(data= data_RT_heb, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = 1, size =3.5, family="Charis SIL", min.segment.length=0, segment.size = .4, segment.alpha = .6) +
    scale_x_discrete(bquote(italic(NAP[bu])~scores~(HN~set:~Experiment~3)), breaks=NULL) +
    ylab("RT (Experiment 3)") +
    ylim(600,1200) +
    coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.x = element_text(size=9, family = "Charis SIL"), 
              axis.text.y = element_blank(), 
              axis.title.x = element_text(size=11, family = "Charis SIL"), 
              axis.title.y = element_blank(), 
              axis.ticks.x = element_blank(), plot.title = element_blank())
    # ggtitle(bquote(italic(NAP[bu])~fit))

```

*NAP~bu~* is different from all the other models in that it presents scores that are specific to each token in a continuous ratio scale, rather than an ordinal scale (i.e., the distances between scores in the model are also predicted). See Figure \@ref(fig:NAPbuFit) where the expected positive correlation between response time and ill-formedness appears to generally hold for both the predictions and the data of the model fit of *NAP~bu~*.

Our criteria for goodness of fit based on the plot analyses (see opening Subsection 
\@ref(sec:traditionalModelfit)) are not all valid when evaluating *NAP~bu~* since we have no classes and no vertical dispersion of data (points) within levels, and since the horizontal overlap of predictions (violins) between levels requires different interpretation. However, the criterion for inclusion of data points within the violins of the models' predictions naturally also holds for the *NAP~bu~* fit, which fails to include the data for the nasal plateaus /nm/ and /mn/ within the respective predictive distribution in Experiment 2 (a failure that is shared also by all the traditional models in Experiment 2; see Subsection \@ref(sec:traditionalModelfit)). Furthermore, in Experiment 2 *NAP~bu~* also fails to include the /z/-initial clusters---/zm/ and /zv/---within their respective predictive distribution.

#### Model fits discussion {#sec:discussionFits} 

The failures in the fit of the *NAP~bu~* model with German-speaking subjects in Experiment 2 can be split into two types:
(i) nasal-initial clusters---*nval*, *nmal*, and *mnal*---which received results on a par with the slowest responses in the data, reflecting an overestimation of well-formedness by the model, and;
(ii) syllables beginning with a voiced sibilant---*zval* and *zmal*---which received results that pattern with faster responses, reflecting well-formedness underestimation by the model.

These results may be taken to suggest language-specific top-down effects of German, in which sibilants are regularly unvoiced/devoiced at edges of clusters, and nasals, on the other hand, can be syllabic. In that sense, German-speaking listeners may be more skewed to consider marginal sibilance as a voiceless nucleus repeller, while nasality, for German-speakers, is a potential nucleus attractor.
Compare this with Hebrew, which is the native language of the subjects in Experiment 3, in which nasals cannot be syllabic and voiced sibilants are common in marginal cluster edges. 

<!-- These results may be taken to suggest that the distinctive high-frequency aperiodic energy of sibilants has a top-down repeller effect on the nucleus, despite additional periodic energy, while nasals, that can attract the nucleus in German (syllabic nasals occur in German), have a top-down attraction effect on the nucleus.[^cf-td]   -->

###  Model comparison
<!-- ##  Results: Model comparison -->
<!-- ### Experiment 2: Model comparison -->
<!-- ### (iii) Model comparison {-} -->
<!-- #### Model comparison {#sec:ModelComparison2} -->

While the model fits give us an insight into the behavior of each model with respect to the data, they are not well-suited to compare the different models against a consistent criterion. To do that, we run out-of-sample predictions using cross-validation to test each model's ability to predict unseen items.

#### Experiment 2

A bird's eye view of all the six model fits in Experiment 2 is available in Figure \@ref(fig:SonFitAll). The results of the model comparison from Experiment 2 are available in Table \@ref(tab:resultsmodels). It reveals a very clear advantage of *NAP~td~* over all the other models. 
The difference in elpd scores from the next three models---*SSP/MSD~col~* and *NAP~bu~*---is about 6 standard errors (considering that the difference is around 90 elpd and the corresponding standard error is around 15), a very robust lead for *NAP~td~*. 
The small differences between the next three models (*SSP/MSD~col~* and *NAP~bu~*) make them all indistinguishable in the second place.
The two traditional models that are based on the *H~exp~* hierarchy---*SSP/MSD~exp~*---are similar to each other at the last place, only marginally better than the null model.

The right-most column in Table \@ref(tab:resultsmodels), *weight*, shows model averaging via stacking of predictive distributions. Stacking maximizes the potential elpd score by pulling the predictions of all the different models together. The values under the *weight* column represent the relative contribution of each model to this combined optimal model.
*NAP~td~* alone contributes the lion's share with 65% and *NAP~bu~* comes second with 14%. This is notable as both NAP models are essentially based on the same principle, lending support to the idea top-down and bottom-up lead to two complementary models.
The other traditional models contribute 8% (*SSP~col~*) and 3% (*MSD~col~*) to this picture, less than the 9% that the null model manages to contribute.

```{r resultsmodels, results = "asis"}

comparison <- loo::loo_compare(x=kfold_german)
ll_matrix <- map2_dfc(kfold_german, names(kfold_german), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_german)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison: Experiment 2",note= "The table is ordered by the expected log-predictive density (elpd) score of the models, with a higher score indicating better predictive accuracy. The highest scored model is used as a baseline for the difference in elpd and the difference standard error (SE). The column weight represents the weights of the individual models that maximize the total elpd score of all the models.")

```

(ref:SonFitAll) Experiment 2: all sonority model fits (unspecified cluster types, see detailed versions above). Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli are ordered from left to right according to their score in a given model in ascending well-formedness. 
```{r SonFitAll,fig.cap = "(ref:SonFitAll)", warning=FALSE, out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.4,.4,.4,.4,.4,.4), fig.show="hold", fig.align = "default"}

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .5, size =.8) +
    xlab(bquote(italic(SSP[col]))) + ylab("Response Time") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
   }


predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[col]))) + ylab("Response Time") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) 
     }

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(SSP[exp]))) +
       ylab("Response Time") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
     }

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =.8, alpha = .5) +
    xlab(bquote(italic(MSD[exp]))) + ylab("Response Time") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank())
     }

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha = .5, size =.8)+
  xlab(bquote(italic(NAP[td]))) + ylab("Response Time") +
      ylim(600,1200) + 
      coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
    }

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =.8) +
    scale_x_discrete(bquote(italic(NAP[bu])), breaks=NULL) +
    ylab("Response Time") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) 

```

#### Experiment 3

A bird's eye view of all the six model fits in Experiment 3 is available in Figure \@ref(fig:SonFitAllHeb). 
The results of the model comparison from Experiment 3 (see Table \@ref(tab:resultsmodelsHeb)) reveal a borderline advantage of *NAP~td~* over all the other models.
The difference in elpd scores from the next two models---*MSD~col~* and *NAP~bu~*---is only about 2 standard errors (considering the difference at around 10 elpd and the corresponding standard error at around 5).

*SSP~col~* is more clearly distinguishable from *NAP~td~*, with a difference that is almost 3 standard errors (about 20:7). *MSD~col~* and *NAP~bu~* are therefore only barely distinguishable from *SSP~col~* below them, and from *NAP~td~* above them. The two traditional models that are based on the *H~exp~* hierarchy---*SSP/MSD~exp~*---are, again, very clearly the worst in the comparison.

The *weight* values of Experiment 3 in Table \@ref(tab:resultsmodelsHeb) show, again, that *NAP~td~* alone provides the biggest relative contribution to a combined optimal model, with 61%. *MSD~col~* covers almost the entire remaining space with 37%, leaving *NAP~bu~* and all the other traditional models with zero additional contribution. 


```{r resultsmodelsHeb, results = "asis"}

comparison <- loo::loo_compare(x=kfold_hebrew)
ll_matrix <- map2_dfc(kfold_hebrew, names(kfold_hebrew), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_hebrew)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison: Experiment 3",note= "(details are the same as above).")


```

(ref:SonFitAllHeb) Experiment 3: all sonority model fits (plot details are the same as above).
```{r SonFitAllHeb,fig.cap = "(ref:SonFitAllHeb)", warning=FALSE, out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.4,.4,.4,.4,.4,.4), fig.show="hold", fig.align = "default"}

predictions_heb$SSP_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha=.5, size =.8) +
    xlab(bquote(italic(SSP[col]))) + ylab("Response Time") +
        ylim(600,1000) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
   }


predictions_heb$MSD_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =.8, alpha=.5) +
    xlab(bquote(italic(MSD[col]))) + ylab("Response Time") +
        ylim(600,1000) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) 
     }

predictions_heb$SSP %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =.8, alpha=.5) +
    xlab(bquote(italic(SSP[exp]))) +
       ylab("Response Time") +
        ylim(600,1000) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
     }

predictions_heb$MSD %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =.8, alpha=.5) +
    xlab(bquote(italic(MSD[exp]))) + ylab("Response Time") +
        ylim(600,1000) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank())
     }

predictions_heb$NAP_td %>% 
  left_join(data_RT_heb) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha=.5, size =.8)+
  xlab(bquote(italic(NAP[td]))) + ylab("Response Time") +
      ylim(600,1000) + 
      coord_trans(y="log") +
        theme(panel.background = element_blank(), 
              axis.text.y = element_text(size=8, family = "Times"), 
              axis.text.x = element_blank(), 
              axis.title.y = element_text(size=9, family = "Times"), 
              axis.title.x = element_text(size=11, family = "Times"), 
              axis.ticks = element_blank(), plot.title = element_blank())
    }

predictions_heb$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=1, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =.8) +
    scale_x_discrete(bquote(italic(NAP[bu])), breaks=NULL) +
    ylab("Response Time") +
    ylim(600,1000) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) 

```

<!-- The right-most column in Table \@ref(tab:resultsmodels), *weight*, shows model averaging via stacking of predictive distributions. Stacking maximizes the potential elpd score by pulling the predictions of all the different models together. The values under the *weight* column represent the relative contribution of each model to this combined optimal model. -->
<!-- *NAP~td~* alone contributes the lion's share with 65% and *NAP~bu~* comes second with 14%. -->
<!-- All the other traditional models together contribute only 11% to this picture. -->

<!-- Model weights can be informative with respect to complementarity vs. redundancy because the ability to contribute to the maximized score entails some uniqueness. This can explain why the two *H~exp~*-based models, *SSP/MSD~exp~*, did not contribute at all with zero weight. Looking at the model fits in Subsection  -->
<!-- \@ref(sec:results2) (ii),  -->
<!-- <!-- \@ref(sec:posterior2),  --> 
<!-- it stands to reason that the main apparent advantage of the *H~exp~* sonority hierarchy over the *H~col~* sonority hierarchy in predicting onset rises is already subsumed by *NAP~td~*'s predictions, such that *SSP/MSD~exp~* had no unique contribution to add when stacked alongside superior *SSP/MSD~col~* and *NAP~td~* together. -->

<!-- The fact that the highest ranked NAP model in this comparison, *NAP~td~*, did not completely neutralize the contribution of *NAP~bu~* is telling, especially given that *NAP~td~* subsumed more of the traditional models, although it is more similar to *NAP~bu~* in principle. This is evident from Table \@ref(tab:resultsmodelsohnenapbu), in which we took *NAP~bu~* out of the equation, resulting in 88% contribution by *NAP~td~* and only 3%  contribution by *SSP~col~*. -->


### Results discussion {#sec:discussionResults} 

The results of the confirmatory studies, Experiments 2-3, can be summarized as follows: 
(i) all of the sonority models we tested are capable of explaining the response time data for different consonant clusters to some good extent; 
(ii) the symbolic top-down NAP model, *NAP~td~*, outperforms all the the other models;
(iii) some interesting differences between *H~col~* and *H~exp~* sonority hierarchies were observed, where the advantages of the minimal *H~col~* sonority hierarchy proved to be more effective.

Experiment 3 exhibits most of the general trends found in Experiment 2, albeit in a less compelling way. One path of explanation for these discrepancies can be found in the differences between the ambient languages. We expect language-specific differences to account for some of the differences between the experiments, as was mentioned in Subsection \@ref(sec:designs). Specifically, the differences between the nasals in Hebrew and German was suggested as explanatory in Subsection \@ref(sec:discussionFits).

On top of that, we suspect that differences between the experiments were also due to the various sources of noise that were introduced in the process. These include the smaller group of participants and the diverse physical locations in which Experiment 3 was administered (see Subsection \@ref(sec:participants3)). 
<!-- The results confirm this with a wider standard error for the intercepts of the different subjects in the statistical models in Experiment 3.  -->
The results may be taken to support this with a larger standard deviation for the by-subject adjustments to the intercept for the models of Experiment 3 in comparison with Experiment 2 (e.g., $\hat\sigma_\alpha = 0.32~[0.25, 0.41]$ in Experiment 3 vs. $\hat\sigma_\alpha = 0.21~[0.17, 0.26]$ in Experiment 2, when comparing the null models).

Furthermore, it was the impression of the experimenter (the first author) that Hebrew speakers in this study tended to have a less robust understanding of the notion of the syllable such that their performance may exhibit higher degrees of misunderstanding of the the meta-linguistic syllable count task. This impression seems consistent with the overall smaller range of response times in Experiment 3, whereby Hebrew speakers tended to respond relatively fast to ill-formed structures. 

The success of our NAP models relative to the traditional models in predicting the data can be mainly attributed to the following traits of NAP:
(i) all the voiceless-initial onset clusters, including onset falls and plateaus (e.g., /sp/ and /sf/), are relatively well-formed in NAP, correctly predicting the patterning together of such data at the low-right parts of the plots (faster response times);
(ii) onset rises like /ml/, nasal plateaus (/nm/ and /mn/), and onset falls like /lm/ pattern together as similar and relatively ill-formed in NAP, correctly predicting the data, whereby sonorant-initial plateaus and rises do not tend to pattern with (better-formed) obstruent-initial plateaus and rises.

<!-- *NAP~bu~* -->
A superficial formal generalization that can illustrate these results in symbolic terms may be that the sonority *intercept* of onset clusters appears to be not less impactful than the sonority *slope* in determining syllabic well-formedness (i.e., the starting level of the onset cluster is not less predictive of well-formedness than the angle of the cluster's slope).

#		General Discussion {#sec:genDiscussion}

Our experimental results provide strong support for our synergy of proposals, including our choice of sonority's perceptual basis and acoustic correlate, the incorporation of continuous entities and in phonological models, and the dual-route modeling strategy that accounts for both top-down and bottom-up inferences with separate compatible machinery. The following subsections discuss  important implications that are borne out of our interpretation of the results: 
(i) in  Subsection \@ref(sec:division) we discuss a potential phonotactic division of labor, demonstrated with a more holistic account of /s/-stop clusters;
(ii) in Subsection \@ref(sec:lingMod) we discusses the current contribution in the context of the tension between *discrete* and *continous* in models of phonology; and
(iii) in Subsection \@ref(sec:projection) we discuss the universality of sonority, with special attention to the body of works by Iris Berent and her colleagues.
<!-- , as well as to the works of the physicist and theoretical biologist, Howard Pattee  -->
<!-- (Subsection \@ref(sec:compofmind)). -->

##		Phonotactic Division of Labor {#sec:division} 

Sonority has been widely used to explain practically any type of phonotactic phenomenon since there is nothing in the standard theory that commits the formal conception of sonority to any specific effect in perception or articulation of speech. The position taken in this work is very different, drawing explicit links between sonority and auditory perception of pitch. As a result, sonority in this work is a narrower concept with limits that can be more carefully considered. This is important since it is very unlikely that all the different phonotactic phenomena stem from a single driving force. A more well-defined notion of sonority therefore allows us to achieve a better understanding of the phonotactic division of labor between different forces that play a role in the processing of speech.
As we shall see below, */s/-stop* clusters make a good case in point. 

###   /s/-stop clusters in traditional accounts

One of the most well-studied problems in the empirical coverage of all traditional sonority principles concerns sequences that are often termed */s/-stop clusters*, referring to cases where a sibilant fricative---most often /s/---precedes a stop consonant, like in the English words ***st**op*, ***sk**y* and ***sp**ort* 
[see, e.g., @yavacs2008sonority; @vaux2009append; @goad2011representation; @goad2016sonority]. The sonority slope of */s/-stop* clusters is either an onset fall or an onset plateau, depending on the given sonority hierarchy. When fricatives and stops are assigned to two distinct classes on a give sonority hierarchy, */s/-stop* clusters exhibit onset sonority fall. If, on the other hand, stops and fricatives are grouped together, */s/-stop* clusters are considered plateaus.
Thus, although */s/-stop* clusters are relatively common in languages that tolerate sequences and should therefore be considered as relatively well-formed [@morelli2003relative; @steriade1999alternativessk], */s/-stop* clusters are predicted to be rare, or even extremely rare, by virtue of their ill-formed sonority slopes.

Rather than redefining sonority principles to be able to account for the phenomenon of */s/-stop* clusters, more successful attempts to solve this problem in the phonological literature redefined "misbehaved" marginal sibilants as exceptional, keeping the traditional sonority principles unaffected by their consistent failure to predict the attested relative well-formedness of */s/-stop* clusters.
The main type of exception that is used to explain sibilant-initial clusters is based on tweaking symbolic representations by removing the symbol of the marginal sibilant segment outside of the syllable that contains the following consonant such that---in theory---there is no tautosyllabic complex cluster to trigger sonority restrictions in the first place [see, e.g., @steriade1982greek; @kaye1992you; @rialland1994phonologysk; @vaux2009append]. 
A slightly different theoretical solution with similar results is to assert that */s/-stop* clusters are, in fact, a single complex segment [see, e.g., @fudge1969syllables; @weijer1996segmental] such that, again, there simply is no cluster to solve in theoretical accounts [for an overview, see @goad2016sonority].

Those theoretical tweaks are not without merit as they follow a strong intuition that marginal sibilants are somehow "outside" the scope of syllabic processes. This intuition is supported by evidence of some unique behaviors of marginal sibilants, such as the kinematic data presented in @hermes2013phonologysk, which finds unique coordination patterns in articulation of sibilant-initial consonantal gestures in Italian (suggesting that the sibilant is outside the syllable). 
We do not have enough data on the subject to make any determination yet, but these findings are so far not inconsistent with NAP-based accounts. Periodic energy has prosodic roles to play in carrying the pitch and the overall prosodic strength at the nucleus of the syllable while marginal voiceless elements might be timed with different considerations. For example, it may be beneficial to prolong marginal voiceless elements to increase their recoverability without the risk of increased nucleus competition.

Eventually, it is important to remember that the problem with */s/-stop* clusters is not that they are common, and, at the same time, unique. The problem with */s/-stop* clusters is that traditional sonority principles fail to account for them in consistent manners, without resorting to exceptions. 

###   Towards a holistic account of /s/-stop clusters

NAP's account of the well-formedness of */s/-stop* clusters does not suffice to explain this phonotactic phenomenon since there is nothing in NAP that is specific to sibilants or stops that would make the specific combination of a sibilant and a following stop consonant stand out. In fact, any voiceless element is practically invisible to NAP as it is only sensitive to portions of the speech signal that contain sufficient periodic energy. Indeed, the predictions of NAP, which were corroborated in the experimental results (see Subsection \@ref(sec:results)), expect non-sibilant counterparts of /s/, like /f/ in the cluster *ftV*, to pattern with *spV* and *ʃpV*. Furthermore, *NAP~bu~* successfully predicted that all the voiceless-initial clusters in the experiment---including the */s/-stop* clusters---generally pattern together as well-formed, as far as sonority-based restrictions are concerned.
This may suffice to explain why */s/-stop* clusters are tolerated, but not why they are so often preferred over other obstruent combinations. The complete phonotactic story of */s/-stop* clusters thus requires an integrative explanation, in which sonority only plays a limited role. 

First, there are various reasons to assume that *fricative-stop* clusters are better-formed than *stop-stop* clusters. This generalization is traditionally captured in abstract formal phonological constraints like the *Obligatory Contour Principle* [OCP; going back to @leben1973suprasegmental; and @goldsmith1976autosegmental], which acts as a general dissimilatory requirement banning two successive units of the same type. The OCP in this case may be the reflection of an articulatory disadvantage of the *stop-stop* configuration since it should be harder to coordinate two successive closure and release gestures, whereby air pressure is built twice within the span of a complex onset.

Note that this also leads to a disadvantage of *stop-stop* from a perceptual point of view, since the first stop in a *stop-stop* configuration is released into the closure phase of the following stop [see @surprenant1998perception]. The release of a stop burst into a silent closure phase of another stop, rather than the periodic signal of a vowel, means that many of the acoustic cues to the identity of the first stop consonant are severely attenuated [see @fujimura1978perception]. 

The last explanation is essentially based on perceptual *cue robustness* [@wright2004review], which is not directly relevant to syllabic organization in any abstract form as much as it is based on adjacency between speech sounds and their chances of being recovered given transitions between them. As @kawasaki1997alternativessk [361] concluded, rather than sonority and syllabicity, it is "the degree of salience of modulations created by segmental transitions", which is the determinant factor of many phonotactics.

Wright's [-@wright2004review] *cue robustness* 
is also critical for the remaining explanation regarding the phonotactic advantage of */s/-stop* over comparable non-sibilant *fricative-stop* clusters, e.g., *spV* vs. *ftV*.
Here, the notion of cue robustness serves to explain why sibilants, with their salient and distinctive high frequency aperiodic energy, can stand out more than other fricatives, thus allowing more effective recoverability from relatively weak marginal positions (i.e., distant from the vocalic nucleus). 

The three phonotactic perspectives are complementary, and although they do not represent an exhaustive list of phonotactic pressures, we need at least these three---*sonority*, *cue robustness*, and *articulatory dissimilation*---in order to properly appreciate the phonotactic phenomenon of */s/-stop* clusters. According to this more holistic account, */s/-stop* clusters are relatively well-formed in terms of sonority because they are not competing for the nucleus, they are well-formed in terms of articulatory coordination complexity due to the two dissimilar successive gestures, where the stop is released into the following vowel, and, finally, they are robust in terms of their acoustic cues: 
stops in C~2~ can be released into a vowel to optimize the effect of the burst in the release phase, while sibilants retain strong cues to their identity thanks to their unique spectral profile, which helps them stand out even in C~1~ positions of complex onset clusters.

##     Reshuffling Dichotomies in Linguistic Models {#sec:lingMod}
<!-- ##     Linguistic Models: Between Discreteness and Continuity {#sec:lingMod} -->

The study of the sound system of human languages has been one of the longest-standing intersections of symbol-based categorical analyses on the one hand, and signal-based continuous descriptions on the other. These two different types of analysis stand at the core of the distinction many researchers make between *phonetics* and *phonology*, where the former addresses continuous and measurable aspects of the speech signal (namely, sensorimotor aspects of articulation, acoustic signals and neurological patterns in perception), while the latter addresses categorical aspects of the speech signal using discrete and symbolic units like *consonants*, *vowels* and *phonemes* [see overviews in @harris2007representation; @ladd2011phonetics].

The incompatibility between the continuous and the discrete types of description did not escape early studies. @menzerath1933koartikulationsk; @wickelgren1969context and @fowler1980coarticulation noted how the reality of co-articulation of segments defies the idealized conception of speech signals as consisting of a sequence of non-overlapping discrete phonemes [see @halle1954strategy]. @warren1982auditory [172-187] also provides an overview of this problem given the limitations of auditory perception.

### Connectionism {#sec:connectionism}

The *connectionist* program in cognitive psychology [e.g., @rumelhart1986parallel1; @rumelhart1986parallel2; @bates1993counectionism] was set to change that classic view with the introduction of connectionist models to phonology [e.g., @goldsmith1992local; @laks1995connectionistsk; @joanisse2000connectionist; @smolensky2006harmonic; @tupper2012sonoritysk]. These models replaced classic symbolic models with *neuromimetic* models [@laks1995connectionistsk 52] that attempt to improve the cognitive plausibility of language models with architectures that resemble neurobiological systems.

It should be noted, though, that the main focus of connectionist models is not so much the symbols in the system as much as it is the classic processes of symbol manipulation. Connectionist models present alternatives to traditional symbol manipulation rules, like the linear rules in generative phonology [@chomsky1968spesk]. The latter are practical in terms of the descriptive notation, but they cannot (and mostly do not) attempt to reflect the cognitive functions of the brain.

In the context of the present work, connectionist models can be effective in modeling the phonology of speech perception in a top-down model, which represents processes that start and end with discrete symbols. However, we assume that there is a functional source for linguistic distinctions in perception, which should be searched for in the bottom-up route, where processes must begin with continuous events in real time.

###    Dynamical systems {#sec:synthesis}

The introduction of dynamical systems to phonology [see @haken1985theoretical; @browman1992articulatorysk; @goldstein2009coupled; @nam2009self] made it possible to model the interactions between continuous aspects of the speech signal itself (e.g., motor gestures in articulation) and the discrete symbolic categories that linguists assume. One way to achieve this is via attractor landscape models, where discrete categorical units can be modeled as attractor basins [see @haken1990synergeticssk]. In this type of models, various continuous events can contribute more or less to the (partial) activation of different, often competing, attractors.
Good examples for application of attractor landscapes in such manners can be found in @tuller1994nonlinear; @case1995evaluation; @raczaszek1999categorization; @gafosbenus2006dynamics; @roessig2019modeling; and @roessig2019dynamics.

Attractor landscape models are therefore an essential component of a language model that needs to represent the interaction between continuous events and discrete entities. However, much like the connectionist models in phonology, attractor landscapes explicate the process by which a discrete alternative can be (partially) activated, but they have little to say about the components of the system otherwise.
For example, attractor landscape models cannot explain or predict the shape and behavior of the attractor landscape itself, they cannot address the limitations on dynamic events that the system can reliably detect, and they pose no restrictions on what a valid symbol is.

The phonological theory saw dynamical systems playing an increasingly more important role with the growing body of work coming from the school of *Articulatory Phonology*.
Applying similar concepts to perception has been thus far a less productive avenue in phonology [although see @tuller1994nonlinear; @case1995evaluation; @hock2003dynamical; @tuller2004categorization; @tuller2008dynamical; and @lancia2013interaction], perhaps because it is much less clear what are the relevant continuous entities that we need to model in perception.

###   The complementarity of mind {#sec:compofmind}

The relationship in this work between the discrete and symbolic modes of language on the one hand, and the continuous and dynamic modes on the other hand, is consistent with the works of physicist and theoretical biologist, Howard Pattee, whose insights have been previously extended to include cognition and human language [see @cariani2001symbols; @raczaszek2008reconciling; and see @pattee2012lawssk for a collection of Pattee's classic papers with contemporary commentary].
In Pattee's original studies, the interaction between symbolic and dynamic modes was related to symbolic aspects of the DNA and their relationship with the dynamics of proteins. For Pattee, the symbolic and dynamic modes of biological systems are two critical components with specific roles to play: symbols are the stable forms that *harness* dynamic events. Symbols, according to @pattee1987instabilities [337], cannot exist outside of a dynamic system that they *constrain*.
It is therefore pertinent to understand symbols in language systems with respect to the continuous events that they relate to. In our interpretation, this means that to fully understand sonority we need to address its potential functions in auditory perception and cognition, and their effects in linguistic communication. Although tightly related to top-down symbol-based generalizations, this bottom-up route is a separate process that is based on different driving forces (e.g., laws of physics rather than statistics).

##		The Universality of Sonority {#sec:projection}

A consistent interest within theoretical phonology concerns the universality of sonority-based principles. An impressive volume of publications devoted to this question can be found in the works of Iris Berent and her colleagues, starting with @berent2007we, and followed by many subsequent studies.[^cf-berentslist]
Berent et al. collected mostly behavioral data from perception tasks, where subjects of various different language backgrounds were found to adhere to the SSP, even when presented with combinations that are not attested in their language. The patterns under Berent's consistent scrutiny are usually limited to a set of initial clusters with an onset rise (e.g., *blif*), an onset plateau (e.g., *bdif*) and an onset fall (e.g., *lbif*). Since /s/-clusters and sonorant plateaus are absent from these studies, Berent's experimental results with SSP-based models are largely compatible with NAP, where the hierarchy *blif* (3) $>$ *bdif* (2) $>$ *lbif* (0) is maintained in *NAP~td~* (model scores in brackets).[^cf-napbucant] 

Berent and her colleagues present these findings in support of the innateness hypothesis, assuming that all humans share a universal linguistic knowledge, which is genetically encoded (the *Universal Grammar* in generative traditions). 
The universality of sonority principles thus implies innate knowledge of ordinal sonority hierarchies that map onto a discrete representation of the speech signal, with mechanisms that compute the sonority slopes within syllables to determine well-formedness.

The interpretation of Berent's findings has been a matter of interest in the literature. Some responses, like @daland2011explaining and @hayes2011interpreting, have argued that the universal phonotactic behaviors that Berent et al. present can be shown to result from speakers' ability to generalize categories and distributions from the attested lexicon, and use analogy and probabilities to predict unattested forms. 
Such models can successfully apply statistical learning methods based on the lexicon, without a requirement for prior formal knowledge of sonority [e.g., @jurafsky2009speech; @coleman1997stochastic; @vitevitch2004webbasedsk; @hayes2008maximum; @hayes2011interpreting; @bailey2001determinants; @albright2009feature; @futrell2017generative; @jarosz2017inputsk; @mayer2019phonotacticsk; and @mirea2019usingsk]. 

Interestingly, connectionist models, such as @goldsmith1992local, @laks1995connectionistsk @smolensky2014optimization and @tupper2012sonoritysk, also seem to be quite compatible with what is considered here as the symbolic version of NAP. Connectionist models were historically related to an opposition to the classic symbol-based models (see Subsection \@ref(sec:connectionism)). However, the inputs and outputs of these models are expressed in discrete symbols and they are designed to capture generalizations in terms of the weights of connections in the system, which may, indeed, serve as a good mechanistic description of top-down operations. 
Likewise, many other models that successfully describe phonotactic knowledge, such as the Bayesian models in @wilson2013bayesian; and @wilson2014effects, are essentially describing what we consider here to be the top-down mode of the system, which is somewhat disconnected from the physical and cognitive sources of phonotactic phenomena.

In contrast to traditional sonority principles, NAP was designed to be compatible with general cognitive processes and auditory perception, such that no unique assumptions are required for innate formal knowledge of sonority. 
Sonority-based patterns in NAP arise from the general cognitive process that underlies the parsing of the stream of speech into syllables with a pitch-bearing nucleus (i.e., nucleus competition). This requirement for pitch-bearing units may be explained in evolutionary timescales as the inevitable result of the important role of pitch in speech communication [@bolinger1978intonation; @house1990tonal; @cutler1997prosody; @roettger2019tune] given the observation that tune-text integration occurs with syllable-sized units [e.g., @goldsmith1976autosegmental; @liberman1975intonationalsk; @pierrehumbert1980phoneticssk; @ladd2008intonational]. 

The NAP approach appears capable of synthesizing the different views on the origins of universal sonority. 
<!-- Like many of the works by Berent and her colleagues,  -->
The bottom-up model of NAP can explain the universality of sonority as the natural development of communication systems that exploit the auditory effects of pitch. The top-down model of NAP is, at the same time, very much in line with phonotactic learners, where it is possible to learn the regularities of language from the symbolic abstractions that reflect the speakers' knowledge in stable forms. 
Top-down inferences reflect the history of the distribution of recognized symbols as they appear in the lexicon of the ambient language, only indirectly expressing the functional aspects that we see in the bottom-up route since they reflect the surface manifestations of the functionally-motivated (bottom-up) dynamics.

To conclude, bottom-up NAP subsumes the innateness claims for formal sonority universals with a more general explanation that is based on the workings of the perceptual and cognitive systems and the evolution of languages as pitch-bearing communication systems. At the same time, top-down NAP is in-line with the rationale of statistical learners and the mechanics of connectionist models, which require symbolic interpretation of the signal in order to abstract variable dynamic events with stable forms (e.g., consonants, vowels, phonological features) and generalize over their distributions.

# Conclusion  {#sec:conclusions}

This work suggests a synergy of novel theoretical and methodological approaches in an attempt to shed new light on old problems in linguistics. 
The novelties that are proposed here for models of sonority, and more generally, for  perception-based models in phonology, will need to amass more supporting evidence from multiple sources in order to be more widely considered, and further developed. 
We hope that in this paper we laid the foundations for such potential long-term contributions. 

In a related work, Anonymous (in press) tests sonority models on a diachronic corpus data of Modern Hebrew, which uses general phonemic transcriptions. In such common cases, when linguistic data is already abstracted into stable forms, only the symbolic NAP model is of relevance, and, once again, *NAP~td~* outperformed traditional SSP-based accounts.
In these works, we demonstrated how our set of proposals results in a model of sonority that can account for some of the most persistent problems in phonological theory. 
Our NAP-based models not only present advantages over traditional sonority models in terms of empirical coverage, they also provide functional explanations as to the source and cause of sonority phenomena, linking sonority to pitch intelligibility at the level of the syllable. 

Furthermore, our dual-route modeling strategy (see Subsection \@ref(sec:complementary)) 
makes the important distinction between (bottom-up) signal-based inferences and (top-down) symbol-based inferences, thus appropriately predicting the complementary contribution of these two essentially different inference routes of the same linguistic phenomena.

The smaller success of the continuous *NAP~bu~* model compared to its symbolic version, *NAP~td~*, is, in fact, still quite impressive. *NAP~bu~*, as it was implemented here, analyzing well-formedness directly from the continuous acoustic signal, is already as good as the best traditional models, and it still has many potential paths for improvement. 
For example, *NAP~bu~* can directly benefit from independent improvements in extraction of periodic energy in digital signal processing, or from improvements in the conception of competition, and the formulas used to estimate or simulate it. We therefore interpret the results of *NAP~bu~* as very promising.

Lastly, this study also provides compelling motivations and strong evidence that support our proposal to link the notion of sonority with periodic energy in the acoustic signal. This proposal entails that the role of periodic energy is of great importance to linguistic analysis of speech, beyond the scope of the current proposal. A partial list of relevant topics includes the following:
(i) acoustic descriptions of prosodic *weight* and prosodic *prominence* in terms of periodic energy mass;
(ii) automatic syllabification procedures based on the smoothed fluctuation rates of the periodic energy curve [useful for a myriad of tasks, including speech rhythmicity studies; see, e.g., @galves2002sonoritysk; @tilsen2013speech; @rasanen2018pre; @lin2020hit]; and
(iii) improved visualization and analysis of pitch contours in intonation research based on the interaction of the periodic energy and the F0 trajectories 
(see Anonymous 2018; Anonymous 2019; and Anonymous 2020 for continuous visualization and quantification procedures in intonational phonology using periodic energy).
<!-- [see @albert2018using; @cangemi2019modellingsk; and @albert2020propersk for continuous visualization and quantification procedures in intonational phonology using periodic energy]. -->

<!-- Given the above, we believe that the new approaches presented in this paper will be able to deliver many more valuable insights into old and new problems in phonology and linguistic theory. -->




<!-- --- -->

<!-- # CRediT authorship contribution statement {-} -->
<!-- **AA**: Conceptualization, Methodology, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision. **BN**: Conceptualization, Methodology, Formal Analysis, Writing - Review & Editing, Visualization.  -->

<!-- # Acknowledgements {-} -->
<!-- This work was supported by the DAAD (German Academic Exchange Service) and SFB 1252 Collaborative Research Center “Prominence in Language". -->

<!-- # Note {-} -->
<!-- All materials associated with the current article are available on *Open Science Framework* (OSF) at -->
<!-- https://tinyurl.com/5jxmjntx -->

<!-- footnotes -->

[^cf-liquid]: The group of *liquids* is the most loosely defined, as it includes both *lateral approximants* (namely /l/) and various types of rhotics such as *trills* (/r,ʀ,ʁ/), *taps* (namely /ɾ/), and alveolar and retroflex *approximants* (/ɹ,ɻ/). 
<!-- In this paper we only consider the lateral approximant /l/ as the relatively more stable and common token of this varied liquid type. -->

[^cf-sdp]: The *Sonority Dispersion Principle* [SDP; @clements1990role; @clements1992sonority] is a slightly different yet related principle that prefers onset rises with large distance and equal dispersion of sonority index values across the consonantal sequence and the following vowel. The results of the SDP are highly contingent on the given sonority hierarchy and it is not very clear how to apply the SDP with onset sonority falls [among other problems listed in @parker2002quantifying 22--24]. 

[^cf-list]: A partial list of some prominent examples includes @sigurd1955rank; @jakobson1956fundamentals; @chomsky1968spesk; @foley1972rule; @ladefoged1971preliminaries; @allen1973accentsk; @fujimura1975syllable; @Donegan1978onthenatural; @ultan1978typological; @price1980sonority; @lindblom1983production; @anderson1986suprasegmental; @vennemann1988preferencesk; @levitt1991syllable; @pierrehumbert1992lenition; @fujimura1997acoustic; @stemberger1997handbook; @boersma1998functional; @zhang2001effects; @howe2004harmonic; @clements2009does; @sharma2018significance.

[^cf-mora]: Moras are used to represent quantitative differences between light and heavy syllables (weight sensitivity), such that light syllables contain one mora while heavier syllables contain two (and sometimes even three) moras [see @hyman1984atheory; @mccarthy1990footsk; @hayes1989compensatory; @ito1989prosodic; @zec1995sonority; @zec2003prosodic].

<!-- [^cf-helsinki]: Informed consent from the participants was obtained before each experimental session. The experiment complied with the June 1964 Declaration of Helsinki (carried out by the World Medical Association and entitled “Ethical Principles for Medical Research Involving Human Subjects”), as last revised. In accordance with German Research Foundation (DFG) guidelines for experiments with unimpaired adult populations, the ethics approval is required by the Principal Investigator (in this case, Prof. Dr. Martine Grice). -->

[^cf-plateaufall]: Depending on whether fricatives are considered higher or similar in sonority to stops, clusters of the type fricative-stop may be considered as either an onset fall or an onset plateau.

<!-- [^cf-sstop]: The term */s/-stop cluster* is used here to refer to voiceless sequences of a sibilant, namely /s/ or /ʃ/, that precedes an oral stop, e.g., /p,t,k/. -->

<!-- [^cf-td]: Note that language-specific top-down biases, such as the question of syllabic consonants in a given language, are not covered by our top-down model, *NAP~td~*, which is limited to the universal aspects of NAP. That said, *NAP~td~* succeeded in fitting the data for /z/-initial and nasal-initial clusters within its predictions. -->

[^cf-berentslist]: For example, @berent2008language; @berent2011syllable; @berent2012language; @berent2012universalsk; @berent2013phnological; @berent2014languagesk; @gomez2014language; @zhao2015universalsk; @berent2015role; @lennertz2015onthesonority; @berent2017origins. 

[^cf-napbucant]: *NAP~bu~* cannot make such determinations based on symbolic representations, but it should be expected to generally follow the same trends in the vast majority of cases.


\newpage

# References
```{r create_r-references}
r_refs(file = "bibs/r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_a.Rmd') 
```

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_b.Rmd')
```