---
title             : "The double life of language systems: Modelling sonority with complementary symbol and signal based models"
shorttitle        : "Modelling sonority with complementary models"

author: 
  - name          : "Anonymous"

affiliation:
  - institution   : "Affiliation"

authornote: |
  Author Note: In accordance with the Peer Reviewers' Openness Initiative (opennessinitiative.org), all materials and scripts associated with this manuscript were made available during the review process and will remain available (see anonymized link: https://tinyurl.com/5jxmjntx).

abstract: |
  *Sonority* is a fundamental notion in phonetics and phonology, central to many descriptions of the syllable and evidently useful as a predictor of phonotactics.
  Although widely-accepted, sonority lacks a clear basis in speech articulation or perception, and traditional sonority principles, which were not designed to be compatible with cognitive capacities, exhibit systematic gaps in empirical coverage. Traditional sonority models have been exclusively based on discrete and symbolic machinery.
  Against this backdrop, we propose an incorporation of symbol-based and signal-based models to adequately account for sonority with two complementary models.
  We claim that sonority is primarily a perceptual phenomenon related to pitch, driving the optimization of syllables as pitch-bearing units. We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we provide a novel principle that can account for syllabic well-formedness, the *Nucleus Attraction Principle* (NAP).
  We present two perception experiments that test our two NAP-based models against four traditional sonority models and we use a Bayesian data analysis approach to test and compare them. We show that our two NAP models retain the highest degree of complementarity while one of them is superior to all the other models we tested. 
  We interpret the results as providing strong support for our proposals:
  (i) the designation of periodic energy as sonority's correlate; 
  (ii) the incorporation of continuity in phonological models, and; 
  (iii) the dual- model strategy that separates and integrates symbol-based top-down processes and signal-based bottom-up processes.
  
keywords          : "Sonority; Periodic energy; Bayesian data analysis; Phonetics and Phonology"
wordcount         : "X"

bibliography      : ["bibs/r-references.bib", "bibs/methods.bib", "bibs/phon.bib", "bibs/phon_sk.bib"]
appendix:
  - "./appendix_a.Rmd"
  - "./appendix_b.Rmd"
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : yes

documentclass     : "apa6"
classoption       : "doc"
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex
    includes:
      in_header: load.tex
keep_tex: yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r knitush, cache=FALSE,include=FALSE}
# global chunk options
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE,fig.path='figure/graphics-', fig.align='center')#, dev="cairo_pdf")
```

```{r libraries, message = FALSE}
library(R.matlab)
library(ggplot2)
library(dplyr)
library(Cairo)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores =  parallel::detectCores())
library(stringr)
library(readr)
library(purrr)
library(tidyr)
library(loo)
library(brms)
library(ggrepel)
```


```{r prepare-mat-per, include=FALSE}
# 60 x length of the audio file binned each 10 ms; 60 frequency bins with 10 ms for each column 
dir_mats <- c("data_tables/APPd_txt_matrices/AA/",
              "data_tables/APPd_txt_matrices/HN/")
files_mat <- list.files(path=dir_mats, pattern="*.txt",full.names=TRUE)

#creates a dataframe with 3 columns, the name of the syllable, syl, the time point, t, and the periodic energy, p
per_df <- map_dfr(files_mat, function(f){
#this is like a loop, it takes each file inside the f and does the following:
    # Read the file
    mat <- read_csv(f,col_names = FALSE,
                    col_types = cols(.default = col_double()))
    #Sum of every column of the matrix
   # vector_weights <- c(rep(1,8),rep(1.1,2),rep(1.3,3),rep(1.4,4),rep(1.5,5), rep(1.4,3), rep(1.3,5),rep(1.2,6),rep(1.1,10),rep(1,15))
    #mat <- mat * vector_weights
    per <- colSums(mat)
    #Extract the name of the syllable form the filename: looks for //(syllable).
    filename <- str_match(f,"//(.*?)_(.*?)\\.")
    syl <- filename[,2] 
    speaker <- filename[,3] 
    tibble(syl=syl,t=(0:(length(per)-1))*10,per=per,speaker=speaker)
})
# Periodic energy at every time point
per_df_full <-  per_df %>%  group_by(syl,speaker) %>% 
                mutate(smooth_per = unclass(smooth(per,"3RS3R"))) 
#head(per_df_full)

# ```
# 
# ```{r prepare-seg, include=FALSE}

dir_seg <- c("data_tables/praat_seg/AA/",
             "data_tables/praat_seg/HN/")

files_praat <- list.files(path=dir_seg, pattern="*.txt",full.names=T)
seg_df <- map_dfr(files_praat, function(f){  
#this is like a loop, it takes each file inside the f and does the following:

    # filename <- str_match(f,"/([^/]*?)_(.*?)_(.*?)\\.")
    filename <- str_match(f,"//(.*?)_(.*?)_(.*?)\\.")
    seg <- read_tsv(f, col_types =cols(
                          rowLabel = col_character(),
                          tmin = col_double(),
                          text = col_double(),
                          tmax = col_double()
                      )) %>% select(-rowLabel) %>%
        mutate(syl = filename[,2],
               speaker = filename[,3],
               text = str_extract_all(syl, ".")[[1]],
               position=row_number(),
               t = map2(tmin,tmax, ~ round(seq(.x,.y,.01)*1000))               ) %>%
        tidyr::unnest(cols = c(t)) %>%
        select(-tmin, -tmax)
})

syl_info <- left_join(per_df_full,seg_df,by = c("syl", "t", "speaker"))

## head(syl_info)

## syl       t   per speaker smooth_per text  position
## <chr> <dbl> <dbl> <chr>        <dbl> <chr>    <int>
## 1 cefal     0  0    AA            0    c            1
## 2 cefal    10  2.37 AA            2.37 c            1
## 3 cefal    20  4.97 AA            4.97 c            1
## 4 cefal    30  5.80 AA            4.99 c            1
## 5 cefal    40  4.99 AA            4.99 c            1
## 6 cefal    50  4.06 AA            4.06 c            1
# ```
# 
# ```{r log-transform, include=FALSE}

subset_voiceless_thresh <- filter(syl_info,
                                  #nchar(as.character(syl))>=4,
                                  position==1,
                                  syl %in% c("cfal","cpal","fsal","ftal","sfal","spal"))
per_thresh <- max(subset_voiceless_thresh$per)

syl_info <- group_by(syl_info,syl,speaker) %>%
    mutate(log_per = ifelse(smooth_per<per_thresh, 0,
                            10*log10(smooth_per/per_thresh)))

## print(syl_info,n=100)

# ```
# 
# ```{r, cog}

syl_info <- syl_info %>% group_by(syl, speaker) %>%
    mutate(com_syl = sum(log_per*t)/sum(log_per), # position of CoM of the whole syllable in time
           t_left_syl = ifelse(t <= com_syl,t,0),
           com_onset = sum(log_per*t_left_syl)/sum(log_per*(t_left_syl>0)),
           NAP_bu = -(com_syl - com_onset), #flipped sign
           NAP_bu_rel = -(com_syl - com_onset)/com_syl) %>%
    select(-t_left_syl)%>%
    select(NAP_bu, everything())
# ```
# 
# ```{r, monosyl, include=FALSE}

monosyl_info_AA <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "AA")

monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "ʃ"
# monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "U+0283"
# monosyl_info_AA$text[which(monosyl_info_AA$text=="c")] <- "\textesh"


monosyl_info_AA$syl <- as.factor(monosyl_info_AA$syl)
monosyl_info_AA <- mutate(group_by(monosyl_info_AA,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info_AA$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info_AA$smog_per[(monosyl_info_AA$smog_per<0)]=0

monosyl_info_AA <- mutate(group_by(monosyl_info_AA, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info_AA <- mutate(group_by(monosyl_info_AA, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```

```{r monosylHN, include=FALSE}

monosyl_info_HN <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "HN")

monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "ʃ"
# monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "U+0283"
# monosyl_info_HN$text[which(monosyl_info_HN$text=="c")] <- "\textesh"


monosyl_info_HN$syl <- as.factor(monosyl_info_HN$syl)
monosyl_info_HN <- mutate(group_by(monosyl_info_HN,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info_HN$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info_HN$smog_per[(monosyl_info_HN$smog_per<0)]=0

monosyl_info_HN <- mutate(group_by(monosyl_info_HN, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info_HN <- mutate(group_by(monosyl_info_HN, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```


<!-- ## Data analysis {#sec:datanlysis} -->

```{r setZeroRT, include=FALSE, message = FALSE}
syl_t <- filter(syl_info) %>%
  # Aviad: no idea why we are subsetting by "a"
    filter(text=="a" ) %>%
    group_by(NAP_bu, syl, speaker) %>%
    summarize(tmin = min(t))

other_scores <- read_tsv(file="data_tables/CCal_model_predictions_fix.tsv", 
                           col_types = cols(
                               syl = col_character(),
                               SSP = col_integer(),
                               SSP_obs = col_integer(),
                               MSD = col_integer(),
                               MSD_obs = col_integer(),
                               NAP_td = col_integer()
                           ))

scores_full <- left_join(syl_t, other_scores) %>% ungroup() %>%
    mutate(NAP_bu = ifelse(nchar(syl)<5,NAP_bu,NA_real_))
```

```{r openSesameFunction, include=FALSE}

read_list_opensesame <- function(list_files, speaker = "AA", scores_tbl= scores_full){

    GLIDES <- c("wlal","wnal","wzal","wsal","wtal","jmal","jval","jfal","jpal",               "welal","wenal","wezal","wesal","wetal","jemal","jeval","jefal","jepal")

    scores_speaker <- scores_tbl[scores_tbl$speaker == speaker,]

    map_dfr(list_files, ~{
        message(.x)
       suppressMessages (read_csv(.x)) %>%
            filter(practice =="no") %>%
           mutate(subj = str_match(logfile, '-([0-9]*)\\.csv')[,2],
                   RT = response_time /1000) %>%
            select(subj, stimulus,RT,correct) })%>%
        mutate(stimulus = str_replace_all(stimulus, 'ʃ','c')) %>% # ʃ
        filter(!stimulus %in% GLIDES) %>% left_join(scores_speaker,by=c("stimulus"="syl")) %>%
        mutate(corrRT = (RT - tmin/1000) * 1000, #in milliseconds
               type = factor(ifelse(nchar(stimulus)==4,"CCal",
                             ifelse(str_detect(stimulus,"^e.*" ),"eCCal","CeCal")),
                             levels=c("CeCal","eCCal","CCal")),
               response = case_when(RT>=3 ~ 0,
                                    correct ==1 & type == "CCal"  | correct ==0 & type != "CCal" ~ 1,
                                    TRUE ~ 2))
}
```

```{r readPilot, include=FALSE}

opensesame_pilot <- "data_tables/exploratry_results"

files_pilot <-
    c(list.files(path=paste0(opensesame_pilot,"/list1"), pattern="*.csv",full.names = TRUE),
      list.files(path=paste0(opensesame_pilot,"/list2"), pattern="*.csv",full.names = TRUE))

data_pilot_all <- read_list_opensesame(files_pilot, speaker="AA")

data_pilot <- data_pilot_all %>%
    filter(corrRT > 100)

N_below_100_pilot <- data_pilot_all %>%
    filter(corrRT < 100)
#0

# Sanity checks
N_trials_pilot <- 58
all(summarize(group_by(data_pilot,subj),N=n()) %>% pull(N)== N_trials_pilot)
summarize(group_by(data_pilot,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_pilot %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyPilot <- summarize(group_by(data_pilot,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyPilot %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyPilot %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)
```

```{r brmsPilot, include=FALSE}
library(brms)
run_brms <- function(data, chains = 4, iter = 3000, warmup=1000){
    data <- data %>% filter(type=="CCal", response ==1) %>%
        mutate_at(c("SSP","SSP_obs","MSD","MSD_obs","NAP_td"), ~ factor(., ordered = TRUE)) %>%
        mutate(sNAP_bu = NAP_bu -.5)
null_priors <- c(prior(normal(6, 2), class = Intercept),
                         prior(normal(.5, .2), class = sigma))
effect_priors <-  c(null_priors, prior(normal(0,1), class = b),
                    prior(normal(0,1), class = sd),
                    prior(lkj(2), class = cor))

    message("NAP_bu...")
    NAP_bu <- brm(corrRT ~ 1 +  sNAP_bu + (NAP_bu|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
               chains =chains, iter =iter, warmup = warmup)

    message("null...")
    null <- brm(corrRT ~ 1 + (1|subj), data=data,
               prior = null_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
    chains =chains, iter =iter, warmup = warmup)

message("SSP...")
    SSP <- brm(corrRT ~ 1 +  mo(SSP) +(mo(SSP)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("SSP_obs...")
    SSP_obs <- brm(corrRT ~ 1 +  mo(SSP_obs) +(mo(SSP_obs)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD...")
    MSD <- brm(corrRT ~ 1 +  mo(MSD) +(mo(MSD)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD_obs...")
    MSD_obs <- brm(corrRT ~ 1 +  mo(MSD_obs) +(mo(MSD_obs)|subj), data=data,
                   prior = effect_priors,
                   family =  lognormal(), 
               control = list(adapt_delta=.9995,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("NAP_td...")
    NAP_td <- brm(corrRT ~ 1 +  mo(NAP_td) +(mo(NAP_td)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

list(data = data, models = list(NAP_bu = NAP_bu, null = null, SSP=SSP, SSP_obs =SSP_obs, MSD = MSD, MSD_obs =MSD_obs, NAP_td = NAP_td))
}
```

```{r mpilot, include=FALSE}

 if(file.exists("data_tables/RDS/m_pilot.RDS")){
     m_pilot <- readRDS("data_tables/RDS/m_pilot.RDS")
 } else {
     m_pilot <- run_brms(data_pilot)
     saveRDS(m_pilot, file = "data_tables/RDS/m_pilot.RDS")
 }
 if(file.exists("data_tables/RDS/kfold_pilot.RDS")){
     
     kfold_pilot <- readRDS("data_tables/RDS/kfold_pilot.RDS")
 } else {
     kfold_pilot <- map(m_pilot$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_pilot, file = "data_tables/RDS/kfold_pilot.RDS")
 }

## loo::compare(x=kfold_pilot)

```

<!-- real data -->

```{r readGer, include=FALSE}
opensesame_german <- "data_tables/confirmatory_results"

files_german <-
    list.files(path=opensesame_german,pattern="*.csv",full.names = TRUE)

data_german_all <- read_list_opensesame(files_german, speaker="AA")

data_german <- data_german_all %>%
    filter(corrRT > 100)

N_below_100 <- data_german_all %>%
    filter(corrRT < 100)

## Sanity checks
N_trials_german <- 232 
all(summarize(group_by(data_german,subj),N=n()) %>% pull(N)== N_trials_german)
summarize(group_by(data_german,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_german %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyGer <- summarize(group_by(data_german,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyGer %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyGer %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracyGer %>% filter(nchar==4)
bi_acc <- subj_accuracyGer %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracyGer %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# 1 bad subject

data_german <- data_german %>% filter(!subj %in% bad_subj)

```

```{r brms-models, include=FALSE}

if(file.exists("data_tables/RDS/m_german.RDS")){
    m_german <- readRDS("data_tables/RDS//m_german.RDS")
 } else {
     m_german <- run_brms(data_german, iter=4000, warmup=2000)
     saveRDS(m_german, file = "data_tables/RDS//m_german.RDS")
 }
# 
```
```{r loo-models, include=FALSE}
if(file.exists("data_tables/RDS/kfold_german.RDS")){
    kfold_german <- readRDS("data_tables/RDS/kfold_german.RDS")
} else {
     kfold_german <- map(m_german$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_german, file = "data_tables/RDS/kfold_german.RDS")
    }
```

```{r weigths-models, include=FALSE}
    
## ## loo::compare(x=loo_pilot)
## loo::compare(x=kfold_german)
## loo::compare(x=kfold_german[-1])
## loo::compare(x=kfold_german[-1][-6])
## loo::compare(x=kfold_german[c("MSD_obs","SSP_obs","null")])


## loo::loo_model_weights(x=loo_german)
## compare(loo_german$NAP_td, loo_german$SSP_obs)
## if(file.exists("data_tables/RDS/weights.RDS")){
##     weights <- readRDS("data_tables/RDS/weights.RDS")
## } else {
    ## weights <-loo_model_weights(,kfold_german$MSD_obs)

        ## xx <- ll_matrix[,c(2,3,4,5,6)]
    ## wxx <- loo::stacking_weights(xx)
    ## names(wxx) <-  colnames(xx)

    ## saveRDS(weights, file = "data_tables/RDS/weights.RDS")
## }
```
```{r loo-proc, include=FALSE}
## w_a <- model_weights(m_german$models$MSD,
##                    m_german$models$MSD_obs,
##                    m_german$models$SSP,
##                   ##  m_german$models$SSP_obs,
##                   ## m_german$models$NAP_bu,
##                   ## m_german$models$NAP_td,
##                   ## m_german$models$null,
##                    weights = "loo")


data_german_s <- m_german$data


## loos <- loo_german %>% map_dfc( ~
##     .x$pointwise[,"elpd_loo"]
##     ) %>% {setNames(.,paste0("elpd_",colnames(.)))}

## data_german_s <-  data_german_s %>% bind_cols(loos)


## data_g_summary <- data_german_s %>%
##     group_by(stimulus, NAP_bu, NAP_td) %>%
##     summarize_at(vars(starts_with("elpd")), mean)


## loo_model_weights(loo_german)

predictions <- m_german$models %>% map(~
                     predict(.x,summary=FALSE) %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_german_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots, eval =TRUE, warning=FALSE, include=FALSE}
data_RT <- data_german_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```


```{r readHeb, include=FALSE}
opensesame_hebrew <- "data_tables/confirmatory_results_Heb"

files_hebrew <-
    list.files(path=opensesame_hebrew,pattern="*.csv",full.names = TRUE)

data_hebrew_all <- read_list_opensesame(files_hebrew, speaker="HN")

data_hebrew <- data_hebrew_all %>%
    filter(corrRT > 100)

N_below_100 <- data_hebrew_all %>%
    filter(corrRT < 100)

## Sanity checks

data_hebrew %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracyHeb <- summarize(group_by(data_hebrew,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracyHeb %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracyHeb %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracyHeb %>% filter(nchar==4)
bi_acc <- subj_accuracyHeb %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracyHeb %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# NO bad subject!

data_hebrew <- data_hebrew %>% filter(!subj %in% bad_subj)
# saveRDS(data_hebrew, "data_hebrew.RDS")
```

```{r brms-models-heb, include=FALSE}

if(file.exists("data_tables/RDS/m_hebrew.RDS")){
    m_hebrew <- readRDS("data_tables/RDS//m_hebrew.RDS")
 } else {
     m_hebrew <- run_brms(data_hebrew, iter=4000, warmup=2000)
     saveRDS(m_hebrew, file = "data_tables/RDS//m_hebrew.RDS")
 }
# 
```
```{r loo-models-heb, include=FALSE}
if(file.exists("data_tables/RDS/kfold_hebrew.RDS")){
    kfold_hebrew <- readRDS("data_tables/RDS/kfold_hebrew.RDS")
} else {
     kfold_hebrew <- map(m_hebrew$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_hebrew, file = "data_tables/RDS/kfold_hebrew.RDS")
    }
```


```{r loo-proc_heb, include=FALSE}

data_hebrew_s <- m_hebrew$data


predictions_heb <- m_hebrew$models %>% map(~
                     predict(.x,summary=FALSE,ndraws = 2000)  %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_hebrew_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots_heb, eval =TRUE, warning=FALSE, include=FALSE}

data_RT_heb <- data_hebrew_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```


# Introduction

<!-- ## tmp {-} -->
The following work models the contribution of *sonority* to phonology in a manner that attempts to be compatible with general auditory perception and cognition, as well as with lingustic theory. This is in contrast to many of the traditional formal principles in linguistic theory that do not tend to specify how they interact with general systems of human capacity, such as auditory perception or sensorimotor control. Traditional linguistic principles are mostly generalizations that explain linguistic typologies as they have been depicted in symbolic writing systems. 

The prevailing sonority-related principles such as the *Sonority Sequencing Principle* (SSP) and its derivatives are, indeed, generalizations of the latter type.
Our novel proposals in this study achieve a better empirical coverage compared to different versions of the SSP that we test in a set of perception tasks, while, at the same time, provide a more comprehensive explanation than common formal linguistic principles to the notion of sonority in phonology and phonetics.

We assume here that a proper model of speech perception involves a bottom-up route and a related, yet separate, top-down route. While both inference routes are able to select between discrete alternatives as the goal of their process, they arrive there in two very different ways: bottom-up processes are based on continuous events and top-down processes are based on existing sets of discrete symbolic entities. In that sense, top-down models are more reminiscent of traditional formal principles in phonology, that, more often than not, cover processes that start and end with discrete symbols.

As two complementary inference routes, the top-down and bottom-up modes should not be considered equal. The bottom-up route is the source of learned linguistic distinctions, it is functionally motivated by the laws of physics and the limitations of the perceptual and cognitive systems of the communicating agents. In contrast, the top-down route is based on linguistic experience and superficial inferences that reflect the history of the symbols in the system (i.e., the distributional probabilities of recurring patterns). In other words, top-down inferences reflect functionally motivated behaviors only indirectly, as the outcome of learning the superficial expressions of functionally-motivated (bottom-up) dynamics.

Sonority in our account is strongly related to pitch perception and the major role of pitch in  all language systems, whether it's lexical *tone* in tone languages or the post-lexical *tunes* of intonation systems in all the known human languages. We claim that sonority is related to the strength and quality of pitch perception, serving as a measure of *pitch intelligibility* that acts as a universal drive to optimize the pitch-bearing ability of syllabic units (see Section \@ref(sec:dynamic)). 

We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we propose a novel principle that accounts for syllabic well-formedness based on general principles of competition in real-time, the *Nucleus Attraction Principle* (NAP). We implement NAP with two complementary models (see Section \@ref(sec:modelimp)): (i) a bottom-up model that directly analyzes continuous acoustic signals; and (ii) a top-down model that is based on discrete segments such as consonants and vowels. 

We present a series of syllable count tasks in Section \@ref(sec:experiments), 
designed in order to test our two NAP-based models (applying NAP with bottom-up and top-down approaches) against four traditional sonority models, considering two types of common sonority hierarchies together with two types of common sonority principles---the *Sonority Sequencing Principle* (SSP) and the *Minimum Sonority Distance* (MSD).

We use a Bayesian data analysis approach to test and compare the six different sonority models. Whereas all the different models are found to be capable of predicting the experimental results to a good extent, the symbolic top-down version of NAP is shown to be the superior model. The bottom-up model of NAP comes in second alongside a few of the traditional models. Interestingly, some of the results suggest a relatively high degree of complementarity between the two NAP models, even though they represent a the same principle. This is a desirable result for our framework, which advocates the need for two complementary models, both dynamic and symbolic.

Our synergy of proposals has many advantages over traditional sonority accounts, including methodological aspects, theoretical perspectives, and, essentially, a better empirical coverage. In Subsection \@ref(sec:discussion2) we interpret the results of the main confirmatory experiments and in Section \@ref(sec:genDiscussion) we address some relevant implications, 
where we discuss the usefulness of the top-down vs. bottom-up distinction instead of the common use of the phonology--phonetics dichotomy to express the distinction between discrete and continuous aspects of linguistic sound systems (Subsection \@ref(sec:dichotomies)).
We also discuss the division of labor between sonority and other phonotactic factors, demonstrated with a holistic account of the phenomenon of /s/-stop clusters (Subsection \@ref(sec:division)), before we present our concluisons in Section \@ref(sec:conclusions). 

In the remainder of this Introduction, we briefly present the relevant background on traditional sonority hierarchies and sonority principles, emphasizing their rationale, their application, and their inherent flaws (Subsections \@ref(sec:hierarchies)-\@ref(sec:failures)). 

## Sonority Background {#sec:sonback}

### Sonority hierarchies {#sec:hierarchies}

A sonority hierarchy is a single scale on which all consonants and vowels can be ranked relative to each other. Early versions of current sonority hierarchies often date back to @sievers1893grundzugesk; @jespersen1899fonetik; and @whitney1865relation, while @ohala1992alternatives even goes as far back as @debrosses1765traite.

While the phonetic basis of sonority hierarchies remains controversial, phonological sonority hierarchies have been primarily based on repeated observations that revealed systematic behaviors of segmental distribution and syllabic organization within and across languages. The general consensus regarding the phonological sonority hierarchy thus stems from attested cross-linguistic phonotactic behaviors of different segmental classes, such as, for instance, the relatively high frequency of stop-liquid sequences in the onset of complex syllables (e.g., /kl/ in the English word ***cl**ean*) and the rarity of the opposite liquid-stop sequences at such onset positions, but not at the mirror-image coda position of syllables (e.g., /lk/ in the English word *mi**lk***). 

Although there are many different proposals for sonority hierarchies [@parker2002quantifying found more than 100 distinct sonority hierarchies in the literature], a very basic hierarchy that seems to reach a considerable consensus, and is often cited in relation to Clements's [-@clements1990role] seminal paper is given in (\@ref(ex:scale)).[^cf-liquid]

\begin{exe} 
\ex \emph{Obstruents} $<$ \emph{Nasals} $<$ \emph{Liquids} $<$ \emph{Glides} $<$ \emph{Vowels}  \label{ex:scale} 
\end{exe} 

The ordering of different speech sounds along the sonority hierarchy is assumed to be universal,
in line with the common assumption that sonority has a phonetic basis in perception and/or articulation, yet the patterning of segmental classes as distinct groups along the scale is considered to be language-specific, i.e., based on phonological categorization. 
For example, voiceless stops may be considered universally lower than voiced fricatives on the sonority hierarchy, yet for some languages and analyses they may constitute a single level of *obstruents*. Classes along the sonority hierarchy are most commonly modeled as a series of integers (often referred to as sonority indices) reflecting the ordinal nature of phonological interpretations of the sonority hierarchy. 

(ref:hierarchy-caption) (\#tab:hierarchy) Traditional phonological sonority hierarchies
(ref:hierarchy-caption2) Index values reflect the ordinal ranking of categories in sonority hierarchies. The obstruents in *H~col~* are collapsed into one category (bottom four rows = 1), while in *H~exp~* they are expanded into four distinct levels.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:hierarchy-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
\multicolumn{2}{c}{\textbf{Sonority index values}} & \multicolumn{1}{l}{\textbf{Segmental classes}} & \multicolumn{1}{l}{\textbf{Phonemic examples}}\\
\multicolumn{1}{c}{\emph{H\textsubscript{col}} hierarchy} & \multicolumn{1}{c}{\emph{H\textsubscript{exp}} hierarchy} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}\\
\midrule
5 & 8 & Vowels & \multicolumn{1}{l}{/u, i, o, e, a/}\\
4 & 7 & Glides & \multicolumn{1}{l}{/w, j/}\\
3 & 6 & Liquids & \multicolumn{1}{l}{/l, r/}\\
2 & 5 & Nasals & \multicolumn{1}{l}{/m, n/}\\
\textbf{1} & \textbf{4} & Voiced Fricatives & \multicolumn{1}{l}{/v, z/}\\
\textbf{1}& \textbf{3} & Voiced Stops & \multicolumn{1}{l}{/b, d, g/}\\
\textbf{1}& \textbf{2} & Voiceless Fricatives & \multicolumn{1}{l}{/f, s/}\\
\textbf{1}&\textbf{1} & Voiceless Stops & \multicolumn{1}{l}{/p, t, k/}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:hierarchy-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}


The main differences that result from variation of the basic hierarchy in (\@ref(ex:scale)) concern the class of obstruents, which may contain voiced and voiceless variants of stops and fricatives (to mention just the most prominent distinctions).
Note that vowels are often also commonly divided into subgroups along the sonority hierarchy [see @gordon2012sonority], but these distinctions will be irrelevant in the context of this paper.
It is therefore not uncommon to expand the class of obstruents, whereby stops are lower than fricatives and voiceless consonants are lower than voiced ones. 

The two variants of the sonority index values given in Table \@ref(tab:hierarchy) thus reflect two ends of a spectrum of common sonority hierarchies, ranging from hierarchies that collapse all obstruents together into a single class (resulting in the same sonority index value for all obstruents), to hierarchies that expand the class of obstruents by employing voicing distinctions as well as distinctions between stops and fricatives (resulting in multiple sonority index values within obstruents). In what follows we will refer to these two versions of the sonority hierarchy as *H~col~* for the sonority hierarchy with *collapsed* obstruents, and *H~exp~* for the sonority hierarchy that exhibits an *expanded* class of obstruent.

### Sonority principles {#sec:principles}

Sequencing principles can be understood as a mapping scheme between the ranks of a sonority hierarchy and the linear order of symbolic speech segments.
Modern formulations of such principles, which use the ordinal sonority hierarchy to generalize over the phonotactics of consonantal sequences in terms of *sonority slopes* were developed mainly throughout the seventies and eighties of the twentieth century in seminal works such as @hooper1976introduction; @steriade1982greek; @selkirk1984majorsk; @harris1983syllable; @mohanan1986theory, and @clements1990role.
<!-- [^cf-sequencingclassics] -->

(ref:slopes-pl-lp) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *lpV*. The red line denotes the sonority slope of the onset cluster (i.e., the two onset consonants), while the grey line denotes the slope between the second consonant and the vowel at the nucleus position (always a rise in these cases). The angle of the red lines reflects the well-formed rising sonority slope of the onset cluster in *plV* and the ill-formed falling sonority slope of the onset cluster in *lpv*. Image taken from Anonymous (in press).
```{r slopes-pl-lp, fig.cap = "(ref:slopes-pl-lp)", fig.asp = .45, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","l","p","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority slopes: different types",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="well-formed",y=9.5,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset rise",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="ill-formed",y=9.5,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset fall",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8, "b",
        5,6, "c",
        6,1, "c",
        6,1, "d",
        7,8, "d") %>%
  slopes_plot()
```

The most basic and widely used sonority-based principle that derives phonotactic predictions in terms of syllabic well-formedness is the *Sonority Sequencing Principle* (SSP). The SSP is a simple yet powerful generalization about phonotactics that has been used in countless theoretical accounts. It identifies three distinct types of slopes---*rises*, *falls*, and *plateaus*---such that sequences of segments should rise in sonority from the consonant(s) in the syllabic onset to the syllable's nucleus (most often a vowel) and fall from the nucleus to the consonant(s) in the syllabic coda. 

In this paper we focus on syllable-initial onset consonant clusters that precede a vowel, whereby a rising sonority slope (e.g., *plV*) is considered well-formed, a falling sonority slope (e.g., *lpV*) is considered ill-formed, and sonority plateaus (e.g., *pkV*) fare in between, giving way to various interpretations, depending on the language and analysis, such that plateaus may pattern as ill- or well-formed [e.g., @blevins1995syllable], although they are generally interpreted as denoting a third mid-level of well-formedness, as we will treat them here.

The *Minimum Sonority Distance* [MSD; @steriade1982greek; @selkirk1984majorsk] is a well-known elaboration on the preferred angle of sonority slopes compared to basic applications of the SSP, given that the SSP makes no distinction between different angles of rising or falling slopes. The MSD was designed to prefer onset rises with steep slopes over onset rises with shallow slopes, under the assumption that consonantal sequences in the onset are preferred with a larger sonority distance between them. For instance, *plV* has a steeper rise compared to *bnV* and it is therefore better-formed according to the MSD (see Figure \@ref(fig:slopes-pl-bn)).[^cf-sdp] 

(ref:slopes-pl-bn) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *bnV* (the red solid line denotes the sonority slope of the onset clusters). The angle of the red lines reflects a steeper rise for *plV* (left) compared with *bnV* (right), due to the larger sonority distance between the consonants in *plV*. Image taken from Anonymous (in press).
```{r slopes-pl-bn, fig.cap = "(ref:slopes-pl-bn)", fig.asp = .4, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","b","n","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority rises: different slopes",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="steep rise",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="shallow rise",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8,"b",
        5,3,"c",
        6,5,"c",
        6,5,"d",
        7,8,"d") %>%
  slopes_plot()
```

##     Linguistic Models: Between Discreteness and Continuity {#sec:lingMod}

The study of the sound system of human languages has been one of the longest-standing intersections of symbol-based categorical analyses on the one hand, and signal-based continuous descriptions on the other. These two different types of analysis stand at the core of the distinction many researchers make between *phonetics* and *phonology*, where the former addresses continuous and measurable aspects of the speech signal (namely, sensorimotor aspects of articulation, acoustic signals and neurological patterns in perception), while the latter addresses categorical aspects of the speech signal using discrete and symbolic units like *consonants*, *vowels* and *phonemes* [see overviews in @harris2007representation; @ladd2011phonetics].

The incompatibility between the continuous and the discrete types of description did not escape early studies. @menzerath1933koartikulationsk; @wickelgren1969context and @fowler1980coarticulation noted how the reality of co-articulation of segments defies the idealized conception of speech signals as consisting of a sequence of non-overlapping discrete phonemes [see @halle1954strategy]. @warren1982auditory [172-187] also provides an overview of this problem given the limitations of auditory perception.

### Connectionism

The *connectionist* program in cognitive psychology [e.g., @rumelhart1986parallel1; @rumelhart1986parallel2; @bates1993counectionism] was set to change that classic view with the introduction of connectionist models to phonology [e.g., @goldsmith1992local; @laks1995connectionistsk; @joanisse2000connectionist; @smolensky2006harmonic; @tupper2012sonority]. These models replaced classic symbolic models with *neuromimetic* models [@laks1995connectionistsk 52] that attempt to improve the cognitive plausibility of language models with architectures that resemble neurobiological systems.

It should be noted, though, that the main focus of connectionist models is not so much the symbols in the system as much as it is the classic processes of symbol manipulation. Connectionist models present alternatives to traditional symbol manipulation rules, like the linear rules in generative phonology [@chomsky1968spesk]. The latter are practical in terms of the descriptive notation, but they cannot (and mostly do not) attempt to reflect the cognitive functions of the brain.

In the context of the present work, connectionist models are effective in modeling the phonology of speech percetion in a top-down model, which represents processes that start and end with discrete symbols. However, we assume that there is a functional source for linguistic distinctions in perception, which shoud be searched for in the bottom-up route, where processes must start with continuous events in real time.

###    Dynamical systems {#sec:synthesis}

The introduction of dynamical systems to phonology [see @haken1985theoretical; @browman1992articulatorysk; @goldstein2009coupled; @nam2009self] made it possible to model the interactions between continuous aspects of the speech signal itself (e.g., motor gestures in articulation) and the discrete symbolic categories that linguists assume. One way to achieve this is via attractor landscape models, where discrete categorical units can be modeled as attractor basins [see @haken1990synergeticssk]. In this type of models, various continuous events can contribute more or less to the (partial) activation of different, often competing, attractors.
Good examples for application of attractor landscapes in such manners can be found in @tuller1994nonlinear; @case1995evaluation; @raczaszek1999categorization; @gafosbenus2006dynamics; @roessig2019modeling; and @roessig2019dynamics.

Attractor landscape models are therefore an essential component of a language model that needs to represent the interaction between continuous events and discrete entities. However, much like the connectionist models in phonology, attractor landscapes explicate the process by which a discrete alternative can be (partially) activated, but they have little to say about the components of the system otherwise.
For example, attractor landscape models cannot explain or predict the shape and behavior of the attractor landscape itself, they cannot address the limitations on dynamic events that the system can reliably detect, and they pose no restrictions on what a valid symbol is.

The phonological theory saw dynamical systems playing an increasingly more important role with the growing body of work coming from the school of *Articulatory Phonology*.
Applying similar concepts to perception has been thus far a less productive avenue in phonology [although see @tuller1994nonlinear; @case1995evaluation; @hock2003dynamical; @tuller2004categorization; @tuller2008dynamical; and @lancia2013interaction], perhaps because it is much less clear what are the relevant continuous entities that we need to model in perception.


#     Sonority, Pitch and the Nucleus Attraction Principle (NAP) {#sec:sonPitch}

##		Sonority and Pitch Intelligibility {#sec:pitchintelligibility}

The observation that sonority summarizes an essential quality that is related to vowels and their propensity to deliver a relatively steady harmonic structure, highlighting pitch and formant information, is by no means new. Previous proposals already defined sonority as either relating to vowels in some general way, or more specifically relating to voicing or vibration of the glottal folds, or to the clarity/strength of the formants.[^cf-list] A few previous accounts went even further, by addressing the function of this vowel-centric feature, suggesting that sonority may be related to periodic energy or pitch/tone [@lass1988phonology; @nathan1989preliminaries; @puppel1992sonority; @ladefoged1997linguistic; @heselwood1998unusual]. What all these proposals share, explicitly or implicitly, is a recurring insight about a strong link between the preferred type of segmental material in syllabic nuclei and a set of features that conspire to optimize pitch intelligibility, a property which characterizes vowels more than consonants. 

Pitch is an inseparable communicative dimension of all linguistic sound systems [@bolinger1978intonation; @house1990tonal; @cutler1997prosody; @roettger2019tune], whether it is lexically determined as in linguistic *tone*, 
or post-lexically employed to convey intonation, i.e., the linguistic *tune* [e.g., @pike1945intonation; @tHart1990perceptual; @jun2005prosodicsk; @jun2015prosodicsk]. 
Tones are used to distinguish lexical items while tunes are used to demarcate units as well as to modulate semantics (e.g., information structure and sentence modality) and to express a vast array of non-propositional meanings (e.g., discourse-pragmatic intention, emotional state, socio-indexical identity, and attitudinal stance). The importance of pitch to human communication cannot be overstated.

Crucially, linguistic pitch events are commonly assumed to target syllable-sized units as their "docking site", regardless of the type of pitch event, whether it is a lexical tone or a post-lexical tune [@lehiste1977suprasegmentals]. 
These linguistic pitch events associate with either syllables or *moras*.[^cf-mora] 
For example, intonation pitch contours that highlight and modulate whole words and phrases essentially target privileged syllables---stressed syllables and/or syllables at the edges of prosodic words and phrases---to achieve their communicative goal with texts of various sizes.
This tone-bearing role of syllables is the hallmark of many prominent theories regarding tone and intonation like Autosegmental and Autosegmental-Metrical Phonology [e.g. @liberman1975intonationalsk; @goldsmith1976autosegmental; @pierrehumbert1980phoneticssk; @ladd2008intonational].

The functionally motivated conclusion that emerges with respect to sonority is therefore that syllables require a pitch-bearing nucleus and sonority is a scalar measure of the ability to bear pitch. In other words, sonority is, most likely, a measure of *pitch intelligibility*. 
This hypothesis comes with an underlying assumption that syllables have followed an evolutionary trajectory that shaped them to optimally carry pitch in their nuclei. Sonority, according to this description, serves as the tool that governs the requirement for intelligible pitch as a fundamental characteristic in the design of the building blocks of prosody.

<!-- [^cf-perception] -->
It is important to note that this view of sonority is explicitly and exclusively based on perception, rather than articulation of speech. However, it does not exclude articulation-based description of syllables under the assumption that restrictions on syllabic structure must be derived from both perception and articulation of speech. A case in point is the *Articulatory Phonology* framework (see Section \@ref(sec:synthesis)), with its valuable descriptions of temporal coordination and phase relations between motor gestures, in manners that can be effectively linked to syllabic organization [see, e.g., @goldstein2007syllablesk; @goldstein2009coupled; @shaw2009syllabificationsk; @gafos2014stochastic; @hermes2017variabilitysk]. 

###     Pitch intelligibility and periodic energy {#sec:periodicenergy}

Pitch is a psychophysical phenomenon based on perception and cognition [see @plomp1976aspects; @plack2005psychophysics]. We can extract perception-related measurements from acoustics, i.e., not directly from the perceived sensation of a human subject but from the digitally-analyzed description of the physical sound in space. Using acoustics to cover auditory psychophysical phenomena is not a straight-forward task though. It requires a consistent and reliable association between acoustics on the one hand, and perception and cognition on the other hand.
This task is potentially complicated further with a complex phenomenon like pitch, that is evidently sensitive to various aspects of the rich acoustic signal[see, e.g., @houtsma1995pitch; @shepard2001pitch; @moore2013anintro 203], as well as to our top-down expectations with regards to learned regularities of pitch behavior in the speech signal [@mcpherson2018diversity]. 

Fortunately, there are strong links that hold between pitch and measurable acoustic markers. This is evident from the extensive use of acoustic F0 measurements to estimate perceived pitch height. 
Pitch estimations from F0 measurements is in fact quite reliable when dealing with specific types of audio such as speech, whereby the bulk of the pitch information is coming from a single source (i.e., one speaker) within a limited range of fundamental frequencies (mostly between 75--400 Hz, rarely below 50Hz or above 600Hz).

To estimate perceived pitch intelligibility from acoustic signals, we need to obtain a measure of *periodic energy*, which is a measurement of the acoustic power of the periodic components in the signal. It may be helpful to think of it as a measurement of general intensity that excludes the contribution of aperiodic noise and transient bursts.
Measurements of periodic energy are not much different from widely-used F0 measurements that are commonly based on abilities to detect periodic components in the complex signal. Roughly speaking, rather than resolving the harmonic denominator of detected periodic components in order to estimate F0, a periodic energy meter needs to sum over their power.

To conclude, our ability to detect periodicity in acoustic signals allows us to extract good estimates of F0 and periodic energy from speech data. We stand on firm grounds when we map these acoustic markers to perception in terms of pitch height and pitch intelligibility (respectively).
Given a causal link between perceived pitch intelligibility and linguistic sonority, we assume by transitivity that acoustic periodic energy maintains a causal link with the linguistically-loaded notion of sonority.  


##   Sonority "correlusions"  {#sec:correlusions}

@parker2002quantifying found a little under 100 different proposals for correlates of sonority in the literature, and he tested five leading proposals in laboratory conditions: *intensity*, *intraoral air pressure*, *F~1~ frequency*, *total air flow* and *duration*. In his study, the tightest correlations with sonority classes were obtained for acoustic intensity measurements, a conclusion that was repeated and elaborated upon in @parker2008sound. Parker's conclusions about the role of acoustic intensity are very much in line with many influential studies in phonetics and phonology that also target acoustic intensity as the phonetic correlate of sonority [e.g. @sievers1893grundzugesk; @heffner1969generalsk; @ladefoged1975acourse; @clements1990role; @blevins1995syllable; and @gordon2012sonority, to name just a few prominent examples].
<!-- [^cf-articultory]  -->

The main problem with intensity-based accounts is related to the distinction between *causation* and *correlation*. Establishing causation from acoustic signals necessitates a theory that can reliably link between acoustic markers and consistent operations or processes in sensorimotor speech articulation and/or auditory speech perception. The problem with accounts that are based on acoustic intensity is that the general acoustic intensity of the signal does not consistently map to any aspect of human auditory perception, not even perceived loudness. 

###  Acoustic intensity $≠$ perceived loudness {#sec:intensity}

The acoustic signal has certain physical qualities contributing to its overall power, but they have different effects on the perceptual system of the human hearer. This discrepancy between acoustic intensity and perceived loudness is a well-known problem, playing a role at different dimensions of the mapping between acoustics and perception.

The prominent points of departure between acoustic intensity and perceived loudness include different frequency bands that lead to different loudness perception [e.g. @fletcher1933loudness; @plack1995loudness; @suzuki2004equal; @moore2013anintro 134], different signal durations that have different loudness effects [e.g. @turk1996processing; @seshadri2009perceived; @olsen2010loudness; @moore2013anintro 143], and different periodic structures, where the difference between harmonic structures vs. random noise, and their respective spectral bandwidths, was also found to influence perception of loudness [e.g. @hellman1972asymmetry; @bao2010psychoacousticsk; @moore2013anintro 140].

Acoustic intensity is therefore a physical description of sound waves in space which does not consistently relate to how loud we perceive them, or to any other perceptual phenomenon for that matter.

###  Loudness is not a good candidate for sonority {#sec:loudness}

Note also that the relevance of perceived loudness to syllabic organization requires some sort of functional explanation, which seems to be lacking. 
The systematic differences in intensity of adjacent speech sounds imply that these differences are neutralized in perception, as it should make sense to assume that the different sounds that compose coherent speech are perceived as having comparable loudness. 
The literature on perceived loudness supports this assumption given that speech portions with relatively low acoustic intensity, like voiceless fricatives, appear in speech next to portions with relatively high acoustic intensity, like that of a vowel.
Our auditory system perceives the aperiodic high-mid frequencies of many obstruents as exceptionally loud compared to the periodic low-mid frequency ranges of vowel sounds, thus compensating in perception for physical differences in acoustic intensity.

Given the above, we should anticipate that perceived loudness will not be a good candidate for the acoustic correlate of sonority hierarchies, as a measure of perceived loudness would bring all speech sounds closer together by diminishing the distinctions provided by acoustic intensity. Indeed, although good approximations of perceived loudness from acoustic signals are available [e.g. @seshadri2009perceived; @skovenborg2012loudnesssk; @lund2014loudnesssk; @itu2015algorithmssk], we are unaware of attempts to employ such measures for sonority. 

Instead of attempts to map acoustic intensity with perception in terms of perceived loudness, most successful endeavors that use intensity-based measures as correlates of sonority do it by essentially enhancing the intensity-loudness discrepancy, targeting certain frequency bands to---roughly speaking---discriminate against energy at the higher frequencies, that are more characteristic of obstruents, in favor of energy at low-mid ranges of the spectrum, that are more characteristic of sonorants and particularly vowels [e.g. @Pfitzinger1996syllablesk; @port1996dynamic; @fant2000source; @galves2002sonoritysk; @wang2007robust; @tilsen2013speech; @patha2016syllablesk; @nakajima2017english; @rasanen2018pre]. 
The relative success of such metrics is not commonly motivated on perceptual grounds. However, they are often tightly linked to the perceptual quality that is identified with sonority in this work---the capacity to perceive pitch.


##      The Nucleus Attraction Principle {#sec:nap}

At the heart of all sonority-based principles lies the idea that the most sonorous segment in a sequence is contained within the nucleus of the syllable. This idea in fact postulates a link between the amount of sonority and the nucleus position of the syllable. We adopt this fundamental insight that guides all other sonority principles in the development of the Nucleus Attraction Principle, but instead of adding further formal assumptions about non-overlapping segments with fixed sonority values and corresponding sonority slopes in symbolic time, the link between sonority and the syllabic nucleus is conceived of as a dynamic process in real time, whereby all the portions of the speech signal compete against each other for available nuclei (*available nuclei* can be thought of as the canonical syllables of words in the *mental lexicon*). 

Sonority is therefore the quality that is capable of *attracting* the nucleus. The varying quantities of this quality, which temporally fluctuate along the stream of speech, determine which portions of speech are prone to succeed in attracting nuclei given their superior local sonority *mass*. The speech portions that fall between those successful attractors are syllabified in the margins of syllables, at onset and coda positions.[^cf-attraction]

[^cf-attraction]: This notion of prosodic *attraction* is, in fact, well-established in phonological theory, with descriptions of *weight sensitivity* in the stress systems of many unrelated languages, in which the stress is said to be attracted to heavy syllables.
Heaviness is mainly the product of a longer vowel in the nucleus, and in some languages heaviness may also result from a (preferably sonorant) consonant in the coda [e.g., @mccarthy1979formalsk; @hayes1980metrical; @prince1990quantitative; @gordon2006syllableweight]. There are also analyses whereby 
vowel qualities that are considered more sonorous can contribute to heaviness and attract the stress [@zec1995sonority; @zec2003prosodic; @kenstowicz1997quality; @delacy2002formal; @gordon2012sonority].
Viewed with NAP in mind, attraction of stress in weight sensitive systems is simply the special case of a regular procedure, whereby weight---i.e., sonority---attracts syllabic nuclei. 

Crucially, NAP treats the postulated link between sonority peaks and syllabic nuclei as the result of a perceptual-cognitive process in real time, rather than describing a formal geometric state of affairs with symbolic discrete tools. 
By modelling the link between sonority and the syllabic nucleus in dynamic terms it is not necessary to add further theoretical postulates about sonority slopes, nor discrete segmental categories of consonants and vowels, to determine well-formedness of syllabic structures. Syllabic ill-formedness in NAP-based models is positively correlated with the degree of nucleus competition that a given syllabic parse incurs. 

It is important to note that the informativeness of NAP-based models is not derived from identifying the winner of the nucleus competition, but from quantifying the degree of competition within different portions of speech that stand for potential syllabic parses. 
NAP-based models can analyze speech parts that are parsed together as a single syllabic unit in order to estimate the degree of competition they give rise to when they compete for a single nucleus. 
In discrete terms, NAP-based models can quantify different sequences of segments to reflect how strongly they compete for a single nucleus. 
The higher the degree of internal competition, the more worse-formed syllable is predicted to result from this parse.
To simplify this further with respect to the subset of instances discussed in this work (i.e., syllables with complex consonantal onset clusters), it is possible to say that the winner of the nucleus competition is always the only vowel in the structure. The determination of ill-formedness in these cases is based on quantifying the amount of competition that the winning vowel has to withstand given different consonantal clusters in the onset of the same syllable.

It should be also useful to note that we do not expect serious competition to arise from a consonant adjacent to the vowel in the same syllable.
Nucleus competition, much like sonority slopes, has a limited impact on syllables with simple onsets and/or codas (e.g., CVC). Principles like SSP and NAP play a role chiefly when sequences of consonants are syllabified within a single syllable as complex onset or coda clusters (e.g., **C**CVC**C**). The phonotactics of these possible sequences are determined, to a large extent, by sonority principles. We interpret this aspect of cluster phonotactics such that sequences within syllables are avoided the more they increase the potential competition for the nucleus in the process of syllabifying/parsing the stream of speech.

###     Schematic NAP sketches {#sec:NAPsketch}

(ref:nap-depictions) Schematic depictions of competition scenarios with symbolic CCV structures. The nucleus competition can be understood as the competition between the blue and the purple areas under the sonority curve. The two examples at the top row---*plV* and *lpV*---suggest replication of successful traditional predictions, while the three examples at the bottom row---*spV*, *sfV* and *nmV*---suggest a divergence from inherent failures of SSP-type models (see text for more details). Image taken from Anonymous (in press).
```{r nap-depictions, fig.cap = "(ref:nap-depictions)", out.width = '100%', fig.align = 'center'}

knitr::include_graphics(rep(c("extrenal_figures/napComb150.png")))

```


To understand the rationale of NAP, a series of schematic sketches are presented in Figure \@ref(fig:nap-depictions), accompanied by an impressionistic description. These will eventually be implemented within formal models that are described in detail in Chapter \@ref(sec:modelimp).
The five examples with specified consonantal clusters exhibit their related sonorant energy depicted as the *area under the curve*, whereby the curve itself is an idealized depiction of schematic sonority.
The purple area in each syllable in Figure \@ref(fig:nap-depictions) denotes the sonorant energy of the winning vowel in the nucleus position while the blue area denotes the sonorant energy of the losing portions in the onset. 
Consider for example the pair *plV* and *lpV*, with schematic NAP-related depictions in the top row of Figure \@ref(fig:nap-depictions) (and with more traditional sonority slopes in Figure \@ref(fig:slopes-pl-lp)). A consonantal onset cluster with a putatively well-formed rising sonority slope like *plV* should be also considered well-formed under NAP due to the very low potential of competition between the marginal minimally-sonorous onset consonant /p/ and the non-adjacent vowel that wins the competition for the nucleus. The intervening /l/ in this case only promotes a continuous rise in sonority from /p/ to V. Likewise, a consonantal onset cluster with a putatively ill-formed falling sonority slope like *lpV* should be also considered ill-formed under NAP due to the strong potential for competition between the marginal sonorous onset consonant /l/ and the non-adjacent vowel, especially given intervening /p/ that leads to discontinuity in the sonority trajectory between /l/ and V.

Unlike the examples above, where the rationale of NAP is expected to replicate successful predictions of the SSP with cases like *plV* and *lpV*, NAP is also expected to diverge from traditional sonority sequencing principles in those cases where traditional principles 
consistently fail. Consider the examples in the bottom row of Figure \@ref(fig:nap-depictions).
<!-- , which were also depicted with traditional sonority slopes in Figures \@ref(fig:slopes-sp-sp)-\@ref(fig:slopes-nm-sf). -->
Under NAP, neither */s/-stop* clusters like *spV* nor voiceless obstruent plateaus like *sfV* are expected to incur a strong competition syllable-internally due to the low potential for competition between the minimally-sonorous onset consonant /s/ and the non-adjacent vowel that wins the competition (here, the intervening voiceless obstruents /p/ and /f/ retain a minimally sonorous trajectory throughout the whole onset).
At the same time, a strong degree of competition is predicted under NAP for nasal plateaus like *nmV* when compared to obstruent plateaus like *sfV*. This should be expected given the strong potential for competition between the marginal sonorous onset consonant /n/ and the non-adjacent winning vowel (here, the intervening nasal retains a relatively leveled sonorous trajectory throughout the onset).

The impressionistic descriptions of NAP, as provided in this section, are implemented within formal models that we describe in detail in Section \@ref(sec:modelimp).


#     NAP Implementations {#sec:modelimp}

##      Complementary NAP Models {#sec:complementary}

NAP essentially describes a bottom-up process, illustrating the parsing of the stream of speech into syllables as the end of a process that starts with continuous events in perception.
A bottom-up perspective on modelling NAP is therefore relatively straight-forward as it requires a similar approach to the process NAP describes: the analysis of continuous acoustic data at the input, which results in well-formedness predictions at the output.

A bottom-up approach for NAP models has no capacity to exploit the power of abstraction so it essentially has no "memory". It is a mechanistic dynamic model that contains discrete symbolic entities only at the (meta-)linguistic target of the task, at the end of the process that determines syllabic well-formedness.
This means that a bottom-up model can be only designed to analyze concrete speech tokens. Unlike traditional sonority principles and their models, a bottom-up model of NAP cannot determine the well-formedness of an abstract syllable as it is depicted in symbolic form, and it will give slightly different scores to different renditions of the same syllable, even by the same speaker.

A NAP-based model that operates on abstracted symbolic units is a top-down model that is taken to be a separate model, complementary to the model of the bottom-up route. Top-down inferences are based on learned regularities and categorical abstractions that reflect linguistic experience. To that end, knowledge about consonantal inventories and the probabilities of consonantal distribution with respect to position in the syllable has to be learned and stored in abstract symbolic forms which are available for top-down inferences. In that sense, top-down inferences in perception are based on the distributional probability of recognized symbols, given their history in the system.

The above description of top-down inferences, which are detached from the functional aspects of the bottom-up route, echo models of the language user as a *statistical learner* [see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing]
and, more specifically, they are very much in line with models of *phonotactic learners* [see, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @jarosz2017inputsk; @mayer2019phonotacticsk].

That said, the current project does not explore the statistical nature of top-down inferences. Instead, we operationalize the rationale behind NAP with symbolic machinery to present what can be understood as the symbolic model of NAP, which is used to estimate NAP-based top-down inferences. 
This choice allows the presentation of a top-down model with a stronger explanatory value with regards to NAP as it uses a similar architecture to that of standard sonority principles, helping to elucidate NAP's core ideas with familiar vocabulary (see Subsection \@ref(sec:naptdmodel)). 

Moreover, it should be noted that since a top-down model in this framework is based on the distributional patterns of recognizable symbols, these distributions are, in fact, "blind" to their various bottom-up sources, which include a host of universal and idiosyncratic phonotactic pressures. A true top-down statistical learner is thus inherently "contaminated" by all the different sources that contribute to phonotactics in a given system, without a clear distinction between sonority and other factors. Thus, it remains an open question whether top-down inferences that target only sonority-based phonotactics can be modeled in a more direct and principled way than the one presented here with the symbolic model of NAP.

Finally, the symbolic NAP model is necessary for the application of NAP in typological, diachronic and many traditional and current studies, where speech data is transcribed into strings of discrete symbols. 

##     Model Implementations in Dynamic and Symbolic Terms {#sec:modelimpOLD}

In order to compare the different proposals, four types of traditional sonority models are considered alongside the two NAP models.
For traditional models we use the two types of sonority hierarchies that were presented in Subsection \@ref(sec:hierarchies), where the class of obstruents is either *collapsed* (*H~col~*) into a single level or *expanded* (*H~exp~*) to include distinctions between voiced and voiceless obstruents, and between stops and fricatives. 
Both hierarchies are applied with each of the two main variants of traditional sonority principles, the *Sonority Sequencing Principle*, SSP, and the *Minimum Sonority Distance*, MSD (see Subsection \@ref(sec:principles)). 
The four traditional sonority models under discussion are therefore a combination of a sonority principle (either SSP or MSD) and a sonority hierarchy (either *H~col~* or *H~exp~*). Accordingly, they bear the notation *SSP~col~* , *SSP~exp~*, *MSD~col~*, and *MSD~exp~*.

The two NAP models use periodic energy as the correlate of sonority, and they apply it either continuously with acoustics (bottom-up model), or in a discrete manner using symbols (top-down model). We refer to these two NAP models with the notation *NAP~td~* for the top-down model and *NAP~bu~* for the bottom-up one.

To test the different sonority models this study focuses on complex onset clusters of the general form CCV, where C denotes consonants in onset position and V denotes a vowel in nucleus position. Traditional sonority models look at the sonority slope of the onset cluster to determine well-formedness of CCV syllables, while NAP-based models apply the notion of *competition* to determine well-formedness.

In the following sections we elaborate on the methods for obtaining well-formedness scores, starting with the ordinal scores obtained from the four traditional sonority models (Subsection \@ref(sec:traditionalmodels)), and the symbolic NAP model, *NAP~td~* (Subsection \@ref(sec:naptdmodel)). 
The implementation of the continuous model, *NAP~bu~* follows in Subsection \@ref(sec:napbu). 
<!-- Finally, this chapter is concluded with a short overview of some of the key advantages of NAP over traditional sonority models (Section \@ref(sec:advantages)). -->

It is very important to note that the implemetations of NAP in the following subsections present procedures for estimated results, based on the rationale of NAP. These procedures are not, in and of themselves, supposed to stand for any perceptual or cognitive mechanism, neither top-down nor bottom-up.

### Traditional Sonority Models {#sec:traditionalmodels}

Implementation of traditional sonority principles like the SSP is based on a calculation of the sonority slope with a given sequence of segments. Speech segments in these frameworks have fixed index values on the sonority hierarchy, based on their class membership, as in the *H~col~* and *H~exp~* hierarchies (see Table \@ref(tab:hierarchy)). These sonority index values are usually expressed in terms of integers since they reflect an ordinal scale, and, for this reason, the mathematical operations that these models employ should be restricted to basic arithmetic functions of addition and subtraction. Sonority slopes can be therefore obtained straight-forwardly by a subtraction between the corresponding sonority indices of two adjacent consonants. In onset clusters with two consonants (CCV) this can be simply achieved by the formula $C_2 – C_1$, which yields positive results for rising sonority slopes, negative results for falling sonority slopes, or a zero for plateaus. This is applied to the two SSP models, *SSP~col~* and *SSP~exp~* (see examples in Table \@ref(tab:ordinalscores)).

The same formula is also used to obtain scores for the Minimum Sonority Distance models, *MSD~col~* and *MSD~exp~*, which elaborate on the well-formedness of onset rises. 
With the MSD, higher positive scores are preferred over lower positive scores to reflect the preference for a larger sonority distance (or a steeper slope) in a rising onset configuration (see examples in Table \@ref(tab:ordinalscores)). 

### The Top-Down Symbolic NAP Model {#sec:naptdmodel}

The symbolic version of NAP, which is used to derive predictions for the top-down NAP (*NAP~td~*), shares a similar architecture with common SSP-based models, yet it reflects the novelties of the current proposal, both in terms of the sonority hierarchy it assumes, and in terms of the design of the sonority principle. *NAP~td~* uses a sonority hierarchy that is based on the periodic energy potential of different phoneme classes as the basis of distinct categorical patterning (see following Subsection \@ref(sec:snaphierarchy)). Furthermore, *NAP~td~* models syllabic well-formedness with the notion of nucleus competition in mind rather than the formal notion of sonority slopes, as in traditional SSP-type models (see Subsection \@ref(sec:snapimplementation)).

#### The sonority hierarchy in NAP~td~ {#sec:snaphierarchy}

The symbolic sonority hierarchy in NAP uses the basic ratio between periodic and aperiodic energy in the speech signal to divide all speech sounds into three distinct groups, reflecting the coarse, yet reliable differences in potential periodic energy mass of different abstract speech sound categories. To achieve that, we rely on the following set of general characteristics: (i) the main source of periodic energy in speech stems from the vocal fold vibration when voicing occurs; (ii) aperiodic energy in speech is mostly the result of the turbulent airflow resulting from articulatory friction (i.e. fricatives), and from articulatory closure in oral stops, which often result in transient bursts when released [see @rosen1992temporal].

Thus, the ratio between periodic and aperiodic components in speech sounds readily yields the following three distinct groups: (i) voiceless obstruents that consist of mostly aperiodic energy are the least sonorous type of speech sounds; (ii) sonorant consonants and vowels that consist of mostly periodic energy are the most sonorous type of speech sounds; and (iii) voiced obstruents that consist of both periodic and aperiodic energy belong in the middle of this 3-place scale (see \@ref(ex:napscale))

\begin{exe}
\ex \emph{Voiceless Obstruents} $<$ \emph{Voiced Obstruents} $<$ \emph{Sonorants}  \label{ex:napscale}
\end{exe}

A further distinction in NAP's sonority hierarchy is based on the general presence or absence of articulatory contact, whic hcan be referred to as the distinction beteen *contoids* and *vocoids* [@pike1943phonetics].
A free and open vocal tract contributes to a potentially stronger and longer vocalic signal that can qualitatively enhance the potential periodic energy mass.
This distinction effectively separates the sonorants into *sonorant vocoids* (glides and vowels) and *sonorant contoids* (nasals and liquids).
<!-- [^cfrhotics]  -->
See Table \@ref(tab:napscale) for the full sonority hierarchy in the symbolic model of NAP.
<!-- [^cf-reconciliation] -->

<!-- [^cfrhotics]: Note that some rhotics, which are traditionally considered liquids, may in fact belong with the vocoid consonants (e.g., most of the English rhotics, especially in coda position). However, we can ignore this issue here since as rhotics are not included in this paper. -->

(ref:napscale-caption) (\#tab:napscale) The symbolic sonority hierarchy in *NAP~td~*
(ref:napscale-caption2) Index values reflect the ordinal ranking of categories in the sonority hierarchy. The distinctions between categories in the symbolic NAP hierarchy are based on the characteristic ratio between periodic and aperiodic energy, and on articulatory contact, both taken to reflect the potential of the periodic energy mass, i.e., the potential for nucleus attraction.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:napscale-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
Sonority index & \multicolumn{1}{c}{Segmental classes} & \multicolumn{1}{c}{Periodic:Aperiodic} & \multicolumn{1}{c}{Articulatory contact}\\
\midrule
4 & \textbf{Sonorant Vocoids} & \multicolumn{1}{c}{1:0} & $-$\\
 & (\emph{glides}, \emph{vowels}) &  & \\
3 & \textbf{Sonorant Contoids} & \multicolumn{1}{c}{1:0} & $+$\\
 & (\emph{nasals}, \emph{liquids}) &  & \\
2 & \textbf{Voiced Obstruents} & \multicolumn{1}{c}{1:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
1 & \textbf{Voiceless Obstruents} & \multicolumn{1}{c}{0:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:napscale-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The complete 4-place sonority hierarchy of *NAP~td~* in Table \@ref(tab:napscale) also reflects a basic typology of nucleus types, which supports the use of this scale as a qualitative measure for nucleus attraction potentials. Sonorant vocoids like glides and vowels can attract the nucleus in all languages we know (a glide is considered a vowel when syllabified in the nucleus position), while sonorant contoids like nasals or liquids can be syllabic (i.e. attract the nucleus) only in a subset of languages, of which a smaller subset may allow obstruents to attract nuclei [but see @easterday2019highly for some divergent patterns with syllabic obstruents relative to syllabic liquids].

#### NAP~td~ implementation {#sec:snapimplementation}

In order to account for C~1~C~2~V syllables with the NAP framework, we essentially want to measure the competition potential between C~1~ and V given C~2~. In and of itself, C~2~ should not be considered as a strong competitor due to its proximity to the vowel, as discussed in Subsection (\@ref(sec:nap)).
The issue of competition may be therefore expressed by the following questions:
(i) what is the potential periodic energy mass of C~1~ (i.e., how sonorous is C~1~, or what is the intercept of the cluster that determines the starting point of the slope);
(ii) how much of the energy in C~1~ is potentially lost, gained or maintained in C~2~, before peaking at the vowel (i.e., what is the sonority slope).
Assessing this relationship between C~1~ and V given C~2~ can be achieved by the combination of two subtraction formulas: 
(i) a calculation of the difference between C~1~ and the non-adjacent vowel, to reflect the potential strength of C~1~ in terms of the difference between the intercept and the nucleus; 
(ii) a calculation of the slope between adjacent C~1~ and C~2~, just like in the SSP-based models, to reflect the trend of the energy's trajectory towards the peak. 
This can be summarized with the formula in \@ref(eq:naptdeq), see examples in Table (\@ref(tab:ordinalscores)).
<!-- [^cf-angle] -->

\begin{equation}
  (V - C_1) + (C_2 - C_1)  \label{eq:naptdeq}
\end{equation}

###   Ordinal Sonority Scores {#sec:ordinalscores}

Table \@ref(tab:ordinalscores) demonstrates and compares the scores of the five ordinal models (2$X$SSP, 2$X$MSD and *NAP~td~*) with different CCV cluster types. It shows that the main difference between the two sonority hierarchies, *H~exp~* and *H~col~*, concerns fricative-stop clusters like the */s/-stop* cluster *spV*, which are considered as either an onset fall (with the *H~exp~* hierarchy) or an onset plateau (with the *H~col~* hierarchy). When the MSD is applied, the two sonority hierarchies also show differences in ranking within onset rises, given their different treatment of obstruents: in models that use the *H~exp~* hierarchy there are four levels of obstruents (voiced and voiceless stops and fricatives) which are collapsed into one level in models that use the *H~col~* hierarchy. 
This results in five distinct sonority rise scores in *MSD~exp~* model, but only two in the *MSD~col~* model, where also some of the trends differ (e.g. *smV* vs. *vlV* in the two MSD-based models).

(ref:ordinalscores-caption) (\#tab:ordinalscores) Ordinal sonority scores
(ref:ordinalscores-caption2) Well-formedness scores with ordinal models. The table demonstrates the predictions we obtain using the two traditional sonority hierarchies, *H~col~* and *H~exp~*, with each of the two traditional sonority principles, SSP and MSD. Numbers in brackets next to "Rise" reflect MSD's ranking of onset rises by distance---higher values indicate better-formed rises. The scores derived from *NAP~td~* on the right column are taken to directly reflect the nucleus competition potential, where higher scores are better-formed.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:ordinalscores-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Traditional sonority principles} & \multicolumn{1}{c}{Symbolic NAP}\\

\multicolumn{1}{l}{Onset} & \multicolumn{2}{c}{\emph{exp} hierarchy} & \multicolumn{2}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\

\multicolumn{1}{l}{clusters} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{exp}}}} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{col}}}} & \multicolumn{1}{c}{$(V-C_1)+(C_2-C_1)$}\\

\midrule
\multicolumn{1}{l}{\textbf{pl}V} & 6$-$1 $=$ 5 & \multicolumn{1}{c}{\textbf{Rise (5)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{fl}V} & 6$-$2 $=$ 4 & \multicolumn{1}{c}{\textbf{Rise (4)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{sm}V} & 5$-$2 $=$ 3 & \multicolumn{1}{c}{\textbf{Rise (3)}} & 2$-$1 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{vl}V} & 6$-$4 $=$ 2 & \multicolumn{1}{c}{\textbf{Rise (2)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$2)$+$(3$-$2) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{ml}V} & 6$-$5 $=$ 1 & \multicolumn{1}{c}{\textbf{Rise (1)}} & 3$-$2 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sf}V} & 2$-$2 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{zv}V} & 3$-$3 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$2)$+$(2$-$2) $=$ \textbf{2}}\\
\multicolumn{1}{l}{\textbf{nm}V} & 5$-$5 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 2$-$2 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sp}V} & 1$-$2 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{lm}V} & 5$-$6 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$3 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{mz}V} & 4$-$5 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{lv}V} & 4$-$6 $=$ $-$2 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$4 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{ms}V} & 2$-$5 $=$ $-$3 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{np}V} & 1$-$5 $=$ $-$4 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{lp}V} & 1$-$6 $=$ $-$5 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$3 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:ordinalscores-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

Unlike traditional models, the predictions of *NAP~td~* are not grouped into levels that reflect the rough angle of the sonority slope in terms of falls, rises and plateaus. The raw score of the *NAP~td~* formula is taken as reflective of the nucleus competition potential such that higher scores denote weaker competition and are thus better-formed. 
The top-down NAP model allows scores within a range that goes from -$\mathit{3}$ for the most ill-formed syllable up to $\mathit{6}$ for the most well-formed, although a more relevant range to consider, given that glides are excluded from this set, is between -$\mathit{1}$ and $\mathit{5}$. These scores are not immediately comparable to the traditional model scores, yet some interesting departures from the traditional models can be observed in Table \@ref(tab:ordinalscores). For example, *NAP~td~* considers the onset rise in the sonorous cluster *mlV* as ill-formed as the inverse fall *lmV*, and both clusters pattern in *NAP~td~* with nasal plateaus (e.g. *nmV*), where they all receive the same relatively low value of $\mathit{1}$. At the same time, voiceless clusters pattern in *NAP~td~* with well-formed combinations (scoring $\mathit{3}$) although they may include sonority plateaus (e.g. *sfV*) or sonority falls (e.g. *spV*) in traditional model terms.

###   The Bottom-Up Dynamic NAP Model {#sec:napbu}

There are various ways to calculate an estimation of the nucleus competition potential within syllables based on the periodic energy data of the acoustic signal. The method that we present here has the advantage of not relying on segmental landmarks that are categorical abstractions of the type that is assumed in the top-down model---they are not supposed to be available in the bottom-up route.

(ref:com-4examples-1) Smoothed periodic energy curve (black) of the four syllables from the experimental stimuli---*lpal*, *nmal*, *vlal*, and *smal*. Red vertical line denotes the center of periodic mass of the entire syllable (*CoM~syllable~*), blue vertical line denotes the center of periodic mass of the left portion (*CoM~onset~*). Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). The distance between the two CoM landmarks is indicative of the energy displacement away from the syllabic center, reflecting the nucleus competition potential within the syllable (see details on this measurement in Subsection \@ref(sec:obtaining))
```{r com-4examples-1, fig.cap = "(ref:com-4examples-1)", fig.width=7, fig.asp=.6, warning=FALSE, dev="cairo_pdf"}
# CoM_ons <-  expression(CoM[ons])# %>% as_label()
monosyl_examp_1 <- filter(monosyl_info_AA,
                          syl %in% c("smal","vlal","nmal","lpal"))
ordered_syl_abs <- unique(monosyl_examp_1[order(monosyl_examp_1$NAP_bu),]$syl)
monosyl_examp_1$syl <- factor(monosyl_examp_1$syl, levels=ordered_syl_abs)
monosyl_examp_1_plot <-
  ggplot(monosyl_examp_1, aes(x=t)) +
  xlim(0,475) + ylim(-3,19) + #ggtitle("") +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-1, yend=13), color="royalblue1", size=1.7, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-1, yend=13),color="red", size=1.7, alpha=.6, linetype = "solid", lineend = "round") +
  # geom_text(aes(x=com_onset, y=15, label=deparse(CoM_ons)), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=com_onset,y=15,label="CoM:onset"), nudge_x = -30, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=x_com_syl,y=15,label="CoM:syllable"), nudge_x = 30, color="red", alpha=.9, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-.75, yend=-.75), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.04, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2,y=-2.5,label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=19), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=18,label=text), size=6, family = "Charis SIL", check_overlap=T) + 
  #
  facet_wrap(~syl, ncol=2) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Charis SIL"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Charis SIL"), strip.text = element_blank())
print(monosyl_examp_1_plot)
```

The periodic energy data that is extracted from acoustic recordings of speech is viewed in terms of a *mass*, i.e., the area under the periodic energy curve, integrating duration and power as the two linked dimensions of quantity in sound [see @turk1996processing on interactions between duration and intensity in linguistic perception contexts]. 
This is using a summing strategy, as opposed to averaging or peak extraction, to represent the quantity of sonority.
Summing takes measurements from the whole duration of a unit, allowing it to accumulate the individual measurements at each time point, and, in contrast to averaging, to consider the contribution of duration at the final calculation (rather than normalize over it).
Summing is therefore essentially different from averaging, as well as from peak extraction, in that it is capable of uncovering the quantitative difference between two periodic sounds that have similar amplitude envelopes yet they differ in duration. 

The contribution of duration to sonority was convincingly illustrated in the seminal work of @price1980sonority, whereby the perception of disyllabic English words like *polite* /pəlʌɪt/ was shown to take place when the duration of the sonorant /l/ in the superficially related monosyllabic word *plight* /plʌɪt/ was manipulated, essentially leading to the perception of another syllable when the duration of the sonorant was increased [see also @berent2007we]. Importantly, the periodic energy mass, which is the integral of power and duration, is the only measurement of the three basic alternatives for continuous curve measurement---sum, average or peak---that is capable of accounting for the contribution of duration to perception of sonority.

It is therefore useful to locate the *center of mass* within regions of interest as a measurement that is sensitive to the two axes of the periodic energy mass---duration (x-axis) and power (y-axis). The center of mass can be viewed as the point in time in which the area under the curve is split into two equal parts. The location of the center of mass in time (x-axis) is attracted to the peak of the curve (on the y-axis), where it is expected to be found with a perfectly symmetrical shape. However, the center of mass most often diverges from the peak of the main rise-fall fluctuations so as to reflect asymmetries in the overall distribution of the mass, either leftward or rightward. The center of mass of the periodic energy curve (henceforth CoM) follows a methodology that was introduced with the *Tonal Center of Gravity* [@barnes2012tonal] by calculating a weighted average time point that uses a continuous time series as the weighting term. The equation in \@ref(eq:com) is used to locate the average point in time (*t*), weighted by continuous periodic energy (*per*) at discrete time points:

\begin{equation}
  CoM = \frac{\sum_i per_i\cdot t_i}{\sum_i per_i}  \label{eq:com}
\end{equation}

<!-- \begin{equation} -->
<!-- \frac{\sum_i per_i\times t_i}{\sum_i per_i}  \label{eq:comasd} -->
<!-- \end{equation} -->

The location of the center of periodic energy mass of the entire syllable (henceforth *CoM~syllable~*) guides us to the point in time where the periodic mass of all the competing forces within that syllable are split into two equal parts. Once we obtain this reference point we can repeat this process within the resulting left-side portion, i.e., from the beginning of the syllable up to *CoM~syllable~*, to focus on the onset position (henceforth *CoM~onset~*).
We therefore measure the center of mass twice---first for the entire syllable (resulting in *CoM~syllable~*) and then for the left portion of the first measurement (resulting in *CoM~onset~*). 
The distance between *CoM~syllable~* and *CoM~onset~* is indicative of the amount of displacement of energy away from the center of the syllable, which is reflective of the degree of nucleus competition (see Figure \@ref(fig:com-4examples-1)).
<!-- [^cf-loess] -->

The center of mass is capable of capturing both components of a two-dimensional mass, considering the non-linear shape of the periodic energy curve. 
The leftward displacement of *CoM~onset~* relative to *CoM~syllable~* is affected by the distance, the amplitude, and the amount of discontinuity between the periodic energy at the onset and the center of mass of the entire syllable.
Any increase in the above results in a larger distance between the two centers of mass as Figure \@ref(fig:com-4examples-1) demonstrates.


#		Experimental assessments {#sec:experiments}

In order to be able to assess NAP's predictions in situations where both bottom-up and top-down inferences contribute to incoming speech processing, an experimental procedure was designed to collect behavioral responses using a perception task. In what follows we present a series of perception experiments: (i) an exploratory pilot study with 12 German-speaking subjects; (ii) a confirmatory study with 51 German-speaking subjects; and (iii) a second confirmatory study with 33 Hebrew-speaking subjects (each group of subjects listened to stimuli of a native speaker from the other language to retain the effects of a foreign accent).[^cf-helsinki] 

<!-- Note that all the media and original code that was used in this study are available at the following link on *Open Science Framework*: https://tinyurl.com/y3l4avap. -->
<!-- (anonymized link): https://tinyurl.com/y4gvobyp  -->

##		Rationale {#sec:rationale}

The goal of the experimental procedure is to tap into the cognitive cost that syllabification of different sequences incurs. To that end, we devised a forced-choice task that allows us to systematically compare response times of forced categorical decisions. Response times are linked with cognitive cost, which, in the context of the task, is understood as the result of syllabic ill-formedness. The working assumption is that worse-formed structures are harder to parse as a single syllable, which is reflected in a slower process altogether.

This design uses nonce words to test different consonantal combinations in comparable structures. This type of experimental design is reminiscent of many of the experiments on sonority effects that Iris Berent and her colleagues have been publishing, starting with the seminal @berent2007we.[^cf-berents] 
In their studies, the misperception and confusion between alternatives (e.g., *lbV*/*lə.bV*) is expected to be greater with worse-formed sonority clusters (e.g., greater confusion between *lbV*/*lə.bV* compared to *blV*/*bə.lV*), leading to a drop in categorical accuracy (e.g., a drop in correct identification of syllable number, or correct detection of similarity in a same/different task) accompanied by a grdual increase in response time.

[^cf-berents]: Examples of further publications by Berent et al. with various experimental settings that test sonority effects in perception with behavioral data include: @berent2008language; @berent2010phonological; @berent2011syllable; @berent2012language; @berent2013phnological; @tamasi2014sensitivity; @zhao2015universalsk; @lennertz2015onthesonority. The following examples include also neurological data: @berent2014languagesk; @gomez2014language; @berent2015role. 

In a NAP-based model we assume that syllabic well-formedness is tightly related to the nucleus competition between different portions of a syllable, such that response times will reflect the degree of nucleus competition within syllables (more competition = slower responses = worse-formed sequence). With traditional sonority models we interpret the processing cost as related to well-formedness in terms of sonority slopes, such that worse-formed clusters are more likely to be misperceived and take longer to process [e.g. @berent2007we; @berent2008language; @berent2009listeners; @berent2012language; @lennertz2010people; @maionchi2015sonoritysk; @sung2016perceptionsk; @young2017markednesssk]. 

To test the different predictions of the six sonority models (2$\times$NAP, 2$\times$SSP, 2$\times$MSD), we designed a perception task that prompts meta-linguistic syllable count judgement with 29 experimental target items. 
Participants were presented with a collection of speech items that were produced with one or two vowels, systematically for each combination of consonants in our set. 
Only the single-vowel productions were considered as targets, and an accurate response to our targets is always the monosyllabic option (note that the term "accuracy" is used here to describe participants' responses with respect to predictions).
By focusing on the response time of "correct" responses to the target words we, in fact, measure the time it took participants to decide that a given single-vowel stimulus is monosyllabic. 
We can therefore interpret the reaction times of monosyllabic responses to single-vowel targets as reflective of the processing cost of assigning one nucleus to a given target stimulus with one vowel.

As detailed below, the experiments were designed as syllable count tasks, i.e., a meta-linguistic forced-choice task. We assume that this design activates top-down inferences which urge subjects to think about possible syllables in their language. However, since we want to also tap into subjects' bottom-up inferences we present these nonce words as real words in another unkown language, uttered by a native speaker of that foreign language. For that matter, the German-speaking sbjects listened to a stimuli set that was comprised of recordings of a native Hebrew-speaking speaker, AA (the first author). Likewise, the Hebrew-speaking subjects listened to a stimuli set that was comprised of recordings of a native German-speaking speaker, HN.

Note that since the bottom-up predictions of NAP are derived via measurements of acoustic signals of particular productions rather than from fixed symbolic predictions, the assumption that all things other than the controlled variable are equal in the experimental stimuli should hold also for a large degree of variation that occurs in natural speech. Thus, if a certain segment in one item is slightly longer, shorter, louder or softer than in other comparable tokens, bottom-up NAP is designed to directly account for this variation, while the other symbol-based ordinal models essentially assume that such variation is mostly negligible as it cannot survive the abstraction into phonemes. This allows us to opt for a slightly more ecologically valid experimental paradigm, by using natural speech recordings that were designed and selected to sound as similar as possible, rather than using synthesized speech, which would have allowed a higher degree of similarity between tokens.


##		Materials {#sec:materials}

The experimental design is focused on onset consonantal clusters with two members. These CC combinations are composed from a set of consonants with *coronal* and *labial* places of articulation to avoid articulatory effects that may arise from *homorganic* sequences (i.e., adjacent consonants that share the same place of articulation) by exploiting both directions of each combination---coronal-labial (back-to-front) and labial-coronal (front-to-back). The consonantal classes in this experiment include *stops*, *fricatives*, *nasals*, and *liquids* to reflect the main classes in traditional sonority hierarchies, with the exclusion of *glides*.
See Appendix \@ref(appendix:a) for a list of considerations and criteria that we used in constructing the experimental stimulus set, and see the full stimulus set in Table \@ref(tab:targetlist).

(ref:targetlist-caption) (\#tab:targetlist) Experimental stimulus set: Onset cluster types in the experiment
(ref:targetlist-caption2) cor = coronal; lab = labial; * = voicing disagreement between obstruents; ** = no labial liquid; *** = dorsal stop /k/ (see list in Appendix \@ref(appendix:a))
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:targetlist-caption)}
\begin{tabular}{cclcclcclcclcclcclcclcclccl}
\toprule
\multicolumn{1}{r}{\textbf{C1}} & \multicolumn{2}{c}{\textbf{Voiceless}} & \multicolumn{2}{c}{\textbf{Voiced}} & \multicolumn{2}{c}{\textbf{Nasals}} & \multicolumn{2}{c}{\textbf{Liquids}}\\
\multicolumn{1}{l}{\textbf{}} & \multicolumn{2}{c}{\textbf{Fricatives}} & \multicolumn{2}{c}{\textbf{Fricatives}} & & & &\\
\multicolumn{1}{l}{\textbf{C2}} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor}\\
\midrule
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sp}, \textbf{ʃp}} & \multicolumn{1}{c}{\textbf{ft}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{np}} & \multicolumn{1}{c}{\textbf{mt}} & \multicolumn{1}{c}{\textbf{lp}} & \multicolumn{1}{c}{\textbf{lk}***}\\
\multicolumn{1}{l}{\textbf{Stops}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sf}, \textbf{ʃf}} & \multicolumn{1}{c}{\textbf{fs}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{nf}} & \multicolumn{1}{c}{\textbf{ms}} & \multicolumn{1}{c}{\textbf{lf}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiced}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{zv}} & \multicolumn{1}{c}{\textbf{vz}} & \multicolumn{1}{c}{\textbf{nv}} & \multicolumn{1}{c}{\textbf{mz}} & \multicolumn{1}{c}{\textbf{lv}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Nasals}} & \multicolumn{1}{c}{\textbf{sm}, \textbf{ʃm}} & \multicolumn{1}{c}{\textbf{fn}} & \multicolumn{1}{c}{\textbf{zm}} & \multicolumn{1}{c}{\textbf{vn}} & \multicolumn{1}{c}{\textbf{nm}} & \multicolumn{1}{c}{\textbf{mn}} & \multicolumn{1}{c}{\textbf{lm}} & \multicolumn{1}{c}{**}\\
& & & & & & & &\\
& & & & & & & &\\
\multicolumn{1}{l}{\textbf{Liquids}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{fl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{vl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{**}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:targetlist-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

Table \@ref(tab:targetlist) presents 29 CC sequence types that reflect 16 different combinations of classes (16 unique cells in Table \@ref(tab:targetlist), excluding differences in place of articulation), of which 7-8 are considered onset falls, 3-4 are considered onset plateaus (11 total), and 5 are considered onset rises.[^cf-plateaufall] Of the 29 different clusters, only 3 clusters regularly occur in German words, /ʃp, ʃm, fl/, 6 clusters are attested to some degree in German loanwords, /sp, sf, sm, vl, zv, ml/
<!-- ,[^cf-source],  -->
[@van2012sonority],
and one cluster, /ʃf/, may be considered as similar to German licit clusters with a voiced obstruent following a voiceless one (i.e., /ʃv/ and /t͡sv/). 
Thus, the experimental set contains 19 cluster types that are unattested in German words. These unattested CC types appear in 13 of the 16 unique combinations, excluding the three rising sonority clusters with a liquid in *C*~2~, /fl, vl, ml/, that are all attested in German complex onsets to some degree (yet only marginally so in the case of /vl/ and /ml/).

The different CC sequences were embedded within a /CCal/ word-like frame, with a recurring *-al* rime. These /CCal/ tokens were produced with a single vowel, intended to yield monosyllabic items that resemble typical content words [i.e., prosodically heavier than a single light syllable; see, e.g., @demuth1996prosodic]. Two disyllabic counterparts were prepared for each CC type---one with an epenthetic vowel, /CəCal/, and another with a prothetic vowel, /əCCal/ (a more accurate annotation should be /(ʔ)əCCal/, given that the presence of an initial glottal stop was not controlled for).
Note that the schwa in this case is produced as a weak (unstressed) central vowel, not necessarily a schwa.
The entire word set therefore includes 29 single-vowel target types and 58 associated bi-vocalic filler types, adding uo to 87 different word-like stimuli.

##		Predictions {#sec:predictions}

(ref:OrdinalTargetPreds-caption) (\#tab:OrdinalTargetPreds) Well-formedness scores for the 29 experimental items using the five ordinal models that are based on symbolic phonemes: SSP~col/exp~, MSD~col/exp~, and NAP~td~
(ref:OrdinalTargetPreds-caption2) Higher values predict better-formed onset clusters in an ordinal scale (i.e., magnitude of differences between values cannot be inferred from these models).
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:OrdinalTargetPreds-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{Onset cluster types} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{exp}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{exp}}}} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\
\midrule
\multicolumn{1}{l}{\textbf{fl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{4 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{sm}, \textbf{ʃm}, \textbf{fn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{3 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{vl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zm}, \textbf{vn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{ml}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sf}, \textbf{ʃf}, \textbf{fs}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zv}, \textbf{vz}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{2}\\

\multicolumn{1}{l}{\textbf{nm}, \textbf{mn}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sp}, \textbf{ʃp}, \textbf{ft}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{lm}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{mz}, \textbf{nv}, \textbf{lv}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{0}\\

\multicolumn{1}{l}{\textbf{ms}, \textbf{nf}, \textbf{np}, \textbf{mt}, \textbf{lf}, \textbf{lp}, \textbf{lk}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{-1}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:OrdinalTargetPreds-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The full set of predictions for the 29 experimental targets is presented for all the symbol-based ordinal models (2$\times$SSP, 2$\times$MSD, and *NAP~td~*) in Table \@ref(tab:OrdinalTargetPreds), and for the signal-based continuous model (*NAP~bu~*) in Figure \@ref(fig:com-monosyl). Note that the scores of *NAP~bu~* are presented on a continuous ratio scale, with specific predictions for each token and consequential intervals between scores. The scores in *NAP~bu~* are not a generalization, rather, they are extracted from the specific set of recordings we measured, and they are expected to vary to some extents when measuring different tokens. We present the *NAP~bu~* scores for the two sets of stimuli used in the experimens: (i) A set spoken by a native Hebrew speaker (first author; see Figure \@ref(fig:com-monosyl)) and a set spoken by a native German speaker who is a trained phonetician (see Figure \@ref(fig:com-monosylHeb)).

(ref:com-monosyl) AA set (Hebrew speaker). Well-formedness scores in the continuous *NAP~bu~* model shown in terms of the distance betweeen the center of mass of the entire syllable, *CoM~syllable~* (red vertical lines), and the center of mass of the left portion, *CoM~onset~* (blue vertical lines). Periodic energy is represented by the black curve. Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). Items are oredered by score (from worse- to better-formed), going from left-to-right and from top-to-bottom.
```{r com-monosyl, fig.cap = "(ref:com-monosyl)", fig.width=7, fig.asp=.8, warning=FALSE, dev="cairo_pdf"}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info_AA[order(monosyl_info_AA$NAP_bu),]$syl)
monosyl_info_AA$syl <- factor(monosyl_info_AA$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info_AA, aes(x=t)) + ylim(-5.5,20) +
  xlab("Time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  # geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, check_overlap=T) + 
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) +
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  # geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, check_overlap=T) +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Charis SIL", check_overlap=T) + #family = "Charis SIL",
  #
  facet_wrap(~syl, ncol=5) +
  theme(text = element_text(family = "Charis SIL"), panel.background = element_blank(), axis.title = element_text(size = 12), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8), strip.text = element_blank())
  # theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="sans"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="sans"), strip.text = element_blank())
print(monosyl_plot_abs)

```

(ref:com-monosylHeb) HN set (German speaker). See previous figure (Figure \@ref(fig:com-monosyl)) for plot details.
```{r com-monosylHeb, fig.cap = "(ref:com-monosylHeb)", fig.width=7, fig.asp=.8, warning=FALSE, dev="cairo_pdf"}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info_HN[order(monosyl_info_HN$NAP_bu),]$syl)
monosyl_info_HN$syl <- factor(monosyl_info_HN$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info_HN, aes(x=t)) + ylim(-5.5,20) +
  xlab("Time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  # geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, check_overlap=T) + 
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) +
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  # geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, check_overlap=T) +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Charis SIL", check_overlap=T) + #family = "Charis SIL",
  #
  facet_wrap(~syl, ncol=5) +
  theme(text = element_text(family = "Charis SIL"), panel.background = element_blank(), axis.title = element_text(size = 12), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8), strip.text = element_blank())
  # theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="sans"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="sans"), strip.text = element_blank())
print(monosyl_plot_abs)

```


<!-- participantsStuff -->

```{r participants, fig.show="hide"}
library(ggplot2)
pie_theme <- theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face="bold"))

## pilot
subjects_pilot <- read.csv("data_tables/subjects/subjects_pilot.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_pilot, aes(x="",y="",fill=subject_age)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_pilot, aes(x=subject_age)) +
  geom_histogram(binwidth = 1)

pilot_mean_age <- round(mean(subjects_pilot$subject_age))
pilot_min_age <- round(min(subjects_pilot$subject_age))
pilot_max_age <- round(max(subjects_pilot$subject_age))
pilot_N <- length(subjects_pilot$subject_nr)
pilot_male <- length(which(subjects_pilot$subject_gender=="male"))

## main
subjects_main <- read.csv("data_tables/subjects/subjects_main.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_main, aes(x="",y="",fill=subject_age)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_education)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_handedness)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_german_native)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void()
#```
#```{r participants_plot, fig.cap = "(ref:participants)", out.width = c('50%', '50%'), fig.align = 'center', fig.show="hold"}
# fig.cap = "(ref:participants)", dev='png', fig.ext=c('png', 'large.png'), fig.height=c(2, 2), fig.width=c(2, 2), fig.show="hold"

# ggplot(subjects_main, aes(x="",y="",fill=subject_age)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) + pie_theme
# 
# ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) +
#   scale_fill_brewer(palette="Dark2") + pie_theme

```


##  Overarching Design

The details in the following analyses address three separate experimetns: 
(i) an exploratory pilot experiment with 12 German speaking subjects listening to stimulus set AA (Hebrew speaker), *Experiment 1*; (ii) a confirmatory experiment with 51 German speaking subjects listening to stimulus set AA, *Experiment 2*; and (iii) a confirmatory experiment with 33 Hebrew speaking subjects listening to stimulus set HN (German speaker), *Experiment 3*.


<!-- --- -->
### Designing the exploratory study 

Given the various novelties in our proposal, the methodologies for data collection, data extraction, and model implementation were first tested on a small body of real data that we collected before finalizing our methodologies (namely the model implementations in Subsection \@ref(sec:modelimp) and the various procedural details in 
Appendix \@ref(appendix:a)).
<!-- Section \@ref(sec:methods).  -->
We used this exploratory study to test our methodologies and to explore the possibilities for properly estimating nucleus competition in each of the NAP models. 

The exploratory study was administered in two versions, each with half of the fillers and all of the targets in one block, yielding a total of 58 data points per subject (29 fillers + 29 targets, no repetitions).
<!-- [^cf-pilot]  -->
The two different versions were evenly split between participants (each version was presented to six participants).

####  Participants (Experiment 1)

The exploratory study consisted of 12 subjects (two males and ten females), all native German-speaking students from the Technische Hochschule Köln, who volunteered to participate in the study. The experiment was administered in a quiet room at the institute's facility in Cologne. The mean age of participants in the exploratory study was 25 (21--30).

### Designing the confirmatory studies

Experiments 2 and 3 are the main confirmatory studies conducted after finalizing our hypotheses and methodologies with the data from Experiment 1. Each experimental block in Experiments 2-3 consisted of two repetitions of the target words (2 $\times$ 29 $=$ 58) and one trial of each filler word (1 $\times$ 58). The experiment consisted of two blocks with randomized trials, generating altogether four repetitions of the target words (4 $\times$ 29 $=$ 116) and two repetitions of the filler words (2 $\times$ 58 $=$ 116), yielding a total of 232 data points per subject. 

The difference between Experiment 2 and 3 concerns the native language of the subjects, and, as a consequence, the stimulus set in use. Experiment 2 tested German speaking subjects on stimulus set AA (Hebrew speaker), while Experiment 3 tested Hebrew speaking subjects on stimulus set HN (German speaker).

####  Participants: Experiment 2

Fifty-one native German speakers (29 female, 19 male, and 3 "other") participated in Experiment 2, of which 48 were monolingual (the 3 bilingual speakers had Polish, Low German, and Hebrew as their heritage language). The vast majority (30 out of 51) were between the ages 19-29. Eight participants were between 30-39, and 11 participants were between 40-59 years old. There were also two younger participants, between 13 and 18 years old. The vast majority (49 of 51 participants) were right-handed. Of the 51 participants, 34 were students at the University of Cologne who took part in the experiment at the sound attenuated booth of the phonetics laboratory. The other 17 participants took part in the experiment at three different locations---all small quiet rooms within private apartments. All subjects were paid five Euros for their participation. 

We excluded the responses from one participant who failed in our participant inclusion criterion requiring accuracy of at least 75% with bi-vocalic fillers. The bi-vocalic fillers of the forms /CəCal/ and /əCCal/ link correct responses to the disyllabic choice ("2"), and we expect relatively few monosyllabic choices ("1") in response to stimuli with two separate vowels. 
Indeed, the overall average accuracy of all the 51 participants, when responding to bi-vocalic filler stimuli, was 96%. The excluded participant achieved a much lower accuracy score for bi-vocalic fillers, nearing chance-level with 65%.

####  Participants: Experiment 3

TBA.

## Data analysis {#sec:datanlysis}

We use a Bayesian data analysis approach implemented in the probabilistic programming language  *Stan* [@Stan2018] using the model wrapper package *brms* [@R-brms_a; @R-brms_b] in *R* [@R-base].^[The complete list of *R* packages that we used is:  `r cite_r("bibs/r-references.bib")`.] An important motivation for using the Bayesian approach is that it    is easy to fit fully hierarchical models with the so-called "maximal random effect structure", which provide the most conservative estimates of uncertainty [@SchielzethForstmeier2009]. In all our models, we use regularizing priors, which we detail below. These priors are minimally informative and have the objective of yielding stable inferences [@chung2013weakly; @gelman2008weakly; @GelmanEtAl2017]. @NicenboimVasishth2016 and @VasishthEtAl2017EDAPS discuss the Bayesian approach in detail in the context of psycholinguistic and phonetic sciences research. We fit the models with four chains and 4000 iterations each, of which 1000 iterations were the burn-in or warm-up phase. In order to assess convergence, we verify that there are no divergent transitions, that all the $\hat{R}$  (the between- to within-chain variances) are close to one,  that the number of effective sample size are at least 10\% of the number of post-warmup samples, and we visually inspect the chains.

For the statistical models, we take into account that the traditional sonority models and the top-down version of NAP (i.e., *SSP~col~* , *SSP~exp~*, *MSD~col~*, *MSD~exp~*, and *NAP~td~*) are ordinal models, while the bottom-up version of NAP (*NAP~bu~*) is a continuous model. The ordinal models predict that certain groups of onset clusters will be better or worse-formed than other group depending on an ordinal score, but they do  not assume that the score will be equidistant with respect to its effect on the response variable, log-transformed response times. For this reason, the discrete scores of these models are assumed to have a monotonic effect on the log-response time in our task, that is, having a monotonically increasing or decreasing relationship with the log-response time, while  the distance between groups are estimated from the data [@burknerModelingMonotonicEffects2018]. 

In contrast,  *NAP~bu~* is modeled with a continuous predictor which is assumed to have a linear relationship with the log-response times. Finally, as baseline, we fitted a "null" model which assumes no relationship between the stimuli and the response times. 

All the models, included a random intercept and slope by subjects (except for the null model that included only a random intercept) and the following weakly regularizing priors: $Normal(6, 2)$ for the intercept, $Normal(0, 1)$ for the slope, $Normal_+(0,1)$ for the variance components, and $lkj(2)$ for the correlation between by-participant adjustments. The ordinal models have as well a Dirichlet prior for the simplex vector that represents the distance between the categories set to one for each of its parameters. 

We evaluate the models in three different ways: (i) estimation, (ii) descriptive adequacy, and (iii) model comparison.

### (i) Estimation {-}
We report mean estimates and 95\% quantile-based Bayesian credible intervals. A 95\% Bayesian credible interval has the following interpretation: it is an interval containing the true value with 95% probability given the data and the model [see, for example, @Jaynes1976; @MoreyEtAl2015]. 

### (ii) Descriptive adequacy {-}
We use posterior predictive checking to examine the descriptive adequacy or "fit" of the models [@shiffrinSurveyModelEvaluation2008]: the observed data should look plausible under the posterior predictive distribution of the models. The posterior predictive distribution of each model is composed of simulated datasets generated based on the posterior distributions of its parameters. Given the posterior of the parameters of the model, the posterior predictive distribution shows how other data may look like. Achieving descriptive adequacy means that the current data could have been predicted with the model. It is important to notice that a good fit, that is, passing a test of descriptive adequacy, is not strong evidence in favor of a model; in contrast, a major failure in descriptive adequacy can be interpreted as strong evidence against a model [@shiffrinSurveyModelEvaluation2008]. Thus, we use posterior predictive checks to assess whether the model behavior is reasonable and in which situations is not [see @gelmanBayesianDataAnalysis2013  for further discussion].

### (iii) Model comparison {-}
For model comparison, we examine the out-of-sample predictive accuracy of the different models using k-fold (k=15) cross validation stratified by subjects.^[Pareto smoothed importance sampling approximation to leave-one-out cross validation  [implemented in the package `loo`; @vehtariPracticalBayesianModel2017; @vehtariParetoSmoothedImportance2015] failed to yield stable estimates.] Cross validation evaluates the different models with respect to their predictive accuracy, that is, how well the models generalize to new data.

##  Results: Estimations

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp_pil <- results_txt(m_pilot$models$SSP, "moSSP")
est_MSD_exp_pil <- results_txt(m_pilot$models$MSD, "moMSD")
est_SSP_col_pil <- results_txt(m_pilot$models$SSP_obs, "moSSP_obs")
est_MSD_col_pil <- results_txt(m_pilot$models$MSD_obs, "moMSD_obs")
est_NAP_td_pil <- results_txt(m_pilot$models$NAP_td, "moNAP_td")
est_NAP_bu_pil <- results_txt(m_pilot$models$NAP_bu, "sNAP_bu")

```

```{r}
scores <- data_pilot %>% filter(!is.na(NAP_bu)) %>% 
  distinct(stimulus, NAP_bu) %>% 
  arrange(NAP_bu)

lpal_nap <- scores %>% filter(stimulus == "lpal") %>% 
  pull(NAP_bu) %>% round(0)
lkal_nap <- scores %>% filter(stimulus == "lkal") %>% 
  pull(NAP_bu)%>% round(0)
spal_nap <- scores %>% filter(stimulus == "spal") %>% 
  pull(NAP_bu)%>% round(0)

```

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp <- results_txt(m_german$models$SSP, "moSSP")
est_MSD_exp <- results_txt(m_german$models$MSD, "moMSD")
est_SSP_col <- results_txt(m_german$models$SSP_obs, "moSSP_obs")
est_MSD_col <- results_txt(m_german$models$MSD_obs, "moMSD_obs")
est_NAP_td <- results_txt(m_german$models$NAP_td, "moNAP_td")
est_NAP_bu <- results_txt(m_german$models$NAP_bu, "sNAP_bu")

```

<!-- ### Experiment 3: Estimation {#sec:results3} -->

```{r}
estHeb_SSP_exp <- results_txt(m_hebrew$models$SSP, "moSSP")
estHeb_MSD_exp <- results_txt(m_hebrew$models$MSD, "moMSD")
estHeb_SSP_col <- results_txt(m_hebrew$models$SSP_obs, "moSSP_obs")
estHeb_MSD_col <- results_txt(m_hebrew$models$MSD_obs, "moMSD_obs")
estHeb_NAP_td <- results_txt(m_hebrew$models$NAP_td, "moNAP_td")
estHeb_NAP_bu <- results_txt(m_hebrew$models$NAP_bu, "sNAP_bu")
```

<!-- #### Estimation {#sec:estimation1} -->
For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

<!-- **Experiment 1** -->

### Experiment 1 {-}
* For *SSP~col~*: `r est_SSP_col_pil`.
* For *SSP~exp~*: `r est_SSP_exp_pil`.
* For *MSD~col~*: `r est_MSD_col_pil`.
* For *MSD~exp~*: `r est_MSD_exp_pil`.
* For *NAP~td~*: `r est_NAP_td_pil`.
* For *NAP~bu~*: `r est_NAP_bu_pil`.

<!-- **Experiment 2** -->

### Experiment 2 {-}
* For *SSP~col~*: `r est_SSP_col`.
* For *SSP~exp~*: `r est_SSP_exp`.
* For *MSD~col~*: `r est_MSD_col`.
* For *MSD~exp~*: `r est_MSD_exp`.
* For *NAP~td~*: `r est_NAP_td`.
* For *NAP~bu~*: `r est_NAP_bu`.

<!-- **Experiment 3** -->

### Experiment 3 {-}
* For *SSP~col~*: `r estHeb_SSP_col`.
* For *SSP~exp~*: `r estHeb_SSP_exp`.
* For *MSD~col~*: `r estHeb_MSD_col`.
* For *MSD~exp~*: `r estHeb_MSD_exp`.
* For *NAP~td~*: `r estHeb_NAP_td`.
* For *NAP~bu~*: `r estHeb_NAP_bu`.

Notice that the posterior of the effect of well-formedness, $\hat\beta$, is not comparable across models. For the ordinal models, it represents the distance between two adjacent categories had they been equidistant, or in other words, $\hat\beta$ multiplied by the number of categories minus one represents the increase in log-scale between the first and the last category. This means that it is highly affected by the number of categories. For the continuous bottom-up model, $NAP_{bu}$, $\beta$ represents, the increase in log-scale for one unit in the well-formedness scale. To make it concrete, between, /lpal/ and /lkal/ there are `r lkal_nap -lpal_nap` units (`r lkal_nap` and `r lpal_nap` respectively); and between /lkal/ and /spal/ there are `r spal_nap  -lkal_nap` units (since their NAP scores are `r spal_nap` and `r lkal_nap` respectively). However, for all the models, $\hat\beta$ is negative, indicating that well-formedness is associated with faster responses. See Appendix \@ref(appendix:b) for the complete output of the models.

The results shown here reflect the final state of the models in the exploratory stage, which is the same as the state of the models in the confirmatory stage. Importantly, the results of the confirmatory studies, Experiments 2-3, which are statistically much more robust, remain consistent with those of Experiment 1, which had a relatively small number of observations. As such, Experiment 1 was not designed to distinguish between the models and it will not be shown here further in the presentation of the results. 

##  Results: Descriptive adequacy

<!-- ### Experiment 2: Descriptive adequacy -->
<!-- ### (ii) Descriptive adequacy {-} -->
<!-- ####  Descriptive adequacy {#sec:posterior2} -->
### Null models

The null models are shown in Figure \@ref(fig:NullFit) as baselines in the respective experiments. The slight differences in predictions for different clusters are due to individual differences in the accuracy. Recall that  we subset the response times conditional on the monosyllabic response ('1') to the forced-choice task. This means that when participants gave more monosyllabic answers for a specific  cluster, their adjusted intercept will have a greater influence on the predictions of the model for that cluster. In addition, clusters with fewer monosyllabic responses show more variability in their predictions (e.g., /lf/ vs. /fl/).

The model fit of the six sonority models is shown in Figures \@ref(fig:SSPcolFit)--\@ref(fig:NAPbuFit). The plots in these figures present the dispersion of the average response time results, depicted as red points for related CC clusters, vis-à-vis each models' predictions in the form of distributions, depicted with blue violins. The order of the stimuli, from left to right, follows from the models' scores such that predictions for better-formed clusters appear further to the right. 

(ref:NullFit) Null model fit: Observed mean log-transformed response times are depicted with red points, distribution of simulated means based on the null model are depicted with blue violins. Stimuli ordered from left to right according to their ascending well-formedness score in *NAP~bu~* (here in forced-ordinal scale for exposition purposes).
```{r NullFit,fig.cap = "(ref:NullFit)", warning=FALSE, fig.width=7, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}
predictions$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL", min.segment.length=.5) + 
    scale_x_discrete("Stimuli", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("log RT (Exp. 2)") +
    # ylim(600,1300) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.text.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), axis.title.x = element_blank(), plot.title = element_blank()) #plot.title = element_text(size=14, family = "Charis SIL"))
    # ggtitle("Null model fit")

predictions_heb$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT_heb, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL", min.segment.length=.5) + 
    scale_x_discrete("Stimuli", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("log RT (Exp. 3)") +
    # ylim(600,1300) +
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_blank()) #plot.title = element_text(size=14, family = "Charis SIL"))
    # ggtitle("Null model fit")

```

<!-- ### (ii.a) Traditional sonority models fit {-} -->
### SSP and MSD models {#sec:traditionalModelfit}

(ref:SSPcolFit) *SSP~col~* model fit: Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness.
```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1.2) +
    xlab(bquote(italic(SSP[col])~scores)) + ylab("log RT (Exp. 2)") +
        # ylim(600,1300) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL"))# +
        # ggtitle(bquote(italic(SSP[col])~fit))
   }

predictions_heb$SSP_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1.2) +
    xlab(bquote(italic(SSP[col])~scores)) + ylab("log RT (Exp. 3)") +
        # ylim(600,1300) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL"))# +
        # ggtitle(bquote(italic(SSP[col])~fit))
   }

```

(ref:SSPexpFit) *SSP~exp~* model fit (plot details are same as above).
```{r SSPexpFit,fig.cap = "(ref:SSPexpFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(SSP[exp])~scores)) +
       ylab("log RT (Exp. 2)") +
        # ylim(600,1300) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(SSP[exp])~fit))
   }

predictions_heb$SSP %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(SSP[exp])~scores)) +
       ylab("log RT (Exp. 3)") +
        # ylim(600,1300) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(SSP[exp])~fit))
   }

```

<!-- ### MSD models {#sec:MSDModelfit} -->

(ref:MSDcolFit) *MSD~col~* model fit (plot details are same as above).
```{r MSDcolFit,fig.cap = "(ref:MSDcolFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[col])~scores)) + ylab("log RT (Exp. 2)") +
        # ylim(600,1300) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[col])~fit))
   }

predictions_heb$MSD_obs %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[col])~scores)) + ylab("log RT (Exp. 3)") +
        # ylim(600,1300) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[col])~fit))
   }

```

(ref:MSDexpFit) *MSD~exp~* model fit (plot details are same as above).
```{r MSDexpFit,fig.cap = "(ref:MSDexpFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[exp])~scores)) + ylab("log RT (Exp. 2)") +
        # ylim(600,1300) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[exp])~fit))
   }

predictions_heb$MSD %>%
  left_join(data_RT_heb) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[exp])~scores)) + ylab("log RT (Exp. 3)") +
        # ylim(600,1300) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[exp])~fit))
   }

```


We consider a good fit in the case of the ordinal models to be roughly characterized by the following three criteria:
(i) the data is contained within the predictions, i.e., the red points appear within the respective violins;
(ii) the data is consistent within each predicted level, i.e., the vertical dispersion of red points pattern together around the same area within each level (preferably in the middle of the distribution), and;
(iii) the model predictors are not redundant, i.e., the violins of the different model levels show little overlap between them.

A quick observation at the four Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) reveals a common failure of all the traditional sonority models to contain the nasal plateaus (/mn/ and more so /nm/) within their predicted distribution alongside all the other plateaus ('0' model score in all these figures). Furthermore, the data within the plateau level appears to be widely dispersed. 

A closer observation at the two left-most columns in Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit), which reflect the onset fall and onset plateau levels, presents a comparison between the two sonority hierarchies---*col* (*SSP/MSD~col~*) and *exp* (*SSP/MSD~exp~*). The difference between these two sonority hierarchies results in different allocation of the fricative-stop clusters /ʃp, sp, ft/, as plateaus in the *col* hierarchy ('0' in Figures \@ref(fig:SSPcolFit) and \@ref(fig:MSDcolFit)) and as falls in the *exp* hierarchy ('-1' in Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). The latter case of the *exp* hierarchy, whereby the fricative-stop clusters are treated alongside the most ill-formed onset falls, leads to two distinct patterns in the vertical dispersion of data at the left-most column of onset falls (Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). 
This suggests that the *col* hierarchy (where all obstruents are grouped into one class on the sonority hierarchy such that fricative-stop clusters are considered plateaus) is better than the *exp* hierarchy in treating fricative-stop clusters, as reflected in the better model fits for onset falls and plateaus when the *col* hierarchy is applied (*SSP/MSD~col~* vs. *SSP/MSD~exp~*). However, the two sonority hierarchies also lead to differences in the grouping of onset rises when the MSD models are taken into account, as we observe next.

The right side of these plots, i.e., the columns that reflect well-formed onset rises with positive model scores, present three types of grouping across the four models. The two SSP models (*SSP~col/exp~*) make identical predictions with respect to onset rises, lumping all rises into one category ('1' in Figures \@ref(fig:SSPcolFit)--\@ref(fig:SSPexpFit)). The vertical dispersion of data in the right-most column of the SSP models appears very wide, with voiced-initial clusters like /ml, vn, vl/ appearing to pattern separately from voiceless-initial clusters like /ʃm, sm, fl/.
In contrast, the MSD models present multiple levels of well-formedness for onset rises. *MSD~col~* exhibits two levels of rises ('1--2' in Figure \@ref(fig:MSDcolFit)) while *MSD~exp~* exhibits four levels of rises ('1--4' in Figure \@ref(fig:MSDexpFit)). The grouping in *MSD~col~* appears to result in a relatively high overlap between the distributions of the three right-most columns (including onset plateaus), suggesting redundancy in the model. In comparison, the grouping in *MSD~exp~* seems to capture the distinct patterns of onset rises more accurately, with more levels yet less overlap. Therefore, it appears that the combination of the *exp* hierarchy with the MSD (*MSD~exp~*) has the best fit with respect to onset rises.

To conclude, an observation of the model fit of the four traditional sonority models brings up a mixed picture: the *col* hierarchy (*SSP/MSD~col~*) appears to result in a better fit with onset falls and plateaus, while *MSD~exp~* seems to have the best fit with onset rises. The advantage of *col* with respect to onset falls and plateaus is related to the treatment of voiceless fricative-stop clusters as better-formed (onset plateau rather than of onset fall). The advantage of *MSD~exp~* with respect to onset rises is related to its ability to separate the voiceless-initial from the voiced-initial onset rises, such that voiceless-initial rises are better-formed. These selective and partial advantages, as we detail next, are built-in into the logic of the NAP models.

<!-- ### (ii.b) NAP~td~ model fit {-} -->
### NAP models {#sec:NAPtdModelfit}

(ref:NAPtdFit) *NAP~td~* model fit (plot details are same as above).
```{r NAPtdFit,fig.cap = "(ref:NAPtdFit)", warning=FALSE, fig.width=8, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", size =1.2)+
  xlab(bquote(italic(NAP[td])~scores)) + ylab("log RT (Exp. 2)") +
      # ylim(600,1300) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
      # ggtitle(bquote(italic(NAP[td])~fit))
  }

predictions_heb$NAP_td %>% 
  left_join(data_RT_heb) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", size =1.2)+
  xlab(bquote(italic(NAP[td])~scores)) + ylab("log RT (Exp. 3)") +
      # ylim(600,1300) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
      # ggtitle(bquote(italic(NAP[td])~fit))
  }

```

Although *NAP~td~* is an ordinal model like all the traditional sonority models, it follows a different rationale (see Subsection \@ref(sec:naptdmodel)), whereby the distinct categories of the model estimate nucleus competition to reflect well-formedness. Figure \@ref(fig:NAPtdFit) shows that *NAP~td~* succeeds in containing all the data (points) within the respective predictions (blue violins). 
A comparison with Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) shows that *NAP~td~* is the only model to achieve this.

However, *NAP~td~* appears to exhibit some redundancy, as suggested by the large degree of overlap between some of the predictive distributions of the model. This is apparent from the overlap between violins in the two right-most columns (3, 5) as well as the three left-most columns (-1, 0, 1) in Figure \@ref(fig:NAPtdFit).

<!-- ### (ii.c) NAP~bu~ model fit {-} -->
<!-- ### NAP~bu~ models {#sec:NAPbuModelfit} -->
(ref:NAPbuFit) *NAP~bu~* model fit (plot details are same as above).
```{r NAPbuFit,fig.cap = "(ref:NAPbuFit)", warning=FALSE, fig.width=7, fig.asp=.28, fig.show="hold", dev="cairo_pdf"}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Charis SIL") +
    scale_x_discrete(bquote(italic(NAP[bu])~scores~(AA~set)), breaks=NULL) +
    ylab("log RT (Exp. 2)") +
    # ylim(600,1300) +
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.x = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title.x = element_blank(), axis.title.y = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
    # ggtitle(bquote(italic(NAP[bu])~fit))

predictions_heb$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT_heb, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Charis SIL") +
    scale_x_discrete(bquote(italic(NAP[bu])~scores~(HN~set)), breaks=NULL) +
    ylab("log RT (Exp. 3)") +
    ylim(600,1300) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
    # ggtitle(bquote(italic(NAP[bu])~fit))

```

*NAP~bu~* is different from all the other models in that it presents scores that are specific to each token in a continuous rather than ordinal scale (i.e., the distances between scores in the model are also predicted). See Figure \@ref(fig:NAPbuFit) where the expected negative correlation between response time and well-formedness appears to generally hold for both the predictions and the data of the model fit of *NAP~bu~*.
Our criteria for goodness of fit based on the plots (see opening Subsection 
\@ref(sec:results2)) (iia)
<!-- \@ref(sec:traditionalModelfit))  -->
are not all valid when evaluating *NAP~bu~* since we have no classes and no vertical dispersion of data (points) within levels, and since the horizontal overlap of predictions (violins) between levels requires different interpretation. However, the criterion for inclusion of data within the model's predictions naturally also holds for the *NAP~bu~* fit, which fails to include the data for the nasal plateaus /nm/ and /mn/ within their respective predictive distribution (a failure that is shared by the traditional models; Subsection 
\@ref(sec:results2)) (iia).
<!-- \@ref(sec:traditionalModelfit)).  -->
Furthermore, *NAP~bu~* also fails to include the /z/-initial clusters---/zm/ and /zv/---within their respective predictive distribution.

(ref:SonFitAll) Combined sonority models fit (small reduced versions, see detailed versions above). Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness. 
```{r SonFitAll,fig.cap = "(ref:SonFitAll)", warning=FALSE, out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.5,.5,.5,.5,.5,.5), fig.show="hold", fig.align = "default"}
# fig.ncol=2

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1) +
    xlab(bquote(italic(SSP[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[col])~fit))}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[col])~fit))}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(SSP[exp]))) +
       ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[exp])~fit))}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[exp]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[exp])~fit))}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    # geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha = .7, size =1)+
  xlab(bquote(italic(NAP[td]))) + ylab("Response time (log scale)") +
      ylim(600,1200) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
      ggtitle(bquote(italic(NAP[td])~fit))}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1) +
    # geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Times") +
    scale_x_discrete(bquote(italic(NAP[bu])), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
    ggtitle(bquote(italic(NAP[bu])~fit))

```

The failures in the fit of the *NAP~bu~* model can be split into two types:
(i) nasal-initial clusters---*nval*, *nmal*, and *mnal*---which received results on a par with the slowest responses in the data, reflecting an overestimation of well-formedness by the model, and;
(ii) syllables beginning with a voiced sibilant---*zval* and *zmal*---which received results that pattern with faster responses, reflecting well-formedness underestimation by the model. 
These results may be taken to suggest that the distinctive high-frequency aperiodic energy of sibilants has a top-down repeller effect on the nucleus, despite additional periodic energy, while nasals, that can attract the nucleus in German (syllabic nasals occur in German), have a top-down attraction effect on the nucleus.[^cf-td]  

##  Results: Model comparison
<!-- ### Experiment 2: Model comparison -->
<!-- ### (iii) Model comparison {-} -->
<!-- #### Model comparison {#sec:ModelComparison2} -->

While the model fits give us an insight into the behavior of each model with respect to the data, they are not well-suited to compare the different models against a consistent criterion. To do that, we run out-of-sample predictions using cross-validation to test each model's ability to predict unseen items.
Results of the model comparisons (see Table \@ref(tab:resultsmodels)) reveal a very clear advantage of *NAP~td~*
over all the other models. 
The difference in elpd scores from the next three models---*SSP/MSD~col~* and *NAP~bu~*---is around -90, which is about 6 times larger in absolute terms than the difference in standard errors of these three models, which is about 15. This is interpreted as a very robust lead for *NAP~td~*. 
The small differences between the next three models (*SSP/MSD~col~* and *NAP~bu~*) make them indistinguishable in second place.
The two traditional models that are based on the *exp* hierarchy---*SSP/MSD~exp~*---are similar to each other at the third place, only marginally better than the null model.

```{r resultsmodels, results = "asis"}

comparison <- loo::loo_compare(x=kfold_german)
ll_matrix <- map2_dfc(kfold_german, names(kfold_german), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_german)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison (Experiment 2)",note= "The table is ordered by the expected log-predictive density (elpd) score of the models, with a higher score indicating better predictive accuracy. The highest scored model is used as a baseline for the difference in elpd and the difference standard error (SE). The column weight represents the weights of the individual models that maximize the total elpd score of all the models.")


comparison <- loo::loo_compare(x=kfold_hebrew)
ll_matrix <- map2_dfc(kfold_hebrew, names(kfold_hebrew), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_hebrew)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison (Experiment 3)",note= "(details are same as above).")


```

The right-most column in Table \@ref(tab:resultsmodels), *weight*, shows model averaging via stacking of predictive distributions. Stacking maximizes the potential elpd score by pulling the predictions of all the different models together. The values under the *weight* column represent the relative contribution of each model to this combined optimal model.
*NAP~td~* alone contributes the lion's share with 65% and *NAP~bu~* comes second with 14%.
All the other traditional models together contribute only 11% to this picture.

Model weights can be informative with respect to complementarity vs. redundancy because the ability to contribute to the maximized score entails some uniqueness. This can explain why the two *exp*-based models, *SSP/MSD~exp~*, did not contribute at all with zero weight. Looking at the model fits in Subsection 
\@ref(sec:results2) (ii), 
<!-- \@ref(sec:posterior2),  -->
it stands to reason that the main apparent advantage of the *exp* sonority hierarchy over the *col* sonority hierarchy in predicting onset rises is already subsumed by *NAP~td~*'s predictions, such that *SSP/MSD~exp~* had no unique contribution to add when stacked alongside superior *SSP/MSD~col~* and *NAP~td~* together.

The fact that the highest ranked NAP model in this comparison, *NAP~td~*, did not completely neutralize the contribution of *NAP~bu~* is telling, especially given that *NAP~td~* subsumed more of the traditional models, although it is more similar to *NAP~bu~* in principle. This is evident from Table \@ref(tab:resultsmodelsohnenapbu), in which we took *NAP~bu~* out of the equation, resulting in 88% contribution by *NAP~td~* and only 3%  contribution by *SSP~col~*.


### Discussion {#sec:discussion2} 

The results of the confirmatory study (Experiment 2) can be summarized as follows: 
(i) all of the sonority models we tested are capable of explaining the response time data for different consonant clusters to some extent; 
(ii) the symbolic top-down NAP model, *NAP~td~*, outperforms all the the other models;
(iii) some interesting differences between *col* and *exp* sonority hierarchies were observed, where the advantages of the minimal *col* sonority hierarchy proved to be more effective; 
(iv) the traditional sonority models appear to be largely subsumed by the symbolic *NAP~td~* model; 
(v) the combined contribution of the top-down and bottom-up NAP-based models (*NAP~td/bu~*) appears to be complementary to some degree.

The success of our NAP models relative to the traditional models in predicting the data can be mainly attributed to the following traits of NAP:
(i) all the voiceless-initial onset clusters, including onset falls and plateaus (e.g., /sp/ and /sf/), are relatively well-formed in NAP, correctly predicting the patterning together of such data at the low-right parts of the plots (faster response times);
(ii) onset rises like /ml/, nasal plateaus (/nm/ and /mn/), and onset falls like /lm/ pattern together as similar and relatively ill-formed in NAP, correctly predicting the data, whereby sonorant-initial plateaus and rises do not pattern with (better-formed) obstruent-initial plateaus and rises.

A simplified generalization that may illustrate this state of affairs in symbolic terms would be to claim that the sonority intercept of onset clusters appears to be more impactful than the sonority slope in determining syllabic well-formedness (i.e., the starting level of the cluster is more predictive of well-formedness than the angle of the cluster's slope).

#		General discussion {#sec:genDiscussion}

Our experimental results provide strong support for our synergy of proposals, including our choice of sonority's perceptual basis and acoustic correlate, the incorporation of continuous entities and dynamic procedures in phonological models, and the dual-route modeling strategy that accounts for both top-down and bottom-up inferences with separate compatible machinery. The following subsections discuss two important implications that are borne out of our interpretation of the results: 
(i) in the following subsection (\@ref(sec:dichotomies)) we discuss the complementarity of discrete and continuous modes in cognitive modeling, suggesting that the top-down--bottom-up distinction exhibits a better fit with the discrete--continuous dichotomy than the classic phonetics--phonology dichotomy;
(ii) in Subsection \@ref(sec:division) we discuss a potential phonotactic division of labor, demonstrated with a more holistic account of /s/-stop clusters.

##		Reshuffling dichotomies  {#sec:dichotomies}

This work rejects the classic dichotomy between phonetics and phonology, whereby continuous phenomena are considered phonetic, while phonology is exclusively modeled with discrete terms. This problematic dichotomy implies that perception and articulation are continuous while cognition is discrete, a perspective that, indeed, has characterized many areas of the cognitive sciences from the second half of the twentieth century.
The antithesis of this discrete view of the mind [often dubbed "the computer metaphor of the mind"; see @searle1990cognitive] motivated a shift in cognitive sciences towards more continuous and dynamic models, which has already amassed a compelling body of evidence to support it [e.g., @barlow1972single; @rosen1992temporal; @laks1995connectionistsk; @port1995mind; @case1995evaluation; @kelso1997dynamic; @pouget2003inference; @greenberg2003temporal; @spivey2007continuity; @ghitza2011linking; @giraud2012cortical; @lancia2013interaction; @poeppel2014current].

We assume here that perception and cognition are continuous processes that can activate categorical
representations [see @case1995evaluation; @gafos2006dynamics; @lancia2013interaction; @roessig2019dynamics for derivation of linguistic categories from continuous inputs].
These categorical representations are the learned symbols of the system in abstract terms, readily equated with co-activation of neural populations in the brain in physical terms [see, e.g., @friederici2011brain; @mesgarani2014phonetic; @tang2017intonational].
Once learned and established, symbols can feed top-down inferences that play a role alongside bottom-up inferences in perception [see e.g., @connor2004visual; @pinto2013bottom; @shuai2014temporal].

@tuller1994nonlinear, @gafos2006dynamics, @gafosbenus2006dynamics, and @roessig2019dynamics serve as good examples for the useful application of *attractor landscape* models in order to link linguistic categorical reflexes with continuous variables. They show that the incorporation of continuity in phonology is not only possible but also advantageous for our understanding of the nature of discrete categories, as they tend to be manifested in fuzzy distributions given multiple (redundant) cues and individual differences. 
It is therefore the case that previous work has already convincingly shown that a phonetics--phonology dichotomy does not fit well with a classic continuous--discrete dichotomy, as we have good reasons to incorporate continuous entities alongside discrete units within phonology.

In essence, works like those in @tuller1994nonlinear, @gafos2006dynamics, @gafosbenus2006dynamics and @roessig2019dynamics present a single model that incorporates continuous and symbolic aspects via the unifying mathematics of attractor landspaes. This is a necessary component in a system where continuous and symbolic entities are assumed to interact, modeling the manner in which the two types of entities can relate to each other. However, this cannot be an exhaustive description of a language system as it lacks the ability to explain the states of the system within which interactions occur (e.g., it cannot explain or predict the shape and behaviour of the attrctor landscape or address the limitations on dynamic events that the system can reliably detect).

Our task here is therefore different, yet related and complementary from a holistic system-wide perspective as we are interested in studying the effects of signal-based and symbol-based processes outside of their interaction.
In this paper we model the effects of processes that respond to signal-based continuous stimuli as bottom-up processes, and we separately model the effects of processes that are initiated by symbol-based discrete units, which we consider as top-down. 
Our work claims that the continuous--discrete dichotomy in phonology should be linked to the bottom-up--top-down dichotomy, such that both of these two distinct processes can *coexist* in a single language system.
It is therefore important to highlight the difference between them.
Bottom-up routes in perception are based on continuous stimuli and they are functional, in the sense that they adhere to the laws of physics and to the limitations of the sensorimotor system of the agents. 
Bottom-up processes that seem to systematically characteraize language processing may be taken to imply an evolutionary benefit that they embody with respect to reliable communication.
In contrast, top-down inferences in perception are based on the history of symbolic representations that speakers can learn from experience. 
This learning ability has its own universal functional limitations (e.g., memory-related capacities) but the learned links between the dynamic and symbolic modes are largely arbitrary as they rely on the superficial history presented by a given language system.
These symbols and their probabilistic distributions are constantly updated, reflecting knowledge about the distribution of categorically analyzable units of speech.

In this study we modeled the notion of sonority and its contribution to linguistic sound systems with the assumption that the two different routes---bottom-up and top-down---are both active when speech inferences take place. Our bottom-up model uses continuous data (periodic energy), dynamic principles (competition), and functional motivation (syllables carry pitch information) to model sonority. Our top-down model is based on generalizations over the discrete segmental units in the system and their distribution given bottom-up sonority restrictions (note that our top-down model is not a true statistical learner for reasons that we explain in Subsection \@ref(sec:complementary)). 
The results of model stacking based on our experimental data (see Table \@ref(tab:modelstacking)) support this move, showing that the combined contribution of the two NAP models is relatively complementary (65% + 14%), and, when taken together, they make 79% of the combined contribution of all the six sonority models to a maximized elpd score, reflecting the combined ability of the models to predict unseen forms (see Subsection 
\@ref(sec:results2)) (iii).
<!-- \@ref(sec:ModelComparison2)). -->

##		Phonotactic division of labor: A holistic account of /s/-stop clusters {#sec:division}

Our NAP account of /s/-stop clusters does not suffice to explain this phonotactic phenomenon since there is nothing in NAP that is specific to sibilants or stops that would make the combination of a sibilant and a stop consonant stand out from other comparable obstruent cluster combinations.[^cf-sstop] 
In fact, all the voiceless elements are practically invisible to NAP as it is only sensitive to portions of the speech signal that contain sufficient periodic energy. Indeed, the predictions of NAP, which were corroborated in the experiment, expect non-sibilant counterparts of /s/, like /f/ in the cluster *ftV*, to pattern with *spV* and *ʃpV*. Furthermore, NAP successfully predicts that all the voiceless-initial clusters in the experiment (including the /s/-stop clusters) generally pattern together as well-formed from NAP's point of view. This may suffice to explain why /s/-stop clusters are tolerated, but not why they are so often preferred over other obstruent combinations. The complete phonotactic story of /s/-stop clusters thus requires an integrative explanation, in which sonority only plays a limited role. 

One complementary explanation for /s/-stop clusters can be found in @wright2004review, where the notion of "cue robustness" serves to explain why sibilants, with their salient and distinctive high frequency aperiodic energy, can stand out more than other fricatives and allow effective recoverability from relatively weak marginal positions (i.e., distant from the vocalic nucleus). Although this explanation is also based on perception, cue robustness does not require the notion of the syllable and can be based on simpler adjacency relations. 

Furthermore, obstruent combinations such as a stop-stop clusters (e.g., *tpV*, *ptV*) entail different articulatory gestural coordination patterns than those entailed by fricative-stop clusters (e.g., *spV*, *ftV*). The former, stop-stop clusters, are expected to be dispreferred due to articulatory similarity effects in the production of two consecutive stops, echoing the formal OCP constraint in many traditional phonological accounts [@leben1973suprasegmental; @mccarthy1979formalsk], as a dissimilatory requirement banning two consecutive units of the same type.

These three phonotactic perspectives are complementary, and although they do not represent an exhaustive list of phonotactic pressures, we need at least these three---*sonority*, *cue robustness*, and *articulatory dissimilation*---in order to properly appreciate the phonotactic phenomenon of /s/-stop clusters.

# Conclusions  {#sec:conclusions}

This project suggests a synergy of novel theoretical and methodological approaches in an attempt to shed new light on old problems in linguistics. Naturally, the paradigm shift that we propose here for models of sonority, and for phonological models in general, will need to accumulate more supporting evidence from multiple sources in order for it to be widely considered and consequently developed further. 
In this paper we therefore lay the foundation for such potential long-term contribution. We demonstrated how our set of proposals results in a model of sonority that can account for some of the most persistent problems in phonological theory. 
Importantly, our NAP-based models not only present clear advantages over traditional sonority models in terms of empirical coverage, they also provide functional explanations as to the source and cause of sonority phenomena, linking sonority to pitch intelligibility at the level of the syllable. 
Furthermore, our dual-route strategy to modeling (see Subsection \@ref(sec:dichotomies)) makes the important distinction between (bottom-up) signal-based inferences and (top-down) symbol-based inferences, thus appropriately predicting the complementary contribution of these two essentially different inference routes of the same linguistic phenomena.

Lastly, this study also provides compelling motivations and strong evidence that support our proposal to link the notion of sonority with periodic energy in the acoustic signal. This proposal entails that the role of periodic energy is of great importance to linguistic analysis of speech, beyond the scope of the current proposal---a partial list of relevant topics includes:
(i) acoustic descriptions of prosodic *weight* and prosodic *prominence* in terms of periodic energy mass;
(ii) automatic syllabification procedures based on the smoothed fluctuation rates of the periodic energy curve [useful for a myriad of tasks, including speech rhythmicity studies; see, e.g., @galves2002sonoritysk; @tilsen2013speech; @rasanen2018pre; @lin2020hit], and;
(iii) improved visualization and analysis of pitch contours in intonation research based on the interaction of the periodic energy and the F0 trajectories 
(see Anonymous 2018; Anonymous 2019; and Anonymous 2020 for continuous visualization and quantification procedures in intonational phonology using periodic energy).
<!-- [see @albert2018using; @cangemi2019modellingsk; and @albert2020propersk for continuous visualization and quantification procedures in intonational phonology using periodic energy]. -->

<!-- --- -->

<!-- # CRediT authorship contribution statement {-} -->
<!-- **AA**: Conceptualization, Methodology, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision. **BN**: Conceptualization, Methodology, Formal Analysis, Writing - Review & Editing, Visualization.  -->

<!-- # Acknowledgements {-} -->
<!-- This work was supported by the DAAD (German Academic Exchange Service) and SFB 1252 Collaborative Research Center “Prominence in Language". -->

<!-- # Note {-} -->
<!-- All materials associated with the current article are available on *Open Science Framework* (OSF) at -->
<!-- https://tinyurl.com/5jxmjntx -->

<!-- footnotes -->

[^cf-liquid]: The group of *liquids* is the most loosely defined, as it includes both *lateral approximants* (namely /l/) and various types of rhotics such as *trills* (/r,ʀ,ʁ/), *taps* (namely /ɾ/), and alveolar and retroflex *approximants* (/ɹ,ɻ/). 
<!-- In this paper we only consider the lateral approximant /l/ as the relatively more stable and common token of this varied liquid type. -->

[^cf-sdp]: The *Sonority Dispersion Principle* [SDP; @clements1990role; @clements1992sonority] is a slightly different yet related principle that prefers onset rises with large distance and equal dispersion of sonority index values across the consonantal sequence and the following vowel. The results of the SDP are highly contingent on the given sonority hierarchy and it is not very clear how to apply the SDP with onset sonority falls [among other problems listed in @parker2002quantifying 22--24]. 

[^cf-list]: A partial list of some prominent examples includes @sigurd1955rank; @jakobson1956fundamentals; @chomsky1968spesk; @foley1972rule; @ladefoged1971preliminaries; @allen1973accentsk; @fujimura1975syllable; @Donegan1978onthenatural; @ultan1978typological; @price1980sonority; @lindblom1983production; @anderson1986suprasegmental; @vennemann1988preferencesk; @levitt1991syllable; @pierrehumbert1992lenition; @fujimura1997acoustic; @stemberger1997handbook; @boersma1998functional; @zhang2001effects; @howe2004harmonic; @clements2009does; @sharma2018significance.

[^cf-mora]: Moras are used to represent quantitative differences between light and heavy syllables (weight sensitivity), such that light syllables contain one mora while heavier syllables contain two (and sometimes even three) moras [see @hyman1984atheory; @mccarthy1990footsk; @hayes1989compensatory; @ito1989prosodic; @zec1995sonority; @zec2003prosodic].

[^cf-helsinki]: Informed consent from the participants was obtained before each experimental session. The experiment complied with the June 1964 Declaration of Helsinki (carried out by the World Medical Association and entitled “Ethical Principles for Medical Research Involving Human Subjects”), as last revised. In accordance with German Research Foundation (DFG) guidelines for experiments with unimpaired adult populations, the ethics approval is required by the Principal Investigator (in this case, Prof. Dr. Martine Grice).

[^cf-plateaufall]: Depending on whether fricatives are considered higher or similar in sonority to stops, clusters of the type fricative-stop may be considered as either an onset fall or an onset plateau.

[^cf-sstop]: The term */s/-stop cluster* is used here to refer to voiceless sequences of a sibilant, namely /s/ or /ʃ/, that precedes an oral stop, e.g., /p,t,k/.

[^cf-td]: Note that language-specific top-down biases, such as the question of syllabic consonants in a given language, are not covered by our top-down model, *NAP~td~*, which is limited to the universal aspects of NAP. That said, *NAP~td~* succeeded in fitting the data for /z/-initial and nasal-initial clusters within its predictions.


\newpage

# References
```{r create_r-references}
r_refs(file = "bibs/r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_a.Rmd') 
```

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_b.Rmd')
```