---
title             : "Take a NAP: A new model of sonority using periodic energy and the Nucleus Attraction Principle"
shorttitle        : "Take a NAP"

author: 
  - name          : "Aviad Albert"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Linguistics---Phonetics, University of Cologne, Herbert-Lewin-Straße 6, 50931 Cologne, Germany"
    email         : "a.albert@uni-koeln.de"
  - name          : "Bruno Nicenboim"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Department of Linguistics---Phonetics, University of Cologne"
  - id            : "2"
    institution   : "Department of Linguistics, University of Potsdam"

authornote: |
  <!-- Acknowledgement: The first author's research was supported by a scholarship from the DAAD. -->
  <!-- Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.  -->
  <!-- Enter author note here.  -->

abstract: |
  *Sonority* is a fundamental notion in phonetics and phonology. Sonority *hierarchies* are used to characterize all speech sounds along a single scale, which sonority *principles* (e.g., the *Sonority Sequencing Principle*; SSP) utilize to derive generalizations about the well-formedness of segmental sequences within syllables.
  Although widely-accepted, sonority hierarchies suffer from the lack of a clear basis in speech articulation or perception, and sonority principles suffer from a formal discrete architecture that is incompatible with many current cognitive accounts, and leads to gaps in empirical coverage.
  In this paper we present two novel proposals that effectively replace traditional sonority hierarchies and  principles, converging to form a radically different model of sonority, where we locate the phonetic basis of sonority in perception of pitch (measured in terms of *periodic energy* from the acoustic signal), and we present a dynamic and cognitively-plausible principle for inferring syllanic well-formedness---the *Nucleus Attraction Principle* (NAP).
  We provide two versions of NAP-based models: 
  (i) accounting for bottom-up inferences with continuous signals and: 
  (ii) accounting for top-down inferences using discretised consonanat and vowel representations.
  We present two perception experiments that we designed in order to test our two NAP-based models against four traditional SSP-based models. 
  We use a Bayesian data analysis approach to test and compare the different sonority models, and we show that our two NAP models are complementary to some degree and have a much better predictive accuracy than the traditional ones.
  We interpret the results as providing a strong support for our proposals: 
  (i) the choice of periodic energy as sonority's correlate; 
  (ii) the incorporation of continuity in phonological models and; 
  (iii) the dual-model strategy that separates and integrates top-down and bottom-up processes to account for symbolic and dynamic aspects of speech.

  
keywords          : "Sonority; Periodic energy; Bayesian data analysis; Bottom-up and Top-down cognitive processes; Phonetics--Phonology interface"
wordcount         : "X"

bibliography      : ["bibs/r-references.bib", "bibs/methods.bib", "bibs/phon.bib", "bibs/phon_sk.bib"]
appendix:
  - "./appendix.Rmd"
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : yes

documentclass     : "apa6"
classoption       : "doc"
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex
    includes:
      in_header: load.tex
keep_tex: yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r , cache=FALSE,include=FALSE}
# global chunk options
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE,fig.path='figure/graphics-', fig.align='center')#, dev="cairo_pdf")
```

```{r libraries, message = FALSE}
library(R.matlab)
library(ggplot2)
library(dplyr)
library(Cairo)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores =  parallel::detectCores())
library(stringr)
library(readr)
library(purrr)
library(tidyr)
library(loo)
library(brms)
library(ggrepel)
#library(linguisticsdown)
#library(extrafont)
```

# Introduction

*Sonority* is a fundamental notion in phonetics and phonology, playing a crucial role in accounts of the syllable in linguistic theory [as a good indication in support of this claim, @parker2018abib; cites 2,413 titles involving sonority, of which more than 250 works present sonority-related experiments, as reported in @parker2017sounding]. 
The topic of sonority can be roughly divided into two related theoretical constructs: 
(i) *sonority hierarchies* (or *scales*) and;
(ii) *sonority principles* (or *generalizations*). 
Sonority hierarchies allocate all speech sounds along a single scale, while sonority principles are universal generalizations about the well-formedness of syllables. Sonority principles require a sonority hierarchy to model the well-formedness of syllables given their underlying sequence of consonants and vowels.
Thus, a model of sonority is capable of predicting distributional patterns of consonants and vowels in all human language systems in terms of *phonotactics*---preferences and restrictions with regards to possible combinations of segmental sequences.

Sonority hierarchies and principles are designed to model speech in discrete terms. Thus, sonority models standardly express sonority in terms of integers that are associated to classes of consonants and vowels along an ordinal sonority hierarchy. Syllabic well-formedness is computed from the concatenation of these values in symbolic time (i.e., from linearly ordered non-overlapping symbols). This type of modeling lacks robust cognitive motivations as it assumes---explicitly or implicitly---that linguistic processing is analogous to the workings of a computer despite strong evidence to the contrary [see @spivey2007continuity; on the problems of fully discrete cognitive models, and see @searle1990cognitive for an earlier critique of the computer metaphor of the mind in cognitive sciences].

Although widely used and accepted, the notion of sonority remains vague and highly contested for various reasons. To date, no considerable consensus exists with respect to the phonetic basis of sonority in terms of a consistent articulatory or perceptual phenomenon that derive sonority distinctions, resulting in a multitude of different sonority hierarchies. Furthermore, the lack of a phonetically useful metric for sonority plagued most sonority models with inherent circularity as sonority hierarchies are 
often both determined and confirmed by attested segmental combinations, without recourse to any independently motivated phenomenon [@ohala1992alternatives].
On top of that, sonority principles such as the widely used *Sonority Sequencing Principle* (SSP) have been taken as axioms with formal definitions that lack an explicit functional motivation relating to speech articulation or perception. 
We claim that this choice of architecture in the SSP resulted in some of the persistent failures of SSP-based models, such as the unpredicted prevalence of /s/-stop clusters on the one hand, and the unpredicted rarity of sonorant plateaus on the other.

In this paper we present two novel proposals that effectively replace both traditional hierarchies and traditional principles, and converge to form a radically different model of sonority:
(i) we claim that sonority is linked to the *pitch intelligibility* of speech in perception, which can be effectively measured in terms of *periodic energy* in the acoustic signal, and;
(ii) we present a novel sonority principle, the *Nucleus Attraction Principle* (NAP), to account for syllabic well-formedness in terms of competition for the nucleus between different portions of the speech signal.
We use NAP with continuous measurements of periodic energy to model the bottom-up effects of sonority, alongside a complementary top-down model that utilizes NAP with standard symbolic representation of consonants and vowels.

Our set of proposals has many advantages over traditional sonority accounts, including methodological aspects, theoretical perspectives and, most importantly, a better empirical coverage.
We use dynamic mechanisms like *attraction* and *competition* in real time, to model sonority in line with general cognitive and sensorimotor mechanisms.
The link we pose between sonority and pitch intelligibility can be readily understood as the functional result of evolutionary pressures to optimize the use of pitch in the speech signal [see @christiansen2016creating for incorporation of evolution-size time scales in language models]. 
Thus, the universality of sonority in NAP is directly implicated by more general cognitive and sensorimotor processes given that all language systems evolved as pitch-bearing communication systems [@bolinger1978intonation; @cutler1997prosody; @roettger2019tune].

In the remainder of the Introduction, we briefly present the relevant background on sonority hierarchies and sonority principles, emphasizing their rationale, their application and their inherent flaws (subsection \@ref(sec:background)). We then discuss the long-standing open question regarding the phonetic basis of sonority hierarchies, where we present motivations to propose pitch intelligibility in perception and periodic energy in the acoustic signal, as sonority's perceptual basis and acoustic correlate (section \@ref(sec:dynamic)). 
Our proposal for determining syllabic well-formedness with NAP is presented in the following section (\@ref(sec:modelimp)), where we operationalize NAP with continuous periodic energy for the bottom-up NAP model, as well as with discrete consonants and vowels for the top-down NAP model.

In section \@ref(sec:experiments) we present two perception experiments---exploratory and confirmatory---that we designed in order to test our two NAP-based models (applying NAP with bottom-up and top-down approaches) against four traditional SSP-based models, considering two types of common sonority hierarchies (with and without distinctions between obstruents), and two types of computations that are typically used with the SSP (with and without *sonority distance* effects). 
We use a Bayesian data analysis approach to test and compare the six different sonority models. Whereas all the different models are found to be capable of predicting the experimental results to some extent, the two versions of NAP are shown to be superior, with the bottom-up version of NAP taking the clear lead in ability to make out-of-sample predictions using cross-validation. Furthermore, an inspection of the contribution of the individual models when maximizing their collective ability to make out-of-sample predictions finds that the bottom-up NAP model contributes 56% and the top-down NAP model contributes 31%, such that their combined contribution is complementary to a large extent. Together the two NAP-based models contribute 87% to the ability of all six models to successfully make out-of-sample predictions using cross-validation, rendering the additional contribution of the four different SSP-based models as relatively negligible. 

We interpret the results of the main confirmatory experiment (Experiment 2) in subsection \@ref(sec:discussion2), and we address the following issues in the ensuing general discussion (section \@ref(sec:genDiscussion)):
(i) we discuss the division of labor between sonority and other phonotactic factors, demonstrated with a holistic account of the phenomenon of /s/-stop clusters (subsection \@ref(sec:division));
(ii) we revisit the *nature vs. nurture* debate regarding sonority, where the formal restrictions of the SSP have been argued to be either necessarily encoded in genetics [reflecting an *innateness* approach, e.g., @berent2007we; @berent2008language; @reb2010mandarinsk; @lennertz2010people] or potentially learned directly from the lexicon [reflecting a *statistical learner* approach, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @frisch2001psychologicalsk; @hay2003speech; @hayes2008maximum; @albright2009feature; @futrell2017generative; @jarosz2017inputsk; @mayer2019phonotactic]. We show that bottom-up NAP subsumes the innateness approach while top-down NAP is generally in line with the statistical learner approach (subsection \@ref(sec:projection)) and; 
(iii) we discuss the idea of complementarity of discrete and continuous modes in cognitive modeling, in line with pioneering works in biosemiotics by Howard Pattee [see @pattee2012lawssk for an overview of Pattee's classic works with contemporary commentary], and in light of the supporting evidence 
for the complementarity of our top-down and bottom-up NAP models (subsection \@ref(sec:dichotomies)).
<!-- and finally; -->
<!-- (iv) We point at the future directions of this research program in subsection \@ref(sec:future), before  -->
Finally, we conclude this paper in section \@ref(sec:conclusions).

## Sonority background: Hierarchies and principles {#sec:background}

### Sonority hierarchies {#sec:hierarchies}

A sonority hierarchy is a single scale on which all consonants and vowels can be ranked relative to each other.[^cf2b]
Such hierarchies can be traced back centuries, and concepts akin to sonority hierarchies can be found already in the pioneering works of early Sanskrit grammarians.[^cf2] 
Early versions of current sonority hierarchies often date back to @sievers1893grundzugesk; @jespersen1899fonetik and @whitney1865relation, while some, e.g., @ohala1992alternatives, even go back to @debrosses1765traite.
A phonetic interpretation of the sonority hierarchy needs to be based on measurable phenomena in articulation or perception (or acoustics, by proxy), yet this type of endeavour suffers from the lack of consensus regarding the appropriate dimension of measurement (i.e., the  phonetic basis of sonority). Moreover, phonetic interpretations of the sonority hierarchy suffer from the fact that natural speech---even when elicited in a lab---is far too variable to conform with attempts to consistently associate certain consonants and vowels with fixed values on a single measurable scale.

While the phonetic basis of sonority hierarchies remains an arguable postulate, phonological sonority hierarchies have been primarily based on repeated observations that revealed systematic behaviors of segmental distribution and syllabic organization within and across languages. The general consensus regarding the phonological sonority hierarchy thus stems from attested cross-linguistic phonotactic behaviors of different segmental classes, such as, for instance, the relatively high frequency of stop-liquid sequences in the onset of syllables (e.g., the English word ***tr**ap*) and the rarity of the opposite liquid-stop sequences at such onset positions, but not at the coda of syllables (e.g., the English word *pa**rt***). See examples in @zwicky1972notesk; @selkirk1984majorsk; @parker2002quantifying; @jany2007universal, and see @ohala1992alternatives for related criticism regarding the resulting circularity when determining sonority hierarchies according to attested behavior.

The ordering of different speech sounds along the sonority hierarchy is assumed to be universal, i.e., based on perception and/or articulation, or even some formal linguistic "universal", yet the patterning of segmental classes as distinct groups along the scale is considered to be language-specific, i.e., based on phonological categorization. 
Thus, voiceless stops may be considered universally lower than voiced fricatives, yet for some languages and analyses they may constitute a single level of *obstruents*.
Classes along the sonority hierarchy are most commonly interpreted as a series of integers (often referred to as sonority indices) reflecting the ordinal nature of phonological interpretations of the sonority hierarchy. 

Most phonological sonority hierarchies group segment types into classes that are based on the standard *manner of articulation* categories in phonology. The distinct categories  commonly used include *stops*, *fricatives*, *nasals*, *liquids*, *glides* and *vowels*, often with additional distinctions such as voicing and vowel height. Although there are many different proposals for sonority hierarchies [@parker2002quantifying found more than 100 distinct sonority hierarchies in the literature], the most basic hierarchy that seems to reach a considerable consensus, and is often cited in relation to Clements' [-@clements1990role] seminal paper is given in (\@ref(ex:scale)).

\begin{exe} 
\ex Obstruents $<$ Nasals $<$ Liquids $<$ Glides $<$ Vowels  \label{ex:scale} 
\end{exe} 

The main differences that result from variation of this basic scale concern the class of obstruents, which may contain voiced and voiceless variants of stops and fricatives (to mention just the most prominent distinctions).[^cf3] It is not uncommon therefore to expand the class of obstruents, whereby stops are lower than fricatives and voiceless consonants are lower than voiced ones. The two variants of the sonority index values given in Table \@ref(tab:hierarchy) thus reflect two ends of a spectrum of this very common type of sonority hierarchies in the literature, ranging from hierarchies that collapse all obstruents together (i.e., they are ranked together on the sonority scale), to hierarchies that expand the class of obstruents by employing voicing distinctions as well as distinctions between stops and fricatives. In what follows we will refer to these two versions of the sonority hierarchy as *col* for the *collapsed* sonority hierarchy, and *exp* for the *expanded* sonority hierarchy.

(ref:hierarchy-caption) (\#tab:hierarchy) Traditional phonological sonority hierarchies
(ref:hierarchy-caption2) Index values reflect the ordinal ranking of categories in sonority hierarchies. The obstruents in *col* are collapsed into one category (bottom four rows = '1'), while in *exp* they are expanded into four distinct levels.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:hierarchy-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
\multicolumn{2}{c}{\textbf{Sonority index values}} & \multicolumn{1}{l}{\textbf{Segmental classes}} & \multicolumn{1}{l}{\textbf{Phonemic examples}}\\
\multicolumn{1}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\emph{exp} hierarchy} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}\\
\midrule
5 & 8 & Vowels & \multicolumn{1}{l}{/u, i, o, e, a/}\\
4 & 7 & Glides & \multicolumn{1}{l}{/w, j/}\\
3 & 6 & Liquids & \multicolumn{1}{l}{/l, r/}\\
2 & 5 & Nasals & \multicolumn{1}{l}{/m, n/}\\
\textbf{1} & \textbf{4} & Voiced Fricatives & \multicolumn{1}{l}{/v, z/}\\
\textbf{1}& \textbf{3} & Voiced Stops & \multicolumn{1}{l}{/b, d, g/}\\
\textbf{1}& \textbf{2} & Voiceless Fricatives & \multicolumn{1}{l}{/f, s/}\\
\textbf{1}&\textbf{1} & Voiceless Stops & \multicolumn{1}{l}{/p, t, k/}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:hierarchy-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}


### Traditional sequencing principles {#sec:principles}

Sequencing principles can be understood as a mapping between the ranks of a sonority hierarchy and the symbolic linear order of speech segments.
In contrast to sonority hierarchies that tend to be flexible to a certain extent, the  small family of sonority principles has seen very little change over the years. Modern formulations, which use the ordinal sonority hierarchy to generalize over the phonotactics of consonantal sequences in terms of *sonority slopes* were developed mainly throughout the seventies and eighties of the twentieth century, in seminal works such as @zwicky1972notesk; @hankamer1974sonority; @hooper1976introduction; @kiparsky1979metrical; @lowenstamm1981maximal; @steriade1982greek; @cairns1982markedness; @selkirk1984majorsk; @harris1983syllable; @mohanan1986theory and @clements1990role.

Sonority index values can be readily plugged into models that can predict distributional patterns of segments vis-à-vis prosodic organization in terms of sonority slopes. Consonants and vowels in a given string are interpreted as a sequence of discrete points in symbolic linear time. The corresponding sonority index values that are associated with these segments are then interpreted in terms of slopes that result from interpolation over the sequence of points. Thus, for instance, going from a low ranking segment to a high one is considered to be a rising slope, while two adjacent segments that share the same sonority index incur a plateau. The notion of a syllable is required to define the type of preferred slopes, which optimally rise from the beginning of the syllable to its middle and fall from the middle of the syllable to its end. Syllabic well-formedness is therefore defined in terms of universal generalizations over the preferred and dispreferred types of sonority slopes that result from the concatenation of different consonants and vowels and their grouping into syllables.

The most basic and widely used sonority-based account that derives phonotactic predictions in terms of syllabic well-formedness is the *Sonority Sequencing Principle* (SSP). The SSP is a simple yet powerful generalization about phonotactics that has been evidently useful in countless theoretical accounts. It identifies three distinct types of slopes---*rises*, *falls* and *plateaus*---such that sequences of segments should rise in sonority from the consonant(s) in the syllabic onset to the syllable's nucleus (most often a vowel) and fall from the nucleus to the consonant(s) in the syllabic coda. In this paper we focus on onset consonant clusters that precede a vowel in initial position, whereby a rising sonority slope (e.g., *blV*) is considered well-formed and a falling sonority slope (e.g., *lbV*) is considered ill-formed (see Figure \@ref(fig:slopes-bl-lb)). Sonority plateaus (e.g., *pkV*) fare in between, giving way to various interpretations, depending on language and analysis, such that plateaus may pattern as ill- or well-formed [e.g., @bat1996selecting], although they are generally interpreted as denoting a third mid-level of well-formedness.

(ref:slopes-bl-lb) Schematic depiction of the sonority slopes of two onset clusters, *blV* and *lbV*. The red solid line denotes the sonority slope of the cluster (i.e., the two onset consonants), while the dashed grey line denotes the slope between the second consonant and the vowel at the nucleus position (always a rise in these cases). The angle of the red lines reflects the well-formed rising sonority slope of the onset cluster in *blV* and the ill-formed falling sonority slope of the onset cluster in *lbv*.
```{r slopes-bl-lb, fig.cap = "(ref:slopes-bl-lb)", fig.asp = .45, out.width = '100%'}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("b","l","V","l","b","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="Sonority slopes: different types",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="well-formed",y=9.5,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  geom_text(data=tibble(seg_token="sonority rise",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="ill-formed",y=9.5,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  geom_text(data=tibble(seg_token="sonority fall",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 3, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8, "b",
        5,6, "c",
        6,3, "c",
        6,3, "d",
        7,8, "d") %>%
  slopes_plot()
```

The *Minimum Sonority Distance* [MSD; @steriade1982greek; @selkirk1984majorsk] is a well-known elaboration on the preferred angle of sonority slopes compared to basic applications of the SSP, given that the SSP makes no distinction between different angles of rising or falling slopes. The MSD was designed to prefer onset rises with steep slopes over onset rises with shallow slopes, under the assumption that consonantal sequences in the onset are preferred with a larger sonority distance between them. For instance, *plV* has a steeper rise compared to *bnV* and it is therefore better-formed according to the MSD (see Figure \@ref(fig:slopes-pl-bn)).[^cf4] 
We test MSD-based models in the experiments (section \@ref(sec:experiments)) and we elaborate further on the notion of sonority distance in the discussion of the confirmatory study (subsection \@ref(sec:distance)).

(ref:slopes-pl-bn) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *bnV* (the red solid line denotes the sonority slope of the onset clusters). The angle of the red lines reflects a steeper rise for *plV* (left) compared with *bnV* (right), due to the larger sonority distance between the consonants in *plV*.
```{r slopes-pl-bn, fig.cap = "(ref:slopes-pl-bn)", fig.asp = .4, out.width = '100%'}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","b","n","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="Sonority rises: different slopes",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="steep rise",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="shallow rise",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8,"b",
        5,3,"c",
        6,5,"c",
        6,5,"d",
        7,8,"d") %>%
  slopes_plot()
```

##		Problems with standard sonority theory {#sec:problems}

###		Slippery sonority slopes {#sec:slippery}

The widely-accepted use of sonority slopes in order to explain and predict phonotactic behaviors has been adopted by many researchers with only few changes such as the above-mentioned elaborations on the angle of sonority slopes. This is a strong testament to the simplicity and power of the concept of sonority slopes. However, given that the role of slopes is essentially formal, with no explicit functional motivation from articulation, perception or cognition, they remain open for interpretation (including interpretations that ignore the phonetic aspect altogether). Indeed, sonority slopes have been used in attempts to explain practically all types of phonotactic phenomena, regardless of their different potential sources. 
This over-application of sonority slopes has resulted in various contradictions in the sonority literature (see the case of /s/-stop clusters and plateaus in the following section \@ref(sec:failures)), which were highlighted in some prominent objections to a notion of sonority which is not phonetically motivated and appears to act like a cover term for various functionally-different processes [e.g., @ohala1984prosodic; @ohala1992alternatives; @laks1995connectionistsk; @steriade1999alternatives; @wright2004review; @henke2012isthessp].

Traditional sonority accounts formalize sonority principles in terms of slopes, essentially disregarding how high or low a given slope is. 
Sonority slopes are obtained from the sonority values of members of a consonantal cluster, while the overall sonority level of the slope can be obtained from the sonority level of the most margial member of the slope, e.g., the first consonant in an onset cluster.
<!-- This type of information can be obtained from the sonority level of the first consonant in an onset cluster, which is informative with regards to the overall sonority level of the slope.  -->
Since we discuss only onset consonant clusters in this paper we simply refer to these marginal values as the *sonority intercepts* of consonantal clusters.
<!-- at which the slope starts (e.g., the first consonant in an onset cluster -->
<!-- For onset slopes that preferably rise in sonority, we could also   -->
<!-- the intercepts, which indicate how high or low the given slope is. The intercept in this case is the sonority level at which the slope starts (e.g., the first consonant in an onset cluster).  -->
Intercepts play no role in the characterization of traditional sonority profiles although they are informative with regard to the amount of underlying sonority that a certain slope implies. This is a curious fact given that sonority-based accounts stem from the assumption that sonority quantities have an effect on the observed phenomena.
<!-- conceptions of sonority in terms of energy were suggested already at early stages in prominent publications [e.g., @sievers1893grundzugesk; @ladefoged1975acourse]. A more informative description of an energy would consider both its intercept and its slope under the assumption that the quantity of this energy has an effect on the observed phenomenon. -->
In that sense, it may be useful to think of the sonority slope as a description of the *quality* of an energy (steady, rising or falling), while the sonority intercept describes the energy's initial *quantity* (large or small).
Thus, even without agreeing on the exact nature of the energy behind sonority, it appears to be unclear why the sonority intercept of onset clusters was practically neglected. Presumably, it is the propensity for simple and elegant rather than functional generalizations in many theoretical linguistic traditions that cemented the formal architecture of sonority principles with a rough conception of slopes without intercepts.
<!-- The propensity for simple and elegant rather than functional generalizations in many theoretical linguistic traditions cemented the formal architecture of sonority principles with a rough conception of slopes without intercepts. -->

Taken together, the over-application of traditional sonority principles that employ a highly reduced conception of slopes, leads to consistent cases of misinterpretation of sonority principles, where superficially similar qualities, e.g., the rising slopes in *mlV* and *psV* (see Figure \@ref(fig:slopes-ml-ps)) are treated similarly regardless of their underlying differences in quantity, which are reflected by their different intercepts (/m/ in *mlV* is higher than /p/ in *psV*, indicating that the cluster /ml/ has a higher underlying sonority level than the comparable sonority rise in /ps/).

(ref:slopes-ml-ps) Schematic depiction of the sonority slopes of two sonority onset rises, *mlV* and *psV*, with comparable angles yet different intercepts. The red solid line which denotes the sonority slope of the onset clusters is higher for *mlV* (left) than for *psV* (right) due to the higher intercept of /m/ compared to /p/. This difference is not accounted for by traditional sequencing principles.
```{r slopes-ml-ps, fig.cap = "(ref:slopes-ml-ps)", fig.asp = .4, out.width = '100%'}
#seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("m","l","V","p","s","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="Sonority rises: different intercepts",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="high rise",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="low rise",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 5, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8, "b",
        5,1, "c",
        6,2, "c",
        6,2, "d",
        7,8, "d") %>%
  slopes_plot()
```

###		Sonority failures  {#sec:failures}

One rather well-known and well-studied consistent flaw in the empirical coverage of all traditional sonority principles concerns sequences that are often termed /s/-stop clusters, referring to cases where a sibilant fricative---most often /s/---precedes a stop consonant, like in the English words ***st**op*, ***sk**y* and ***sp**ort* [see e.g., @kenstowicz1994phonology; @wright2004review; @yavacs2008sonority; @vaux2009append; @olender2013acoustic; @goad2016sonority]. The sonority slope of /s/-stop clusters is either an onset fall or an onset plateau, depending on the given sonority hierarchy (see Figure \@ref(fig:slopes-sp-sp)). Thus, although /s/-stop clusters are relatively common in languages that tolerate sequences, i.e., they should be considered relatively well-formed [@morelli2003relative; @steriade1999alternatives], /s/-stop clusters are predicted to be rare, or even very rare, by their ill-formed sonority slopes.

As can be seen in the sketches of the syllable *spV*, illustrated here in Figure \@ref(fig:slopes-sp-sp) with two different sonority hierarchies, the sonority slopes of the consonantal sequence (red solid line) is either a fall or a plateau depending on the given sonority hierarchy (*col* vs. *exp* in Table \@ref(tab:hierarchy)). The very low intercept of the clusters may serve as an indication that the effect of these ill-formed slopes may be somewhat diminished due to the low amount of underlying sonority. This would make it a case of misinterpretation (i.e., /s/-stop clusters do not violate sonority principles) due to over-application of sonority slopes, implying that sonority has a limited explanatory contribution to the phonotactics of /s/-stop clusters. We return to /s/-stop clusters in the experiments (section \@ref(sec:experiments)), and we propose a full account of /s/-stop clusters, which illustrates the division of labor between sonority and other phonotactic principles in the general discussion (subsection \@ref(sec:division)).

(ref:slopes-sp-sp) Schematic depiction of the two potential sonority slopes of the /s/-stop cluster *spV*. The red solid line that denotes the sonority slope of the consonantal clusters is falling when applied with the expanded sonority hierarchy *exp* (left), and it is a plateau when applied with the collapsed sonority hierarchy *col* (right).
```{r slopes-sp-sp, fig.cap = "(ref:slopes-sp-sp)", fig.asp = .4, out.width = '100%'}
#seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("s","p","V","s","p","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="Obstruents",y=1,x=4.7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times", angle=90) +
  geom_text(data=tibble(seg_token="Fricative-stop clusters: different hierarchies",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="exp (expanded)",y=9.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  geom_text(data=tibble(seg_token="sonority fall",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="col (collapsed)",y=9.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  geom_text(data=tibble(seg_token="sonority plateau",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  # geom_text(data=tibble(seg_token="exp (expanded) hierarchy",y=9,x=3),
  #           aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  # geom_text(data=tibble(seg_token="col (collapsed) hierarchy",y=9,x=6),
  #           aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times", fontface = "italic") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 2, "a",
        3, 1, "a",
        3, 1, "b",
        4, 8, "b",
        5,2.5, "c",
        6,2.5, "c",
        6,2.5, "d",
        7,8, "d") %>%
  slopes_plot()
```

A second problem that received much less attention in the literature [but see @baroni2014language] is the general failure of traditional sonority principles to correctly account for sonority plateaus. Sonority plateaus can result from different types of consonants of the same class, regardless of the type of class. Thus, a voiceless fricative plateau such as *sfV*, like in the English word ***sph**ere*, should be exactly as ill-/ well-formed as a nasal plateau like *nmV* (see Figure \@ref(fig:slopes-nm-sf)), which is, in fact, a much less common (more *marked*) cluster among the languages of the world [@greenberg1978some; @lindblom1983production; @kreitman2008phoneticssk]. This problem can be, again, attributable to the lack of an intercept in traditional sonority models. Different sonority plateaus have the same flat angle in sonority terms, yet they differ in their apparent distribution and this difference seems to correlate with the different intercepts of the plateaus: a plateau with a low-sonority intercept like *sfV* is less marked, i.e., better-formed, than plateaus with higher sonority intercepts like *nmV*. We return to plateaus in the experiments (section \@ref(sec:experiments)) and we discuss plateaus in the context of sonority distance in the discussion of the confirmatory study (subsection \@ref(sec:distance)).
<!-- @maddieson2013syllable -->

(ref:slopes-nm-sf) Schematic depiction of the sonority slopes of two sonority plateaus, *nmV* and *sfV*. The red solid line which denotes the sonority slope of the onset clusters is higher for *nmV* (left) than for *sfV* (right) due to the higher intercept of /n/ compared to /s/. This difference is not accounted for by traditional sequencing principles.
```{r slopes-nm-sf, fig.cap = "(ref:slopes-nm-sf)", fig.asp = .4, out.width = '100%'}
#seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("n","m","V","s","f","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="Sonority plateaus: different intercepts",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="high plateau",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token="low plateau",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Times") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 5, "a",
        3, 5, "a",
        3, 5, "b",
        4, 8,"b",
        5,2,"c",
        6,2,"c",
        6,2,"d",
        7,8,"d") %>%
  slopes_plot()
```
Note that our critique regarding the lack of intercepts in traditional sonority principles is given from within a discrete framework, where non-overlapping segments and their associated sonority values are interpolated into slopes in symbolic time. This is only the first step towards a more radically different treatment of sonority with continuous entities and dynamic procedures, which we propose in the following section (section \@ref(sec:dynamic)).

###		Sonority "correlusions"  {#sec:correlusions}
Given that the main evidence for sonority hierarchies comes from attested cross-linguistic patterns, the search for the phonetic basis of sonority is essentially a search for the perceptual or articulatory phenomenon that would consistently correlate with classes on the sonority hierarchy (note that correlations are often observed in acoustics as proxy for perception or articulation).[^cf5] 

@parker2002quantifying found a little under 100 different correlates of sonority in the literature, with proposals ranging between articulatory-based perspectives addressing airflow volume or pressure, or degree of jaw opening, and perceptually-based perspectives that often target acoustic intensity, duration or other parameters like the frequency of F~1~ (the lowest formant). 
@parker2002quantifying tested five leading proposals---*intensity*, *intraoral air pressure*, *F~1~ frequency*, *total air flow* and *duration*---in laboratory conditions. The correlations that Parker obtained were partial for all potential correlates, with intensity interpreted as taking the lead, a conclusion that was repeated and elaborated upon in @parker2008sound.
Parker's conclusions are very much in line with many influential studies in phonetics and phonology that target acoustic intensity (also referred to as *amplitude* or *power*) as the phonetic correlate of sonority [e.g., @sievers1893grundzugesk; @heffner1969generalsk; @ladefoged1975acourse; @clements1990role; @blevins1995syllable; and @gordon2012sonority, to name just a few prominent examples].[^cfartic] 

Our current proposal for linking sonority with *pitch intelligibility* as its perceptual basis and with *periodic enegy* as its acoustic correlate (Section \@ref(sec:dynamic)) is related to the intensity-based perceptual approach, with its relative success and its various inherent problems, which we will briefly unpack.
In the following subsection (\@ref(sec:intensity)) we shed light on the problems of intensity-based accounts from a methodological perspective and from a theoretical perspective, considering that intensity in acoustics is most often taken as proxy for loudness in perception.

####		The intensity/loudness problem  {#sec:intensity}
From a methodological standpoint, it is not at all clear how to measure intensity of speech segments from acoustic signals, creating a large space for undesirable *researcher degrees of freedom* due to lack of a unified code for measurements [see @roettger2018researcher].
<!-- [^cf6]  -->
Typical issues include the following: where to set the boundaries of each segment? Should we measure peaks or averages of selected portions? How to deal with the unique profile of stop consonants that include a soft closure portion and a loud transient burst? How to normalize values within and across measurements?[^cf6]

Theoretically speaking, it should bother us that acoustic intensity does not tightly correlate with perceived loudness, or any other consistent dimension of human perception. The acoustic intensity of the signal 
may have certain physical qualities contributing to its overall power, but they have different effects on the perceptual system of the human hearer. This discrepancy between acoustic intensity and perceived loudness is known to play a role at different dimensions of the mapping between acoustics and perception.
The prominent points of departure between acoustic intensity and perceived loudness include
different frequency ranges that lead to different loudness perception [e.g., @fletcher1933loudness; @plack1995loudness; @suzuki2004equal; @moore2013anintro 134], different signal durations, including transient bursts, that have different loudness effects [e.g., @turk1996processing; @seshadri2009perceived; @olsen2010loudness; @moore2013anintro 143], and different periodic structures, where the difference between harmonic structures vs. random noise, and their respective spectral bandwidths, was also found to influence perception of loudness [e.g., @hellman1972asymmetry; @bao2010psychoacousticsk; @moore2013anintro 140].
Acoustic intensity is therefore a physical description of sound waves in space which does not consistently correlate with how loud we perceive them, or with any other perceptual phenomenon for that matter.
Given the above, it would have been a surprising fact that we would struggle to explain on perceptual grounds if sonority was actually found to tightly correlate with acoustic intensity.

The literature on perceived loudness may serve to explain how portions with relatively low acoustic intensity, like voiceless fricatives, appear in speech next to portions with relatively high acoustic intensity, like that of a vowel.
The systematic differences in intensity of adjacent speech sounds imply that the these differences are neutralized in perception, as we assume that the different sounds that compose coherent speech should be perceived as having comparable loudness. This is also in line with the fact that our auditory system perceives the transient and continuous aperiodic high-mid frequencies of obstruents as exceptionally loud compared to the periodic low-mid frequency ranges of vowels. 
<!-- (i.e., they are perceived as louder than sounds with similar acoustic intensity at lower frequencies).  -->
Given the above, we should anticipate that perceived loudness will not be a good candidate for correlation with the sonority hierarchy, as a measure of perceived loudness would bring all speech sounds closer together by diminishing the distinctions provided by acoustic intensity. Indeed, although good approximations of perceived loudness from acoustic signals are available [e.g., @seshadri2009perceived; @skovenborg2012loudnesssk; @lund2014loudnesssk; @itu2015algorithmssk], we are unaware of attempts to employ such measures for sonority. 

Instead of attempts to map acoustic intensity with perception in terms of perceived loudness, most successful endeavors to use intensity-based measures as correlates of sonority do it by essentially increasing the intensity-loudness discrepancy, targeting certain frequency bands to---roughly speaking---discriminate against energy at the higher frequencies that are more characteristic of obstruents, in favor of energy at low-mid ranges of the spectrum that are more characteristic of sonorants, and in particular vowels [e.g., @Pfitzinger1996syllablesk; @galves2002sonoritysk; @wang2007robust; @tilsen2013speech; @patha2016syllablesk; @nakajima2017english; @rasanen2018pre]. The relative success of such metrics remains unmotivated on perceptual grounds. However, they are mostly in tight correlation with the perceptual quality that we identify with sonority in this paper---the capacity to perceive pitch.

#		A dynamic model of sonority in terms of periodic energy {#sec:dynamic}

##		Sonority relates to pitch intelligibility {#sec:pitchintelligibility}
The observation that sonority summarizes some essential quality that is related to vowels and their propensity to deliver a relatively steady harmonic structure, highlighting pitch and formant information, is by no means new. Previous proposals already defined sonority as either relating to vowels in some general way, or more specifically relating to voicing or glottal vibrations, or to the clarity/strength of the formants.[^cf7] A few previous accounts went even further, by addressing the function of this evasive vowel-centric feature, suggesting that sonority may be related to periodic energy or pitch/tone [@lass1988phonology; @nathan1989preliminaries; @puppel1992sonority; @ladefoged1997linguistic; @heselwood1998unusual]. What all these proposals share, explicitly or implicitly, is a recurring insight about a strong link between the preferred type of segmental material in syllabic nuclei and a set of features that conspire to optimize pitch intelligibility, characterizing vowels more than consonants. 

Pitch is an inseparable communicative dimension of all linguistic sound systems, whether it is lexically determined as in linguistic *tone*, or post-lexically employed to convey intonation, the linguistic *tune* [see typology of prosody in the worlds' languages in @jun2015prosodicsk]. Importantly, linguistic pitch events are known to target syllable-sized units as their "docking site", regardless of the type of pitch event (linguistic tones and tunes). Tone languages associate linguistic tones with Tone-Bearing Units [TBU; see @leben1973suprasegmental], i.e., syllables or moras.[^cf8] Likewise, intonation pitch contours target privileged syllables (prominent or phrase-edge syllables) to highlight and modulate whole words and phrases---a hallmark of Autosegmental and Autosegmental-Metrical Phonology [e.g., @liberman1975intonationalsk; @goldsmith1976autosegmental; @pierrehumbert1980phoneticssk; @ladd2008intonational]. 

The functionally motivated conclusion that emerges with regard to sonority is therefore that syllables require a pitch bearing nucleus and sonority is a scalar measure of the ability to bear pitch. In other words, sonority is a measure of *pitch intelligibility*. We hypothesize that pitch-bearing units, which are privileged in perception and essential in communication, were selected in the process of language evolution to optimize the rich usage of linguistic pitch events in speech, making pitch intelligibility a fundamental requirement for the building blocks of prosody.[^cf9]

Crucially, our auditory system is specialized for pitch detection, underlying our ability to perceive pitch from quasi-periodic structures with attention to very minute details [our visual system, in comparison, while also good at detecting periodicity in terms of symmetry, is far less sensitive than the auditory system to small differences within a larger repetitive structure, see @chowning2001perceptual].
Over the years, different lines of investigation have reached related conclusions about the strong link that holds between syllables and pitch, with converging evidence from articulatory [e.g., @prieto2007coordinationsk; @karlin2014articulatorysk], perceptual [@house1990tonal; @zhang2001effects; @barnes2014segmental] and neurological studies [e.g., @krishnan2014cortical; @tang2017intonational; @myers2019pushing].

###		Pitch intelligibility correlates with periodic energy {#sec:periodicenergy}
While it is essentially pitch intelligibility in perception that we assume here to be the phonetic basis of sonority, we would be hard pressed to find simple ways to measure the scalar strength of pitch intelligibility directly as a perceptual construct. Fortunately, a very tight correlation holds between the periodic content of the acoustic signal and the effective sensation of pitch in perception. The mapping from periodic energy in the acoustic signal to pitch intelligibility in perception can be equated with the mapping we often employ from F0 in the acoustic signal to pitch height in perception. In both of these cases we use acoustics to refer to perceptual phenomena, knowing full well that the entirety of pitch perception phenomena cannot be reduced to just a few acoustic dimensions [see, e.g., @houtsma1995pitch; @shepard2001pitch; @moore2013anintro 203; @mcpherson2018diversity]. However, we stand on relatively firm ground when we link the main cue to pitch height with the (present or inferred) fundamental period in acoustics (F0), and when we link the main cue to pitch strength with the acoustic power of the periodic components in the signal (periodic energy) [see, e.g., @pierce2001intro; @de2005pitch; @oxenham2012pitch]. Since the signals in our case are limited to speech, the relative consistency of these mappings between acoustic measurements and perceptual aspects of pitch only strengthens.

##		The Nucleus Attraction Principle {#sec:nap}
At the heart of all sonority-based principles lies the idea that the most sonorous segment in a sequence is contained within the nucleus of the syllable, assuming a link between the amount of sonority and the nucleus position of the syllable. We adopt this fundamental insight that guides all other sonority principles in the development of the Nucleus Attraction Principle, but instead of adding further formal assumptions about non-overlapping segments with fixed sonority values and corresponding sonority slopes in symbolic time, we simply model the link between sonority and the syllabic nucleus as a dynamic process in real time, whereby all the portions of the speech signal compete against each other for the nucleus. In our version of the guiding sonority principle, NAP, sonority is the quality that attracts the nucleus such that the most sonorous portion in a given sequence is predicted to attract it the most and win the competition against the rest of the speech material, which ends up being syllabified in the margins (i.e., onset and coda positions).

Crucially, we take the link between sonority peaks and syllabic nuclei in the NAP model to be the result of a perceptual-cognitive process that is functionally motivated, rather than a formal axiomatic state of affairs. In fact, by modelling the sonority-nucleus link in dynamic terms we do not need to add further theoretical assumptions about sonority slopes or even segments to determine ill-/well-formedness---syllabic ill-formedness is directly related to the degree of nucleus competition that a given syllabified portion incurs. For example, consider the pair *blV* and *lbV* from Figure \@ref(fig:slopes-bl-lb). A consonantal onset cluster with a well-formed rising sonority slope like *blV* should be also considered well-formed under NAP due to the very low potential of competition between the marginal non-sonorous onset consonant /b/ and the non-adjacent vowel that wins the competition for the nucleus. Likewise, a consonantal onset cluster with an ill-formed falling sonority slope like *lbV* should be also considered ill-formed under NAP due to the high potential of competition between the marginal sonorous onset consonant /l/ and the non-adjacent winning vowel.

Furthermore, very little competition is expected to arise from CV structures since even a very sonorous onset consonant is not expected to compete with an immediately following vowel (sonority levels continuously rise towards the adjacent vowel, with no discontinuities between them). Nucleus competition, much like sonority slopes, plays a role chiefly when sequences of consonants are syllabified within a single syllable as complex onset/coda clusters. The phonotactics of these possible sequences are determined, to a large extent, by sonority principles. We interpret this aspect of cluster phonotactics such that sequences within syllables are avoided the more they increase the potential competition for the nucleus in the process of syllabifying/parsing the stream of speech.

Many novelties in the NAP proposal are well-established in phonological theory. The central idea behind NAP, whereby sonority *attracts* syllabic nuclei, is a case in point. In classic descriptions of stress systems it is often noted that some languages exhibit *weight sensitivity*, i.e., they regularly assign the primary stress to a certain syllable within a phonological word according to some general rule (e.g., the final syllable, the penultimate syllable, etc.), but they may diverge from this rule and assign the stress to a different syllable if it is *heavier* than the syllable that should have received the stress by rule. This is standardly understood as *attraction* of the primary stress by the heavy syllable, where heaviness is mainly the product of a longer vowel in the nucleus, and in some languages heaviness may also result from a (preferably sonorant) consonant in the coda [see, e.g., @mccarthy1979formalsk; @hayes1980metrical; @prince1990quantitative; @gordon2006syllableweight]. There is also evidence for cases where more sonorous (more open) vowel types in the nucleus can contribute to heaviness and attract the stress [@zec1995sonority; @zec2003prosodic; @kenstowicz1997quality; @delacy2002formal; @gordon2012sonority].

Viewed from NAP's perspective, attraction of stress in weight sensitive systems is simply the special case of a regular procedure, whereby different portions of the speech signal compete for a limited amount of nuclei, and the most sonorant material in speech (i.e., mostly vowels and their respective sonorant energy) attracts the syllabic nuclei the most. 
Weight sensitivity is, by extension, a similar scenario only one level higher, where the different syllables compete for the stress, and one syllable wins the competition given its more sonorant energy (e.g., more open vowel/ longer vowel/ sonorant coda). 
The stressed syllable in this sense is simply the most sonorous syllable in the word, which is optimized to serve the prominence requirement for accents---the stressed syllable is the docking site for *pitch accents* in speech. 
Attraction in prosody thus follows a consistent rationale: sufficiently pitch intelligible units satisfy the requirement for a syllable by attracting nuclei, and exceptionally pitch intelligible units satisfy the requirement for the stressed syllable by attracting the most prominent nuclei.
<!-- [^cfstress]: It is important to note that weight sensitivity is not a universal process, as stress assignment patterns vary from language to language, and not all languages even have stress to begin with. However, it is one of the naturally occurring stress assignment patterns that various languages exhibit, e.g., Arabic, Tibetan (Lhasa), Wolof, Finnish, Latin and many more [see @goedemans2013weight; and @gordon2006syllableweight 23 for more exhaustive lists]. -->
Moreover, while weight sensitivity characterizes only a subset of the world's languages,[^cfstress] there is a related and wide-spread text-tune interaction that leads to local sonority enhancement of prominent syllables (i.e., stressed syllables and syllables at unit-edge positions). These are mainly cases of post-lexical prosodic enhancements of nuclei (by increase in duration and/or intensity of the nucleus---effectively enhancing the periodic energy mass) in order to accommodate certain tonal events in intonation [@roettger2019tune].

To conclude, the understanding that sonority is linked to pitch via syllabic units is well established in phonology. NAP is modeling this intricate link with machinery that successfully bridges between discrete representations of segments (consonants and vowels), abstract prosodic structure (syllables) and continuous descriptions of prosody (length, loudness and pitch).

##		Complementary cognitive models {#sec:complementary}
NAP essentially models a bottom-up process, describing the parsing of the stream of speech into syllables as the end of a process that starts in perception.
We assume that perception and cognition are continuous processes that can activate categorical representations at the end of their trajectories, in line with models presented in @case1995evaluation; @lancia2013interaction; @grice2017integrating; and @roessig2019dynamics for derivation of linguistic categories from dynamic inputs. 
These categorical representations are the learned symbols of the system in the speaker's mind, often equated with co-activation of neural populations in the brain [see, e.g., @friederici2011brain; @mesgarani2014phonetic; @tang2017intonational].
This description acknowledges that cognitive processes are dynamic events but they should also be discretized with symbols in order to be recognized and replicated. To borrow Howard Pattee's terminology [see @pattee2012lawssk], symbols "harness" dynamics in a language system.

Once learned and established, symbols can feed top-down inferences that play a role alongside bottom-up inferences in perception.
Top-down perception inferences in our framework are different from bottom-up inferences in that they access categorical perception, where speech is analyzable in terms of discrete symbols. Top-down inferences are thus limited to predictions that are based on the distributional probablity of the recognized symbols, given their (constantly updated) history in the system.
This type of top-down inferences, which are detached from the functional aspects of the bottom-up mode, echo models of the language user as a *statistical learner* [see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing; @roettger2019evidential] and, more specifically, they are mostly in line with models of *phonotactic learners* [e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @futrell2017generative; @jarosz2017inputsk; @mayer2019phonotactic; @mirea2019usingsk].
We go back to the phonotactic learner models in the context of the universality of sonority in subsection \@ref(sec:projection) of the general discussoin, where we briefly discuss some of their differences with respect to our view of top-down modeling in this paper, which is based on categorizable surface forms without access to formal abstract phonological distinctions. 

<!-- Note however that various phonotactic learners employ formal notions such as *phonological features* to enrich the model with formal linguistic distinctions that assist in defining classes by their shared features [e.g., @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @futrell2017generative; @jarosz2017inputsk].  -->
<!-- Such use of formal discrete entities is not supported by the framework we are presenting here if they require the postulation of symbols that do not have a consistent perceptible surface expression (i.e., cannot be claimed to be categorizable from perception in and of themselves). -->

In this paper we do not explore the statistical nature of top-down inferences. Instead, we operationalize the rationale behind NAP with symbolic machinery to present what we refer to as the top-down model of NAP. It is important to note that the discrete arithmetic operations that we present in the top-down version of NAP are by no means assumed to model cognitive processes, given that top-down processes in perception are limited to probabilistic inferences in our framework. We do, however, interpret the results of these operations as a good estimation of potential symbol-based top-down statistical inferences, guided by the rationale of NAP. This choice allows us to present a top-down model with a stronger explanatory value as it uses a similar architecture to that of standard sonority principles, helping to elucidate NAP's core ideas with familiar vocabulary (see subsection \@ref(sec:ordinalmodels)). 

Moreover, it should be noted that since a cognitively-plausible top-down architecture in this framework is based on the distributional patterns of recognizable symbols, these distributions are "blind" to their functional sources, which include a host of universal and idiosyncratic phonotactic pressures. A true top-down statistical learner is thus inherently "contaminated" by all the different sources that contribute to phonotactics in a given system, without a clear distinction between sonority and other factors. Thus, it remains an open question whether top-down inferences that target only sonority-based phonotactics can be modeled in a more direct and principled way than the one we use here.

Processing of speech occurs bottom-up and top-down simultaneously, and there is no reliable way to fully tease the two apart. Our approach takes both routes into account where they stand for separate processes based on either continuous or discrete sources. Suggesting two types of models may be considered redundant by Occam's razor, yet it is very much in line with Pattee's view of cognitive modeling, where models of dynamic processes and models of their related discrete symbols are both necessary for "a complete understanding of the system", and "no one model is logically or mathematically derivable from, or reducible to, the others" [@pattee2012lawssk 18-19].

As two complementary inference routes, the top-down and bottom-up models should not be considered equal. The bottom-up route is the source of learned linguistic distinctions, it is functionally motivated by the rules of physics and the limitations of the sensorimotor system.
In contrast, the top-down route is based on linguistic experience and superficial inferences that reflect the history of the symbols in the system (i.e., the distributional probabilities of recognizable recurring patterns and their extensions by analogy). In other words, top-down inferences reflect functionally motivated behaviors only indirectly, as the outcome of learning the superficial expressions of functionally-motivated (bottom-up) dynamics.

```{r prepare-mat-per, include=FALSE}
# 60 x length of the audio file binned each 10 ms; 60 frequency bins with 10 ms for each column 
dir_mats <- "data_tables/APPd_txt_matrices/AA/"
files_mat <- list.files(path=dir_mats, pattern="*.txt",full.names=TRUE)

#creates a dataframe with 3 columns, the name of the syllable, syl, the time point, t, and the periodic energy, p
per_df <- map_dfr(files_mat, function(f){
#this is like a loop, it takes each file inside the f and does the following:
    # Read the file
    mat <- read_csv(f,col_names = FALSE,
                    col_types = cols(.default = col_double()))
    #Sum of every column of the matrix
   # vector_weights <- c(rep(1,8),rep(1.1,2),rep(1.3,3),rep(1.4,4),rep(1.5,5), rep(1.4,3), rep(1.3,5),rep(1.2,6),rep(1.1,10),rep(1,15))
    #mat <- mat * vector_weights
    per <- colSums(mat)
    #Extract the name of the syllable form the filename: looks for //(syllable).
    filename <- str_match(f,"//(.*?)_(.*?)\\.")
    syl <- filename[,2] 
    speaker <- filename[,3] 
    tibble(syl=syl,t=(0:(length(per)-1))*10,per=per,speaker=speaker)
})
# Periodic energy at every time point
per_df_full <-  per_df %>%  group_by(syl,speaker) %>% 
                mutate(smooth_per = smooth(per,"3RS3R")) 
#head(per_df_full)

# ```
# 
# ```{r prepare-seg, include=FALSE}

dir_seg <- "data_tables/praat_seg/AA/"
files_praat <- list.files(path=dir_seg, pattern="*.txt",full.names=T)
seg_df <- map_dfr(files_praat, function(f){  
#this is like a loop, it takes each file inside the f and does the following:

    # filename <- str_match(f,"/([^/]*?)_(.*?)_(.*?)\\.")
    filename <- str_match(f,"//(.*?)_(.*?)_(.*?)\\.")
    seg <- read_tsv(f, col_types =cols(
                          rowLabel = col_character(),
                          tmin = col_double(),
                          text = col_double(),
                          tmax = col_double()
                      )) %>% select(-rowLabel) %>%
        mutate(syl = filename[,2],
               speaker = filename[,3],
               text = str_extract_all(syl, ".")[[1]],
               position=row_number(),
               t = map2(tmin,tmax, ~ round(seq(.x,.y,.01)*1000))               ) %>%
        tidyr::unnest() %>%
        select(-tmin, -tmax)
})

syl_info <- left_join(per_df_full,seg_df,by = c("syl", "t", "speaker"))

## head(syl_info)

## syl       t   per speaker smooth_per text  position
## <chr> <dbl> <dbl> <chr>        <dbl> <chr>    <int>
## 1 cefal     0  0    AA            0    c            1
## 2 cefal    10  2.37 AA            2.37 c            1
## 3 cefal    20  4.97 AA            4.97 c            1
## 4 cefal    30  5.80 AA            4.99 c            1
## 5 cefal    40  4.99 AA            4.99 c            1
## 6 cefal    50  4.06 AA            4.06 c            1
# ```
# 
# ```{r log-transform, include=FALSE}

subset_voiceless_thresh <- filter(syl_info,
                                  #nchar(as.character(syl))>=4,
                                  position==1,
                                  syl %in% c("cfal","cpal","fsal","ftal","sfal","spal"))
per_thresh <- max(subset_voiceless_thresh$per)

syl_info <- group_by(syl_info,syl,speaker) %>%
    mutate(log_per = ifelse(smooth_per<per_thresh, 0,
                            10*log10(smooth_per/per_thresh)))

## print(syl_info,n=100)

# ```
# 
# ```{r, cog}

syl_info <- syl_info %>% group_by(syl, speaker) %>%
    mutate(com_syl = sum(log_per*t)/sum(log_per), # position of CoM of the whole syllable in time
           t_left_syl = ifelse(t <= com_syl,t,0),
           com_onset = sum(log_per*t_left_syl)/sum(log_per*(t_left_syl>0)),
           NAP_bu = -(com_syl - com_onset), #flipped sign
           NAP_bu_rel = -(com_syl - com_onset)/com_syl) %>%
    select(-t_left_syl)%>%
    select(NAP_bu, everything())
# ```
# 
# ```{r, monosyl, include=FALSE}

monosyl_info <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "AA")

monosyl_info$text[which(monosyl_info$text=="c")] <- "S"
# monosyl_info$text[which(monosyl_info$text=="c")] <- "U+0283"
# monosyl_info$text[which(monosyl_info$text=="c")] <- "\textesh"


monosyl_info$syl <- as.factor(monosyl_info$syl)
monosyl_info <- mutate(group_by(monosyl_info,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info$smog_per[(monosyl_info$smog_per<0)]=0

monosyl_info <- mutate(group_by(monosyl_info, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info <- mutate(group_by(monosyl_info, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```

(ref:nap-ccv) Schematic depictions of competition scenarios with symbolic CCV structures (left) and with a continuous energy curve, exemplified with the onset cluster *npV* (right). We estimate the competition potential of CCV structures in discrete terms as the difference between *C*~1~ (the first consonant, in blue) and the non-adjacent vowel (*V*, in purple) whereby the vowel is expected to win the competition (connected to the nucleus with a solid arrow) and *C*~1~ is expected to lose (connected to the nucleus with a dashed arrow). In continuous terms, sonority is measured as the integral of duration and amplitude (i.e., the area under the curve), which we consider as a *mass*. As illustrated in the onset cluster *npV* (right), the mass of the nasal /n/ at the beginning of the syllable (the blue area under the curve) is smaller than the mass of the non-adjacent vowel (the purple area under the curve), yet big enough to substantially compete with the vowel for the nucleus.
```{r nap-ccv, fig.cap = "(ref:nap-ccv)", fig.show='hold', out.width = c('50%', '50%'), fig.align = 'center'}
knitr::include_graphics(rep(c("extrenal_figures/NAP_CCV.png","extrenal_figures/NAP_npV.png")))
```

#   Model implementation {#sec:modelimp}

In order to compare the different proposals, we consider four types of traditional sonority models alongside our two novel sonority models.
For the traditional models we use the two types of sonority hierarchies that we presented in subsection \@ref(sec:hierarchies), where the class of obstruents is either *collapsed* (*col*) into a single level or *expanded* (*exp*) to include distinctions between voiced and voiceless obstruents, and between stops and fricatives. 
We apply both hierarchies on each of the two variants of traditional sonority principles, the Sonority Sequencing Principle (SSP) and the Minimum Sonority Distance (MSD). 
The four traditional sonority models we discuss are therefore a combination of a sonority principle (SSP or MSD) and a sonority hierarchy (*col* or *exp*). Accordingly, they bear the notation *SSP~col~* , *SSP~exp~*, *MSD~col~* and *MSD~exp~*.
In our two NAP models we use periodic energy as the correlate of sonority, and we apply it either continuously (bottom-up model) or categorically (top-down model). We refer to these two NAP models with the notation *NAP~td~* for the top-down model and *NAP~bu~* for the bottom-up one.

To demonstrate the different sonority models we focus in this paper on complex onset clusters of the general form CCV, where *C* denotes a consonant and *V* denotes a vowel. While traditional sonority models look at the sonority slope of the onset cluster to determine well-formedness of CCV syllables, NAP-based models apply the notion of *competition* to determine well-formedness.
We always assume for the CCV sequences under discussion that the vowel 
is the ultimate winner of this competition so we are only interested in quantifying how much competition the vowel needs to face in order to win. We further assume that the consonant immediately adjacent to the vowel in *C*~2~ is not a valid competitor from this vowel-adjacent position, since periodic energy is expected to continuously rise from a consonant to an immediately following vowel (same reason why we do not expect nucleus competition to play a role in simple CV syllables, see subsection \@ref(sec:nap)).[^cfglides] 
It is therefore useful to schematize the question of competition in CCV cases in terms of the relation between (loosing) *C*~1~ and (winning) *V* given (intervening) *C*~2~, as depicted in Figure \@ref(fig:nap-ccv). 

The logic of the above-mentioned schematic simplification is directly applied in the symbolic interpretation of the NAP model (*NAP~td~*), where we target phonemic entities vis-à-vis their position in the CCV sequence to quantify the competition potential of *C*~1~ against *V* given *C*~2~ (see subsection \@ref(sec:naptdmodel)). 
In contrast to *NAP~td~*, as well as all other sonority models we discuss, the bottom-up NAP model---*NAP~bu~*---does not require discrete entities, not even segmental boundaries. It takes the continuous periodic energy data directly from the acoustic signal, where sonority levels are measured in terms of a *mass*, i.e., the area under the periodic energy curve (see schematic depiction in Figure \@ref(fig:nap-ccv)). We thus quantify the competition potential of the onset in *NAP~bu~* in terms of the leftward displacement of the periodic energy mass (see subsection \@ref(sec:napbu)).

In the following subsections we elaborate on our methods for obtaining scores from the ordinal models, the four traditional sonority models (subsection \@ref(sec:traditionalmodels)) and *NAP~td~* (subsection \@ref(sec:naptdmodel)), as well as the continuous model, *NAP~bu~* (subsection \@ref(sec:napbu)).
We conclude this section with a short overview of some of the key advantages of NAP over traditional sonority models (subsection \@ref(sec:advantages)).

## Ordinal models {#sec:ordinalmodels}
The four traditional sonority models under consideration---*SSP~col~* , *SSP~exp~*, *MSD~col~* and *MSD~exp~*---as well as the symbolic version of NAP---*NAP~td~*---all share a similar architecture by assigning ordinal integer values to phoneme classes, according to a given sonority hierarchy. The predictions that we derive from these models are hence also ordinal, i.e., the size of the intervals between levels is unknown.

### Traditional models (*SSP/MSD~col/exp~*) {#sec:traditionalmodels}

Implementation of traditional sonority principles like the SSP is based on a calculation of the sonority slope with a given sequence of segments. Speech segments in these frameworks have fixed values on the sonority hierarchy, based on their class membership in *col* or *exp* (see Table \@ref(tab:hierarchy)). These sonority index values are usually expressed in terms of integers since they reflect an ordinal scale. For this reason, the operations that these models employ should be restricted to simple arithmetic functions like addition and subtraction. Sonority slopes can therefore be straight-forwardly obtained by a subtraction between the corresponding sonority indices of two adjacent consonants. In onset clusters with two consonants (CCV) this can be simply achieved by the formula *C*~2~ – *C*~1~, which yields positive results for rising sonority slopes, negative results for falling sonority slopes, or a zero for plateaus. We apply this to the two SSP models, *SSP~col~*  and *SSP~exp~*.

The exact same formula is also used to obtain scores for the Minimum Sonority Distance models, *MSD~col~* and *MSD~exp~*, which elaborate on the well-formedness of onset rises. 
MSD models differ from the SSP in the interpretation of positive values that reflect rising sonority slopes. While in the SSP all positive scores belong in the same group (i.e., all rises are well-formed to the same extent), under the MSD higher positive scores are preferred over lower positive scores to reflect the preference for a larger sonority distance (or steeper slope) in a rising configuration.  

### The top-down NAP model (*NAP~td~*) {#sec:naptdmodel}

The symbolic version of NAP, which we use to derive predictions for *NAP~td~*, shares a similar architecture with common SSP-based models, yet it reflects the novelties of the current proposal, both in terms of the sonority hierarchy it assumes, and in terms of the design of the sonority principle. *NAP~td~* uses a sonority hierarchy that is based on the periodic energy potential of different phoneme classes as the basis of distinct categorical patterning (see following subsection \@ref(sec:snaphierarchy)). Furthermore, *NAP~td~* models syllabic well-formedness in functional terms of nucleus competition rather than the strictly formal use of sonority slopes as in traditional SSP-type models (see  subsection \@ref(sec:snapimplementation)).

#### The sonority hierarchy in NAP~td~ {#sec:snaphierarchy}
The symbolic sonority hierarchy in NAP uses the basic ratio between periodic and aperiodic energy to divide all speech sounds into three distinct groups, reflecting the coarse, yet reliable differences in their potential periodic energy mass. 
The main source of periodic energy in speech stems from the vocal folds vibration when voicing occurs. Aperiodic energy in speech is mostly the result of the turbulent airflow of articulatory friction (i.e., fricatives), while full articulatory closure, like that of oral stops, results in aperiodic transient bursts [see @rosen1992temporal].
It is therefore possible to conclude that voiceless obstruents that contribute mostly aperiodic energy are the least sonorous, while sonorant consonants and vowels, that contribute mostly periodic energy, are the most sonorous. Finally, voiced obstruents, with both periodic and aperiodic energy, belong in the middle of this 3-place scale (see \@ref(ex:napscale)).

\begin{exe}
\ex Voiceless Obstruents $<$ Voiced Obstruents $<$ Sonorants  \label{ex:napscale}
\end{exe}

A full symbolic sonority hierarchy for NAP should include one more distinction between speech sounds that require an articulatory contact, *contoids*, and speech sounds that require no articulatory contact, *vocoids*.[^cf13a] This distinction separates the sonorants into *sonorant vocoids* (glides and vowels) and *sonorant contoids* (nasals and liquids) that require articulatory contact in the vocal tract.[^cf13] This distinction between two articulatory contact modes entails a qualitative difference between contoids and vocoids that justifies their separation on the symbolic sonority hierarchy: the open vocal tract of vocoids contributes to a stronger, and relatively more stable vocalic signal, which has a considerable larger range for duration and intensity enhancements that greatly increase the potential periodic energy mass of vocoids.[^cf14]

The complete 4-place hierarchy in *NAP~td~* (see Table \@ref(tab:napscale)) also reflects a basic typology of nucleus types, which supports the use of this scale as a qualitative measure for nucleus attraction potentials: while sonorant vocoids like glides and vowels can attract the nucleus in all languages we know, sonorant contoids like nasals and liquids can be syllabic (i.e., attract the nucleus) only in a subset of languages, of which a much smaller subset may allow obstruents to attract nuclei, where voiceless obstruents are expected to be the least preferred [but see @easterday2019highly for some divergent patterns with syllabic obstruents relative to syllabic liquids].

(ref:napscale-caption) (\#tab:napscale) The symbolic sonority hierarchy in *NAP~td~*
(ref:napscale-caption2) Index values reflect the ordinal ranking of categories in the sonority hierarchy. The distinctions between categories in the symbolic NAP hierarchy are based on the characteristic ratio between periodic and aperiodic energy, and on articulatory contact, both taken to reflect the potential of the periodic energy mass, i.e., the potential for nucleus attraction.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:napscale-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
Sonority index & \multicolumn{1}{c}{Segmental classes} & \multicolumn{1}{c}{Periodic:Aperiodic} & \multicolumn{1}{c}{Articulatory contact}\\
\midrule
4 & \textbf{Sonorant Vocoids} & \multicolumn{1}{c}{1:0} & $-$\\
 & (\emph{glides}, \emph{vowels}) &  & \\
3 & \textbf{Sonorant Contoids} & \multicolumn{1}{c}{1:0} & $+$\\
 & (\emph{nasals}, \emph{liquids}) &  & \\
2 & \textbf{Voiced Obstruents} & \multicolumn{1}{c}{1:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
1 & \textbf{Voiceless Obstruents} & \multicolumn{1}{c}{0:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:napscale-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

#### NAP~td~ implementation {#sec:snapimplementation}

When dealing with CCV syllables under the NAP framework we essentially want to measure the competition potential between *C*~1~ and *V* given *C*~2~. In and of itself, *C*~2~ is not considered a competitor due to its proximity to the vowel, as discussed in the opening of this section (\@ref(sec:modelimp)).
The question of competition may be thus expressed by the following questions:
(i) what is potential periodic energy mass of *C*~1~ (i.e., how sonorous is *C*~1~, or what is the intercept of the cluster);
(ii) how much of the energy in *C*~1~ is potentially lost, gained or retained in *C*~2~, before peaking at the vowel (i.e., what is the sonority slope).
Assessing this relationship between *C*~1~ and *V* given *C*~2~ is thus achieved by the combination of two subtraction formulas: 
(i) a calculation of the difference between *C*~1~ and the non-adjacent vowel, to reflect the potential strength of *C*~1~ in terms of the intercept relative to the nucleus, and;
(ii) a calculation of the slope between adjacent *C*~1~ and *C*~2~, just like in the SSP model, to reflect energy retention towards the peak. 
This can be summarized with the formula in \@ref(eq:naptdeq).[^cf15]

\begin{equation}
(V - C_1) + (C_2 - C_1)  \label{eq:naptdeq}
\end{equation}

### Ordinal sonority scores {#sec:ordinalscores}

Table \@ref(tab:ordinalscores) compares the scores of the five ordinal models with different CCV cluster types. It shows that the main categorical difference between the two sonority hierarchies, *exp* and *col*, concerns fricative-stop clusters like the /s/-stop cluster *spV*, which are considered as either an onset fall (*exp*) or a plateau (*col*). When the MSD is applied, the two sonority hierarchies also show differences in ranking within onset rises, given their different treatment of obstruents. In *exp* there are four levels of obstruents (voiced and voiceless stops and fricatives) which are collapsed into one in *col*, resulting in a smaller range of possible sonority distances.

Unlike traditional models, the predictions of *NAP~td~* are not grouped into levels that reflect the rough angle of the sonority slope in terms of falls, rises and plateaus. We take the raw score of the *NAP~td~* formula as reflective of the nucleus competition potential such that higher scores are better-formed. The top-down NAP model allows scores within a range that goes from '-3' for the most ill-formed syllable up to '6' for the most well-formed, although a more relevant range to consider, given that we exclude glides from our set, is between '-1' and '5'. These scores are not immediately comparable to the traditional model scores, yet some interesting departures from the traditional models can be observed (see a more in-depth analysis in the following subsection). For example, *NAP~td~* considers the onset rise in the sonorous cluster *mlV* as ill-formed as the inverse fall *lmV*, and both clusters pattern in *NAP~td~* with nasal plateaus (e.g., *nmV*), where they all receive the same border-line value of ‘1'. At the same time, voiceless clusters pattern in *NAP~td~* with well-formed combinations (scoring ‘3') although they may include sonority plateaus (e.g., *sfV*) or sonority falls (e.g., *spV*) in traditional model terms.

(ref:ordinalscores-caption) (\#tab:ordinalscores) Ordinal sonority scores
(ref:ordinalscores-caption2) Well-formedness scores with ordinal models. The table demonstrates the predictions we obtain using the two traditional sonority hierarchies, *col* and *exp*, with each of the two traditional sonority principles, SSP and MSD. Numbers in brackets next to "Rise" reflect MSD's ranking of onset rises by distance---higher values indicate better-formed rises. The scores derived from *NAP~td~* on the right column are taken to directly reflect the nucleus competition potential, where higher scores are better-formed.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:ordinalscores-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Traditional sonority principles} & \multicolumn{1}{c}{Symbolic NAP}\\

\multicolumn{1}{l}{Onset} & \multicolumn{2}{c}{\emph{exp} hierarchy} & \multicolumn{2}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\

\multicolumn{1}{l}{clusters} & \multicolumn{1}{c}{C2$-$C1} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{exp}}}} & \multicolumn{1}{c}{C2$-$C1} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{col}}}} & \multicolumn{1}{c}{(V$-$C1)$+$(C2$-$C1)}\\

\midrule
\multicolumn{1}{l}{\textbf{pl}V} & 6$-$1 $=$ 5 & \multicolumn{1}{c}{\textbf{Rise (5)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{fl}V} & 6$-$2 $=$ 4 & \multicolumn{1}{c}{\textbf{Rise (4)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{sm}V} & 5$-$2 $=$ 3 & \multicolumn{1}{c}{\textbf{Rise (3)}} & 2$-$1 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{vl}V} & 6$-$4 $=$ 2 & \multicolumn{1}{c}{\textbf{Rise (2)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$2)$+$(3$-$2) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{ml}V} & 6$-$5 $=$ 1 & \multicolumn{1}{c}{\textbf{Rise (1)}} & 3$-$2 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sf}V} & 2$-$2 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{zv}V} & 3$-$3 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$2)$+$(2$-$2) $=$ \textbf{2}}\\
\multicolumn{1}{l}{\textbf{nm}V} & 5$-$5 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 2$-$2 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sp}V} & 1$-$2 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{lm}V} & 5$-$6 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$3 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{mz}V} & 4$-$5 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{lv}V} & 4$-$6 $=$ $-$2 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$4 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{ms}V} & 2$-$5 $=$ $-$3 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{np}V} & 1$-$5 $=$ $-$4 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{lp}V} & 1$-$6 $=$ $-$5 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$3 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:ordinalscores-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

## The bottom-up NAP model (*NAP~bu~*) {#sec:napbu}

(ref:com-4examples-1) Smoothed periodic energy curve (black) of the four syllables from the experimental stimuli---*lpal*, *nmal*, *vlal* and *smal*. Red vertical line denotes the center of periodic mass of the entire syllable (*CoM~syl~*), blue vertical line denotes the center of periodic mass of the left portion (*CoM~ons~*). Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). The distance between the two CoM landmarks is indicative of the energy displacement away from the syllabic center, reflecting the nucleus competition potential within the syllable.
```{r com-4examples-1, fig.cap = "(ref:com-4examples-1)", fig.width=7, fig.asp=.6, warning=FALSE}
# CoM_ons <-  expression(CoM[ons])# %>% as_label()
monosyl_examp_1 <- filter(monosyl_info,
                          syl %in% c("smal","vlal","nmal","lpal"))
ordered_syl_abs <- unique(monosyl_examp_1[order(monosyl_examp_1$NAP_bu),]$syl)
monosyl_examp_1$syl <- factor(monosyl_examp_1$syl, levels=ordered_syl_abs)
monosyl_examp_1_plot <-
  ggplot(monosyl_examp_1, aes(x=t)) +
  xlim(0,475) + ylim(-3,19) + #ggtitle("") +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-1, yend=13), color="royalblue1", size=1.7, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-1, yend=13),color="red", size=1.7, alpha=.6, linetype = "solid", lineend = "round") +
  # geom_text(aes(x=com_onset, y=15, label=deparse(CoM_ons)), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Times", check_overlap=T, na.rm = T) +
  geom_text(aes(x=com_onset,y=15,label="CoM:ons"), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Times", check_overlap=T, na.rm = T) +
  geom_text(aes(x=x_com_syl,y=15,label="CoM:syl"), nudge_x = 20, color="red", alpha=.9, size=3, family = "Times", check_overlap=T, na.rm = T) +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-.75, yend=-.75), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.04, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2,y=-2.5,label=paste0(round(NAP_bu)," ms")), size=3, family = "Times", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=19), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=18,label=text), size=6, family = "Times", check_overlap=T) + 
  #
  facet_wrap(~syl, ncol=2) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Times"), strip.text = element_blank())
print(monosyl_examp_1_plot)
```

There are various ways to calculate an estimation of the nucleus competition potential within syllables based on the periodic energy data of the acoustic signal. The method that we present here has the advantage of not relying on segmental landmarks that would have been the result of separate segmentation and annotation processes. Segmentation of the signal into non-overlapping consonants and vowels is theoretically arguable (the transition between segments does not tend to occur at a single point in time), it is error-prone and it requires labor-intensive resources.

We view the periodic energy data in terms of a *mass*, i.e., the area under the periodic energy curve. It is therefore of interest to locate the *center of mass* within regions of interest as a measurement that is sensitive to the two axes of the periodic energy mass---duration (x-axis) and amplitude (y-axis). The center of mass can be viewed as the point in time in which the area under the curve is split into two equal parts. The location of the center of mass in time (x-axis) is attracted to the peak of the curve (on the y-axis), where it is expected to be found with a perfectly symmetric shape. However, the center of mass most often diverges from the peak of rise-fall curves so as to reflect asymmetries in the overall distribution of the mass, either leftward or rightward. The center of mass of the periodic energy curve (henceforth CoM) follows a methodology that was introduced with the Tonal Center of Gravity [@barnes2012tonal] by calculating a weighted average time point that uses a continuous time series as the weighting term. Thus, we use the equation in \@ref(eq:com) to locate the average point in time (*t*), weighted by continuous periodic energy (*per*) at discrete time points (every 10 ms):

\begin{equation}
\frac{\sum_i per_i t_i}{\sum_i per_i}  \label{eq:com}
\end{equation}

The location of the center of periodic energy mass of the entire syllable (henceforth *CoM~syl~*) guides us to the point in time where the periodic mass of all the competing forces within that syllable are split into two equal parts. Once we obtain this reference point we can repeat this process within the resulting left-side portion, i.e., from the beginning of the syllable up to *CoM~syl~*, to focus on the onset position (henceforth *CoM~ons~*).
We therefore measure the center of mass twice---first for the entire syllable (resulting in *CoM~syl~*) and then for the left portion of the first measurement (resulting in *CoM~ons~*). 
The distance between *CoM~syl~* and *CoM~ons~* is indicative of the amount of displacement of energy away from the center of the syllable, which we interpret as reflective of the degree of nucleus competition (see Figure \@ref(fig:com-4examples-1)).[^cf12]

The center of mass is capable of capturing both components of a two-dimensional mass, considering the non-linear shape of the periodic energy curve. 
The leftward displacement of *CoM~ons~* relative to *CoM~syl~* is affected by the distance, the amplitude and the amount of discontinuity between the periodic energy at the onset and the center of mass of the entire syllable.
Any increase in the above results in a larger distance between the two centers of mass as Figure \@ref(fig:com-4examples-1) demonstrates.

##		NAP advantages {#sec:advantages}
Before we turn to the experiment, the advantages of NAP over traditional models can be already glimpsed over with four examples that illustrate major differences in the models' predictions. Consider the clusters in the syllables *spV* (an /s/-stop cluster), *sfV* (a voiceless fricative plateau), *nmV* (a nasal plateau) and *npV* (a sonority fall from sonorant to voiceless). In traditional sonority slope terms, all of these clusters are either highly ill-formed (i.e., sonority falls) or border-line ill- /well-formed (i.e., sonority plateaus). Predictions may slightly differ with different sonority hierarchies, such that these examples can represent three sonority plateaus and one fall with the *col* hierarchy (*spV* $=$ *sfV* $=$ *nmv* $<$ *npV*), or two plateaus and two falls with the *exp* hierarchy (*sfV* $=$ *nmV* $<$ *spV* $=$ *npv*).
Figure \@ref(fig:slopes4examples) schematizes these four examples with traditional sonority slopes, using red lines to denote the portion of the trajectory that represents the relevant slope of the consonantal clusters. These red slopes are leveled for the onset plateaus *sfV* and *nmV* and they are falling in the onsets of *npV* and *spV* (note again that with the *col* hierarchy, *spV* is a plateau, see Figure \@ref(fig:slopes-sp-sp)).
The visual representation in Figure \@ref(fig:slopes4examples) highlights the irrelevance of the intercept of clusters (i.e., their overall height on the y-axis) in traditional sonority formalizations---only the general trend of the slope matters.

In contrast to the traditional approach, NAP is explicitly concerned with energetic quantities that compete for the nucleus and therefore considers the intercept of the onset cluster as a crucial variable. In NAP terms, the two voiceless clusters *spV* and *sfV* have only minimal, if any, sonorant energy (effectively zero periodic mass) that would make the onset a serious competitor for the nucleus, regardless of the slope. Therefore, even if *spV* exhibits a sonority fall it should not pattern with *npV* in terms of ill-formedness. Likewise, if we consider *spV* as a plateau, neither *spV* nor *sfV* should pattern with *nmV* just because they are all considered plateaus. The two nasal-initial clusters, *nmV* and *npV*, should in fact be considered as much worse-formed than the the two /s/-initial voiceless clusters given their distribution within and across languages. Previous works by @greenberg1965some; @greenberg1978some; @lindblom1983production; @lombardi1995laryngeal; @lombardi1991laryngeal; @kreitman2008phoneticssk; @kreitman2010mixed have basically confirmed (although with some considerable differences) that voiceless initial consonant clusters are less *marked* (more common) than voiced clusters, and both types of clusters are less marked than a voiced-voiceless initial cluster. Such hierarchy is reflected in the rationale of NAP, but not in any of the traditional sonority models.


(ref:slopes4examples) Schematic depiction of the sonority slopes of four different onset clusters. The solid red line which denotes the sonority slope of the onset clusters is a plateau in the case of *nmV* and *sfV* and it is falling in the case of *npV* and *spV*. Note that these determinations are based solely on the angle of the red line, regardless of its overall height (i.e., regardless of the intercept of the cluster).
```{r slopes4examples, fig.cap = "(ref:slopes4examples)", fig.asp = .4, out.width = '100%'}
seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("n","p","V","n","m","V","s","p","V","s","f","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=13.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_segment(aes(x=7.5, xend=7.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_segment(aes(x=10.5, xend=10.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.1),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:13),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Times") +
  geom_line() +
  scale_x_continuous("",breaks=2:13, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid","solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey","red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.6,13.5))
}
tribble(~x , ~y, ~line,
        2, 5, "a",
        3, 1, "a",
        3, 1, "b",
        4, 8, "b",
        5,5, "c",
        6,5, "c",
        6,5, "d",
        7,8, "d",
        8,2, "e",
        9,1, "e",
        9,1, "f",
        10,8, "f",
        11,2, "g",
        12,2, "g",
        12,2, "h",
        13,8, "h") %>%
  slopes_plot()
```

(ref:com-4examples-2) Smoothed periodic energy curve (black) of the four syllables from the experimental stimuli---*npal*, *nmal*, *spal* and *sfal* (other details, same as in similar figure above).
```{r com-4examples-2,fig.cap = "(ref:com-4examples-2)", fig.asp=.75, fig.asp=.6, warning=FALSE}
monosyl_examp_2 <- filter(monosyl_info,
                          syl %in% c("sfal","spal","nmal","npal"))
ordered_syl_abs <- unique(monosyl_examp_2[order(monosyl_examp_2$NAP_bu),]$syl)
monosyl_examp_2$syl <- factor(monosyl_examp_2$syl, levels=ordered_syl_abs)
monosyl_examp_2_plot <-
  ggplot(monosyl_examp_2, aes(x=t)) +
  xlim(0,475) + ylim(-3,19) + #ggtitle("") +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-1, yend=13), color="royalblue1", size=1.7, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-1, yend=13),color="red", size=1.7, alpha=.6, linetype = "solid", lineend = "round") +
  geom_text(aes(x=com_onset,y=15,label="CoM:ons"), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Times", check_overlap=T, na.rm = T) +
  geom_text(aes(x=x_com_syl,y=15,label="CoM:syl"), nudge_x = 20, color="red", alpha=.9, size=3, family = "Times", check_overlap=T, na.rm = T) +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-.75, yend=-.75), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.04, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2,y=-2.5,label=paste0(round(NAP_bu)," ms")), size=3, family = "Times", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=19), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=18,label=text), size=6, family = "Times", check_overlap=T) + 
  #
  facet_wrap(~syl, ncol=2) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Times"), strip.text = element_blank())
print(monosyl_examp_2_plot)
```

Under NAP, the empirical coverage of sonority-based predictions does not run into the same problems that traditional models have by design. NAP is in agreement with hierarchy of traditional predictions with regards to *nmV* and *npV*, where the sonority level of *C*~1~ (the intercept) is relatively high. The syllable-initial energy of the nasal is retained in *nmV* due to the following nasal in *C*~2~, while it is lost in *npV*, given the transition to a voiceless obstruent in *C*~2~. Both cases are ill-formed to some extent due to the sonorous consonant in *C*~1~, yet the falling sonority in *npV* makes it even worse-formed than the nasal plateau due to the discontinuity between two competing masses of *C*~1~ and *V*. NAP is however in disagreement with traditional sonority predictions with regards to the voiceless clusters *sfV* and *spV*. In *NAP~td~*, the lack of sonorous energy at the onset portion of these clusters immediately renders them as well-formed in terms of sonority, making room for other phonotactic restrictions to prefer certain voiceless clusters on articulatory and perceptual grounds, not directly related to perception of syllables (see discussion on the phonotactic division of labor in subsection \@ref(sec:division)). 

This impressionistic description is reflected in the scores of *NAP~td~*, shown in Table \@ref(tab:ordinalscores), where *sfV* and *spV* both receive the relatively high value '3'.
Likewise, *nmV* receives a border-line score of '1', and *npV* is almost at the bottom of this NAP scale with '-1'. Unlike the symbol-based ordinal scores of *NAP~td~*, the continuous signal-based *NAP~bu~* makes no a-priori predictions via symbols, yet the stimuli that we measured for the experiment can be shown to reflect the exact same trend. Figure \@ref(fig:com-4examples-2) shows the periodic energy curve of the four examples, taken from the experimental stimuli. The vertical red line denotes the center of periodic mass of the entire syllable (*CoM~syl~*), while the vertical blue line denotes the center of periodic mass of the first half of the entire syllabic mass (*CoM~ons~*). Greater distance between the two lines implies more competition (= worse-formed). Here, the distance in milliseconds between *CoM~syl~* and *CoM~ons~* is around 50 ms for the two voiceless clusters (***sf**al* and ***sp**al*), it is close to 100 ms with the nasal plateau ***nm**al*, and above 150 ms for the nasal-initial falling sonority slope in ***np**al* (see details in Figure \@ref(fig:com-4examples-2)).
<!-- [^cf17] -->

At a broad empirical level, considering the vast literature on phonotactic issues related to sonority, these results are already significant because they successfully reflect very basic universal trends that have been widely acknowledged since @greenberg1965some, although traditional sonority models consistently contradict them.

#		Experiments {#sec:experiments}
In what follows we present two experiments: (i) an exploratory study with 12 subjects and no repetitions and; (ii) a confirmatory study with 51 subjects and four repetitions [see @nicenboim2018exploratorysk on the importance of dividing up experimental endeavors into exploratory and confirmatory studies]. We start by describing the procedures (subsection \@ref(sec:procedures)), predictions (subsection \@ref(sec:predictions)) and data analysis (subsection \@ref(sec:datanlysis)), before we present the specific details, results and related discussions concerning the exploratory study (Experiment 1, subsection \@ref(sec:experiment1)) and the ensuing confirmatory study (Experiment 2, subsection \@ref(sec:experiment2)).

The audio stimuli, R code and OpenSesame file that are mentioned in the following subsections are all available at the following Open Science Framework address (anonymized link): https://osf.io/y477r/?view_only=07b5a4256f964d829ab8153029958863

##		Procedures {#sec:procedures}
To test the different predictions of the six sonority models (2$\times$NAP, 2$\times$SSP, 2$\times$MSD), we designed a perception task that prompts meta-linguistic syllable count judgement with 29 experimental target items. 
We recorded accuracy and response time results from native German-speaking participants. 
<!-- We recorded accuracy and response time results from 51 native German-speaking participants.  -->
An accurate response to our targets is always the monosyllabic option, yet we use the term "accuracy" to describe participants' responses only in a technical sense, without any assumption about correctness of responses. Participants were presented with a collection of speech items that were produced with one or two vowels, systematically for each combination of consonants in our set. By focusing on the response time of "correct" responses to the target words we in fact measure the time it took participants to decide that a single-vowel stimulus is monosyllabic. We thus interpret the response times of monosyllabic responses as reflective of the processing cost of assigning one nucleus to a given target stimulus with one vowel.

A NAP-based model assumes that this processing cost is tightly related to the nucleus competition between different portions of a syllable, such that response times will reflect the degree of nucleus competition within syllables (more competition = slower responses = worse-formed sequence). Traditional sonority models interpret the processing cost as related to well-formedness in terms of sonority slopes, such that worse-formed clusters are more likely to be misperceived and take longer to process [e.g., @berent2007we; @berent2008language; @berent2009listeners; @berent2012language; @lennertz2010people; @maionchi2015sonority; @sung2016perceptionsk; @young2017markednesssk]. The SSP derives a 3-level ordinal hierarchy of complex onset well-formedness---onset rise > onset plateau > onset fall---essentially predicting that response times will pattern into three groups, in line with the sonority slope of the onset clusters. MSD models derive a slightly more elaborate ordinal hierarchy, where onset rises with small sonority distance pattern below onset rises with a larger sonority distance. The latter are predicted to evoke the fastest responses in MSD models.

###		Materials {#sec:materials}

(ref:targetlist-caption) (\#tab:targetlist) Experimental stimulus set: Onset cluster types in the experiment
(ref:targetlist-caption2) cor = coronal; lab = labial; * = voicing disagreement between obstruents; ** = no labial liquid; *** = dorsal stop /k/ to avoid homorganic coronal cluster /lt/.

\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:targetlist-caption)}
\begin{tabular}{cclcclcclcclcclcclcclcclccl}
\toprule
\multicolumn{1}{r}{\textbf{C1}} & \multicolumn{2}{c}{\textbf{Voiceless}} & \multicolumn{2}{c}{\textbf{Voiced}} & \multicolumn{2}{c}{\textbf{Nasals}} & \multicolumn{2}{c}{\textbf{Liquids}}\\
\multicolumn{1}{l}{\textbf{}} & \multicolumn{2}{c}{\textbf{Fricatives}} & \multicolumn{2}{c}{\textbf{Fricatives}} & & & &\\
\multicolumn{1}{l}{\textbf{C2}} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor}\\
\midrule
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sp}, \textbf{\textesh p}} & \multicolumn{1}{c}{\textbf{ft}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{np}} & \multicolumn{1}{c}{\textbf{mt}} & \multicolumn{1}{c}{\textbf{lp}} & \multicolumn{1}{c}{\textbf{lk}***}\\
\multicolumn{1}{l}{\textbf{Stops}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sf}, \textbf{\textesh f}} & \multicolumn{1}{c}{\textbf{fs}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{nf}} & \multicolumn{1}{c}{\textbf{ms}} & \multicolumn{1}{c}{\textbf{lf}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiced}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{zv}} & \multicolumn{1}{c}{\textbf{vz}} & \multicolumn{1}{c}{\textbf{nv}} & \multicolumn{1}{c}{\textbf{mz}} & \multicolumn{1}{c}{\textbf{lv}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Nasals}} & \multicolumn{1}{c}{\textbf{sm}, \textbf{\textesh m}} & \multicolumn{1}{c}{\textbf{fn}} & \multicolumn{1}{c}{\textbf{zm}} & \multicolumn{1}{c}{\textbf{vn}} & \multicolumn{1}{c}{\textbf{nm}} & \multicolumn{1}{c}{\textbf{mn}} & \multicolumn{1}{c}{\textbf{lm}} & \multicolumn{1}{c}{**}\\
& & & & & & & &\\
& & & & & & & &\\
\multicolumn{1}{l}{\textbf{Liquids}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{fl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{vl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{**}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:targetlist-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

The experimental design is focused on onset consonantal clusters with two members. These CC combinations are composed from a set of consonants with *coronal* and *labial* places of articulation to avoid articulatory effects that may arise from *homorganic* sequences (i.e., adjacent consonants that share the same place of articulation) by exploiting both directions of each combination---coronal-labial (back-to-front) and labial-coronal (front-to-back). The consonantal classes in this experiment include *stops*, *fricatives*, *nasals* and *liquids* to reflect the main classes in traditional sonority hierarchies, with the exclusion of *glides*. 
Although glides (namely /j/ and /w/) were originally included in the exploratory study, we eventually excluded glides from the analysis of the exploratory study, and we completely excluded glides from the ensuing confirmatory study. The main reason being that glides have a complex status, which is not only structurally-determined (a glide in the nucleus is considered a vowel), but it is also theory-dependent: a glide immediately adjacent to a nuclear vowel may be analyzed as a vowel (i.e., syllabified within the nucleus as part of a diphthong) or as a consonant (i.e., syllabified in the onset or coda positions), depending on language and analysis.
The following list summarizes further concerns that were taken into consideration when constructing the stimulus set (see the full set in Table \@ref(tab:targetlist)):

*	For the class of liquids we consider only the lateral /l/, disregarding the sub-class of rhotics that are phonetically varied and inconsistent across different languages. We therefore present no liquid plateau in the experimental set.

*	We use the alveolar /s/ for the class of voiceless sibilants (voiceless coronal fricatives). In *C*~1~ positions we also use the post-alveolar /\textesh/ to monitor language-specific effects that may appear due to restrictions in German, in which /\textesh C/ onset clusters can be licit, while /sC/ onset clusters occur only marginally in loanwords.
<!-- should not be allowed. -->

*	We use stops only in *C*~2~ position. We avoid stops in *C*~1~ position since it is also the phrase-initial position of the stimuli, which is practically devoid of acoustic cues for the closure phase of the stop. Furthermore, we use only voiceless stops in order to keep the amount of stimuli at a reasonable low.

* We use one instance of the dorsal consonant /k/ instead of the coronal /t/ as an alternative to a labial-coronal cluster with a liquid in *C*~1~, thus retaining the same direction of a labial-coronal cluster (i.e., both labial-coronal and coronal-dorsal are front-to-back in terms of their places of articulation). We do not use other dorsals for fricative, nasal or liquid consonants, as these tend to be relatively more marked and more inconsistent between languages.

*	Lastly, we avoid sequences of obstruents that differ in voicing due to the strong cross-linguistic tendency of obstruent clusters to agree in voicing (although note that German allows /\textesh v/ and /\texttslig v/ clusters while banning /\textesh f/).

We thus have 29 CC sequence types that reflect 16 different combinations of classes (16 unique cells in Table \@ref(tab:targetlist), excluding differences in place of articulation), of which 7-8 are considered onset falls, 3-4 are considered onset plateaus (11 total) and 5 are considered onset rises.[^cf18] Of the 29 different clusters, only 3 clusters regularly occur in German words, /\textesh p, \textesh m, fl/, 6 clusters are attested to some degree in German loanwords, /sp, sf, sm, vl, zv, ml/,[^cf19] and one cluster, /\textesh f/, may be considered as similar to German licit clusters with a voiced obstruent following a voiceless one (i.e., /\textesh v/ and /\texttslig v/). 
Thus, the experimental set contains 19 cluster types that are unattested in German words. These unattested CC types appear in 13 of the 16 unique combinations, excluding the three rising sonority clusters with a liquid in *C*~2~, /fl, vl, ml/, that are all attested in German complex onsets to some degree (yet only marginally so in the case of /vl/ and /ml/).

The different CC sequences were embedded within a /CCal/ word-like frame, with the recurring *-al* rime. These /CCal/ tokens were produced with a single vowel, intended to yield monosyllabic items. We also prepared two disyllabic counterparts for each CC type---one with an epenthetic vowel, /CəCal/, and another with a prothetic vowel, /əCCal/.[^cf20] We thus obtained 29 single-vowel target types and 58 associated bi-vocalic filler types.

To record the stimuli, the combined 87 word-like tokens were embedded within carrier sentences, appearing in a non-final position with broad-focus intonation in order to maintain consistent prosody. Carrier sentences were also designed to minimize potential effects of resyllabification as well as co-articulation by controlling the segmental makeup immediately preceding and following target words. 

###		Audio recordings {#sec:audio}
Audio stimuli for the experiment were recorded by a phonetically trained monolingual Hebrew speaker (the first author) in a sound attenuated booth at the phonetics laboratory of the University of Cologne. Speech was recorded via head-mounted headset condenser microphone (AKG C420), capturing mono digital files at a resolution of 44.1kHz sample-rate and 24 bit depth with a Metric Halo MIO 2882 audio interface. Selected audio takes were treated in the original high resolution for DC offset correction and compression of ultra low frequencies under 52Hz (to comensate for some room reverbration effects). Audio was then downgraded from 24 to 16 bit depth with Goodhertz Good Dither dithering to be used in the perception task running on OpenSesame 3.1.9 [@mathot2012opensesame]. The audio that was submitted to analyses by the APP Detector (see subsection \@ref(sec:obtaining)) was also downgraded in sample-rate to 16kHz. Finally all audio takes, at all resolutions, were normalized to the same RMS target of -20dBFS.

Note that since the bottom-up predictions of NAP are derived via measurements of acoustic signals of particular productions rather than from fixed symbolic predictions, the assumption that all things other than the controlled variable are equal in the experimental stimuli should hold also for a large degree of variation that occurs in natural speech. Thus, if a certain segment in one item is slightly longer, shorter, louder or softer than in other comparable tokens, bottom-up NAP is designed to directly account for this variation, while the other symbol-based ordinal models need to assume that such variation is mostly negligible. This allows us to opt for a slightly more ecologically valid experimental paradigm, by using natural speech recordings that were designed and selected to sound as similar as possible, rather than using synthesized speech.

###		Obtaining periodic energy data {#sec:obtaining}
We extract continuous measurements of periodic energy from acoustic signals using the *APP Detector*, a computer code that was introduced in @deshmukh2003detectionsk and developed in subsequent publications [@deshmukh2005use; @vishnubhotla2007detection], with the ability to measure the spectral distribution of periodic energy from digital audio files with a 16kHz sample-rate, effectively measuring periodic energy up to 8kHz [consider that our pitch perception range is limited to periods up to approximately 4kHz, reflecting a ceiling effect of the phase-locking of neural firing rates; @wever1937perception; @attneave1971pitch; @pressnitzer2001lower; @moore2013anintro 46]. We export the periodic energy data from the APP Detector's analysis into R [@R-base], for further data manipulation, visualization, modelling and statistics. 

We sum over the different frequencies that the APP Detector measures to obtain a time series  with the sum of periodic energy over different frequencies at 10 ms intervals.
We fit a smoothed curve to the periodic energy time series, to eliminate small-scale fluctuations in the curve's trajectory, and, finally, the sum of periodic energy is log transformed, where it is divided by a value that reflects the threshold of effective voicing periodicity, thus setting a meaningful zero for the periodic energy floor.[^cf11] 

###		Perception task {#sec:ptask}
The experiment was designed as a forced-choice 2-alternative perception task, prompting meta-linguistic syllable count judgements and recording accuracy and response time information. 
Since all stimuli share the form /CCal/ where the rime *al* is predictable (this becomes clear to participants during the training session), we determined the zero time for valid response times at the the mid point of transition from *C*~2~ to /a/, individually for each one of the 87 stimuli. We used manual segmentation to determine this point for each target. We then excluded response times shorter than 100ms (after the mid point of transition from *C*~2~ to /a/) as too fast to be considered as valid. This led to no exclusions from Experiment 1 and to only one observation being excluded from Experiment 2.

Participants were seated in a quiet room in front of a laptop computer (a MacBook Air 13-inch, Early 2014) running the experiment on OpenSesame 3.1.9 [@mathot2012opensesame], where they listened to the stimuli through a set of closed headphones (Sennhieser HD 201), fed directly from the laptop's internal audio interface. After verifying that participants share a standard understanding of the notion of the syllable with a few German examples of words with one and two syllables, they were instructed to listen to nonce words in an "unknown" foreign language (the use of a speaker with a Hebrew accent in the stimuli that were presented to German listeners was intended to further support a "foreign" type of perception). Participants were instructed to respond quickly and accurately on whether they hear one or two syllables by clicking '1' or '2' on the computer keyboard.[^cf21] 
A training session of ten trials preceded the experimental trials, allowing the participants to familiarize with the task, and allowing the experimenter to adjust listening volume and monitor potential problems and any misunderstandings regarding the task.

##		Predictions {#sec:predictions}

(ref:OrdinalTargetPreds-caption) (\#tab:OrdinalTargetPreds) Well-formedness scores for the 29 experimental items using the five ordinal models that are based on symbolic phonemes: SSP~col/exp~, MSD~col/exp~ and NAP~td~
(ref:OrdinalTargetPreds-caption2) Higher values predict better-formed onset clusters in an ordinal scale (i.e., magnitude of differences between values cannot be inferred from these models).
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:OrdinalTargetPreds-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{Onset cluster types} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{exp}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{exp}}}} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\
\midrule
\multicolumn{1}{l}{\textbf{fl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{4 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{sm}, \textbf{\textesh m}, \textbf{fn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{3 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{vl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zm}, \textbf{vn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{ml}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sf}, \textbf{\textesh f}, \textbf{fs}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zv}, \textbf{vz}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{2}\\

\multicolumn{1}{l}{\textbf{nm}, \textbf{mn}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sp}, \textbf{\textesh p}, \textbf{ft}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{lm}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{mz}, \textbf{nv}, \textbf{lv}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{0}\\

\multicolumn{1}{l}{\textbf{ms}, \textbf{nf}, \textbf{np}, \textbf{mt}, \textbf{lf}, \textbf{lp}, \textbf{lk}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{-1}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:OrdinalTargetPreds-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

(ref:com-monosyl) Well-formedness scores for the 29 experimental items using the continuous *NAP~bu~* model. Smoothed periodic energy curves in black, red vertical lines denote the center of periodic mass of the entire syllable (*CoM~syl~*) and blue vertical lines denote the center of periodic mass of the left portion (from the beginning up to the red line; *CoM~ons~*). Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). Distance between *CoM~syl~* and *CoM~ons~* decreases from left-to-right and from top-to-bottom, reflecting weaker competition scores, i.e., predictions for better-formed clusters.
```{r com-monosyl, fig.cap = "(ref:com-monosyl)", fig.width=7, fig.asp=.8, warning=FALSE}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info[order(monosyl_info$NAP_bu),]$syl)
monosyl_info$syl <- factor(monosyl_info$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info, aes(x=t)) + ylim(-5.5,20) +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Times", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Times", check_overlap=T) +
  #
  facet_wrap(~syl, ncol=5) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Times"), strip.text = element_blank())
print(monosyl_plot_abs)

```

The full set of predictions for the 29 experimental targets is presented for all the symbol-based ordinal models (2$\times$SSP, 2$\times$MSD and *NAP~td~*) in Table \@ref(tab:OrdinalTargetPreds), and for the signal-based continuous model (*NAP~bu~*) in Figure \@ref(fig:com-monosyl). Note that the scores of *NAP~bu~* are presented on a continuous ratio scale, with specific predictions for each token and consequential intervals between scores. The scores in *NAP~bu~* are not a generalization, rather, they are tailored to the specific set of recordings we measured, and they are expected to vary to some extents when measuring different tokens. In contrast, all the other models perform computations on discrete symbolic phonemes and derive scores in an ordinal scale, where the size of interval between scores is unknown.

```{r, include=FALSE}
syl_t <- filter(syl_info) %>%
    filter(text=="a" ) %>%
    group_by(NAP_bu, syl, speaker) %>%
    summarize(tmin = min(t))

other_scores <- read_tsv(file="data_tables/CCal_model_predictions_fix.tsv", 
                           col_types = cols(
                               syl = col_character(),
                               SSP = col_integer(),
                               SSP_obs = col_integer(),
                               MSD = col_integer(),
                               MSD_obs = col_integer(),
                               NAP_td = col_integer()
                           ))

scores <- left_join(syl_t, other_scores) %>% ungroup() %>%
    mutate(NAP_bu = ifelse(nchar(syl)<5,NAP_bu,NA_real_))
```

## Data analysis {#sec:datanlysis}

```{r, include=FALSE}

read_list_opensesame <- function(list_files, speaker = "AA", scores_tbl= scores){

    GLIDES <- c("wlal","wnal","wzal","wsal","wtal","jmal","jval","jfal","jpal",               "welal","wenal","wezal","wesal","wetal","jemal","jeval","jefal","jepal")

    scores_speaker <- scores_tbl[scores_tbl$speaker == speaker,]

    map_dfr(list_files, ~{
        message(.x)
       suppressMessages (read_csv(.x)) %>%
            filter(practice =="no") %>%
           mutate(subj = str_match(logfile, '-([0-9]*)\\.csv')[,2],
                   RT = response_time /1000) %>%
            select(subj, stimulus,RT,correct) })%>%
        mutate(stimulus = str_replace_all(stimulus, 'ʃ','c')) %>% # ʃ
        filter(!stimulus %in% GLIDES) %>% left_join(scores_speaker,by=c("stimulus"="syl")) %>%
        mutate(corrRT = (RT - tmin/1000) * 1000, #in milliseconds
               type = factor(ifelse(nchar(stimulus)==4,"CCal",
                             ifelse(str_detect(stimulus,"^e.*" ),"eCCal","CeCal")),
                             levels=c("CeCal","eCCal","CCal")),
               response = case_when(RT>=3 ~ 0,
                                    correct ==1 & type == "CCal"  | correct ==0 & type != "CCal" ~ 1,
                                    TRUE ~ 2))
}
```

```{r, include=FALSE}

opensesame_pilot <- "data_tables/exploratry_results"

files_pilot <-
    c(list.files(path=paste0(opensesame_pilot,"/list1"), pattern="*.csv",full.names = TRUE),
      list.files(path=paste0(opensesame_pilot,"/list2"), pattern="*.csv",full.names = TRUE))

data_pilot_all <- read_list_opensesame(files_pilot, speaker="AA")

data_pilot <- data_pilot_all %>%
    filter(corrRT > 100)

N_below_100_pilot <- data_pilot_all %>%
    filter(corrRT < 100)
#0

# Sanity checks
N_trials_pilot <- 58
all(summarize(group_by(data_pilot,subj),N=n()) %>% pull(N)== N_trials_pilot)
summarize(group_by(data_pilot,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_pilot %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracy <- summarize(group_by(data_pilot,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracy %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracy %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)
```

```{r, include=FALSE}
library(brms)
run_brms <- function(data, chains = 4, iter = 3000, warmup=1000){
    data <- data %>% filter(type=="CCal", response ==1) %>%
        mutate_at(c("SSP","SSP_obs","MSD","MSD_obs","NAP_td"), ~ factor(., ordered = TRUE)) %>%
        mutate(sNAP_bu = NAP_bu -.5)
null_priors <- c(prior(normal(6, 2), class = Intercept),
                         prior(normal(.5, .2), class = sigma))
effect_priors <-  c(null_priors, prior(normal(0,1), class = b),
                    prior(normal(0,1), class = sd),
                    prior(lkj(2), class = cor))

    message("NAP_bu...")
    NAP_bu <- brm(corrRT ~ 1 +  sNAP_bu + (NAP_bu|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
               chains =chains, iter =iter, warmup = warmup)

    message("null...")
    null <- brm(corrRT ~ 1 + (1|subj), data=data,
               prior = null_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
    chains =chains, iter =iter, warmup = warmup)

message("SSP...")
    SSP <- brm(corrRT ~ 1 +  mo(SSP) +(mo(SSP)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("SSP_obs...")
    SSP_obs <- brm(corrRT ~ 1 +  mo(SSP_obs) +(mo(SSP_obs)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD...")
    MSD <- brm(corrRT ~ 1 +  mo(MSD) +(mo(MSD)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD_obs...")
    MSD_obs <- brm(corrRT ~ 1 +  mo(MSD_obs) +(mo(MSD_obs)|subj), data=data,
                   prior = effect_priors,
                   family =  lognormal(), 
               control = list(adapt_delta=.9995,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("NAP_td...")
    NAP_td <- brm(corrRT ~ 1 +  mo(NAP_td) +(mo(NAP_td)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

list(data = data, models = list(NAP_bu = NAP_bu, null = null, SSP=SSP, SSP_obs =SSP_obs, MSD = MSD, MSD_obs =MSD_obs, NAP_td = NAP_td))
}
```

```{r, mpilot, include=FALSE}

 if(file.exists("data_tables/RDS/m_pilot.RDS")){
     m_pilot <- readRDS("data_tables/RDS/m_pilot.RDS")
 } else {
     m_pilot <- run_brms(data_pilot)
     saveRDS(m_pilot, file = "data_tables/RDS/m_pilot.RDS")
 }
 if(file.exists("data_tables/RDS/kfold_pilot.RDS")){
     
     kfold_pilot <- readRDS("data_tables/RDS/kfold_pilot.RDS")
 } else {
     kfold_pilot <- map(m_pilot$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_pilot, file = "data_tables/RDS/kfold_pilot.RDS")
 }

## loo::compare(x=kfold_pilot)

```

<!-- real data -->

```{r, include=FALSE}
opensesame_german <- "data_tables/confirmatory_results"

files_german <-
    list.files(path=opensesame_german,pattern="*.csv",full.names = TRUE)

data_german_all <- read_list_opensesame(files_german, speaker="AA")

data_german <- data_german_all %>%
    filter(corrRT > 100)

N_below_100 <- data_german_all %>%
    filter(corrRT < 100)

## Sanity checks
N_trials_german <- 232 
all(summarize(group_by(data_german,subj),N=n()) %>% pull(N)== N_trials_german)
summarize(group_by(data_german,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_german %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracy <- summarize(group_by(data_german,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracy %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracy %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracy %>% filter(nchar==4)
bi_acc <- subj_accuracy %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracy %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# 1 bad subject

data_german <- data_german %>% filter(!subj %in% bad_subj)

```

```{r brms-models, include=FALSE}

if(file.exists("data_tables/RDS/m_german.RDS")){
    m_german <- readRDS("data_tables/RDS//m_german.RDS")
 } else {
     m_german <- run_brms(data_german, iter=4000, warmup=2000)
     saveRDS(m_german, file = "data_tables/RDS//m_german.RDS")
 }
# 
```
```{r loo-models, include=FALSE}
if(file.exists("data_tables/RDS/kfold_german.RDS")){
    kfold_german <- readRDS("data_tables/RDS/kfold_german.RDS")
} else {
     kfold_german <- map(m_german$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_german, file = "data_tables/RDS/kfold_german.RDS")
    }
```

```{r weigths-models, include=FALSE}
    
## ## loo::compare(x=loo_pilot)
## loo::compare(x=kfold_german)
## loo::compare(x=kfold_german[-1])
## loo::compare(x=kfold_german[-1][-6])
## loo::compare(x=kfold_german[c("MSD_obs","SSP_obs","null")])


## loo::loo_model_weights(x=loo_german)
## compare(loo_german$NAP_td, loo_german$SSP_obs)
## if(file.exists("data_tables/RDS/weights.RDS")){
##     weights <- readRDS("data_tables/RDS/weights.RDS")
## } else {
    ## weights <-loo_model_weights(,kfold_german$MSD_obs)

        ## xx <- ll_matrix[,c(2,3,4,5,6)]
    ## wxx <- loo::stacking_weights(xx)
    ## names(wxx) <-  colnames(xx)

    ## saveRDS(weights, file = "data_tables/RDS/weights.RDS")
## }
```
```{r loo-proc, include=FALSE}
## w_a <- model_weights(m_german$models$MSD,
##                    m_german$models$MSD_obs,
##                    m_german$models$SSP,
##                   ##  m_german$models$SSP_obs,
##                   ## m_german$models$NAP_bu,
##                   ## m_german$models$NAP_td,
##                   ## m_german$models$null,
##                    weights = "loo")


data_german_s <- m_german$data


## loos <- loo_german %>% map_dfc( ~
##     .x$pointwise[,"elpd_loo"]
##     ) %>% {setNames(.,paste0("elpd_",colnames(.)))}

## data_german_s <-  data_german_s %>% bind_cols(loos)


## data_g_summary <- data_german_s %>%
##     group_by(stimulus, NAP_bu, NAP_td) %>%
##     summarize_at(vars(starts_with("elpd")), mean)


## loo_model_weights(loo_german)

predictions <- m_german$models %>% map(~
                     predict(.x,summary=FALSE) %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_german_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots, eval =TRUE, warning=FALSE, include=FALSE}
data_RT <- data_german_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```

```{r predacc, eval = FALSE, results="hold", include=FALSE}

#-   
   # data_german_s %>% ggplot(aes(x = NAP_bu, y= elpd_NAP_bu - elpd_null) ) +
   #  ##geom_jitter(width = 2, height=0,alpha=.1) +
   #   geom_hex(bins=50) +
   #   scale_fill_gradientn(colours = c("skyblue","darkblue")) +
   #  geom_text_repel(data= data_g_summary, aes(label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "blue", size =5)+
   #  geom_hline(yintercept = 0, linetype ="dashed")+
   #  theme(panel.background = element_blank(), axis.text.y = element_text(size = 6)) +
   #  coord_cartesian(ylim=c(-3,3))

   ## data_german_s %>% ggplot(aes(x = NAP_bu, y= elpd_NAP_bu - elpd_SSP_obs) ) +
   ##  ##geom_jitter(width = 2, height=0,alpha=.1) +
   ##   geom_hex(bins=50) +
   ##   scale_fill_gradientn(colours = c("skyblue","darkblue")) +
   ##  geom_text_repel(data= data_g_summary, aes(label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "blue", size =3)+
   ##   geom_hline(yintercept = 0, linetype ="dashed")+
   ##  coord_cartesian(ylim=c(-3,3))

```

We use a Bayesian data analysis approach implemented in the probabilistic programming language  *Stan* [@Stan2018] using the model wrapper package *brms* [@R-brms_a; @R-brms_b] in *R* [@R-base].^[The complete list of *R* packages that we used is:  `r cite_r("bibs/r-references.bib")`.] An important motivation for using the Bayesian approach is that it    is easy to fit fully hierarchical models with the so-called "maximal random effect structure", which provide the most conservative estimates of uncertainty [@SchielzethForstmeier2009]. In all our models, we use regularizing priors, which we detail below. These priors are minimally informative and have the objective of yielding stable inferences [@chung2013weakly; @gelman2008weakly; @GelmanEtAl2017]. @NicenboimVasishth2016 and @VasishthEtAl2017EDAPS discuss the Bayesian approach in detail in the context of psycholinguistic and phonetic sciences research. We fit the models with four chains and 4000 iterations each, of which 1000 iterations were the burn-in or warm-up phase. In order to assess convergence, we verify that there are no divergent transitions, that all the $\hat{R}$  (the between- to within-chain variances) are close to one,  that the number of effective sample size are at least 10\% of the number of post-warmup samples, and  we visually inspect the chains.

For the statistical models, we take into account that the traditional sonority models and the top-down version of NAP (i.e., *SSP~col~* , *SSP~exp~*, *MSD~col~*, *MSD~exp~*, and *NAP~td~*) are ordinal models, while the bottom-up version of NAP (*NAP~bu~*) is a continuous model. The ordinal models predict that certain groups of onset clusters will be better or worse-formed than other group depending on an ordinal score, but they do  not assume that the score will be equidistant with respect to its effect on the response variable, log-transformed response times. For this reason, the discrete scores of these models are assumed to have a monotonic effect on the log-response time in our task, that is, having a monotonically increasing or decreasing relationship with the log-response time, while  the distance between groups are estimated from the data [@burknerModelingMonotonicEffects2018]. For these models, we assume that all the differences between adjacent categories were a-priori the same, that is, the prior for the distance between scores is a uniform Dirichlet distribution.

In contrast,  *NAP~bu~* is modeled with a continuous predictor which is assumed to have a linear relationship with the log-response times. Finally, as baseline, we fitted a "null" model which assumes no relationship between the stimuli and the response times. 

All the models, included a random intercept and slope by subjects (except for the null model that included only a random intercept) and  the following weakly regularizing priors: $Normal(6, 2)$ for the intercept, $Normal(0, 1)$ for the slope, $Normal_+(0,1)$ for the variance components, and $lkj(2)$ for the correlation between by-participant adjustments. The ordinal models have as well a Dirichlet prior for the simplex vector that represents the distance between the categories set to one for each of its parameters. 


We evaluate the models in three different ways: (i) estimation, (ii) descriptive adequacy, and (iii) model comparison.

### (i) Estimation {-}
We report mean estimates and 95\% quantile-based Bayesian credible intervals. A 95\% Bayesian credible interval has the following interpretation: it is an interval containing the true value with 95% probability given the data and the model [see, for example, @Jaynes1976; @MoreyEtAl2015]. 


### (ii) Descriptive adequacy {-}
We use posterior predictive checking to examine the descriptive adequacy or "fit" of the models [@shiffrinSurveyModelEvaluation2008]: the observed data should look plausible under the posterior predictive distribution of the models. The posterior predictive distribution of each model is composed of simulated datasets generated based on the posterior distributions of its parameters. Given the posterior of the parameters of the model, the posterior predictive distribution shows how other data may look like. Achieving descriptive adequacy means that the current data could have been predicted with the model. It is important to notice that a good fit, that is, passing a test of descriptive adequacy, is not strong evidence in favor of a model; in contrast, a major failure in descriptive adequacy can be interpreted as strong evidence against a model [@shiffrinSurveyModelEvaluation2008]. Thus, we use posterior predictive checks to assess whether the model behavior is reasonable and in which situations is not [see @gelmanBayesianDataAnalysis2013  for further discussion].


### (iii) Model comparison {-}
For model comparison, we examine the out-of-sample predictive accuracy of the different models using k-fold (k=15) cross validation stratified by subjects.^[Pareto smoothed importance sampling approximation to leave-one-out cross validation  [implemented in the package `loo`; @vehtariPracticalBayesianModel2017; @vehtariParetoSmoothedImportance2015] failed to yield stable estimates.] Cross validation evaluates the different models with respect to their predictive accuracy, that is, how well the models generalize to new data.

<!-- special way of handling discrete predictors that are on an ordinal or higher scale (Bürkner & Charpentier, in review). A predictor, which we want to model as monotonic (i.e., having a monotonically increasing or decreasing relationship with the response), must either be integer valued or an ordered factor. As opposed to a continuous predictor, predictor categories (or integers) are not assumend to be equidistant with respect to their effect on the response variable. Instead, the distance between adjacent predictor categories (or integers) is estimated from the data and may vary across categories.  -->
<!-- A main application of monotonic effects are ordinal predictors that can be modeled this way without falsely treating them either as continuous or as unordered categorical predictors. -->
<!-- Akaike model weights -->
<!-- (Vehtari et al., 2017; Wagenmakers & Farrell, 2004) 4 : -->


## Experiment 1: Exploratory study {#sec:experiment1}
Given the various novelties in our proposal, the methodologies for data collection and model implementation were first tested on a small body of real data that we collected before finalizing the methodologies presented above (section \@ref(sec:modelimp) and subsections \@ref(sec:procedures)--\@ref(sec:datanlysis)). We used this exploratory study to test our perception task procedures with traditional models implementation, and to test various methods to estimate competition potentials from the periodic energy curve and from symbolic representations of consonants and vowels. 

The exploratory study was administered in two versions, each with half of the fillers and all of the targets in one block, yielding a total of 58 data points per subject (29 fillers + 29 targets, no repetitions).[^cfpilot] The two different versions were evenly split between participants (six each).

The results of this exploratory study are presented below with the finalized parameters that we also applied to the confirmatory study that follows (subsection \@ref(sec:experiment2)). Due to the small sample size and exploratory nature of the study, we present only the estimation values of Experiment 1, without going into the descriptive adequacy of the models and without analysis of model comparison. 
```{r participants, fig.show="hide"}
library(ggplot2)
pie_theme <- theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face="bold"))

## pilot
subjects_pilot <- read.csv("data_tables/subjects/subjects_pilot.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_pilot, aes(x="",y="",fill=subject_age)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_pilot, aes(x=subject_age)) +
  geom_histogram(binwidth = 1)

pilot_mean_age <- round(mean(subjects_pilot$subject_age))
pilot_min_age <- round(min(subjects_pilot$subject_age))
pilot_max_age <- round(max(subjects_pilot$subject_age))
pilot_N <- length(subjects_pilot$subject_nr)
pilot_male <- length(which(subjects_pilot$subject_gender=="male"))

## main
subjects_main <- read.csv("data_tables/subjects/subjects_main.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_main, aes(x="",y="",fill=subject_age)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_education)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_handedness)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_german_native)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void()
```
```{r participants_plot, fig.cap = "(ref:participants)", out.width = c('50%', '50%'), fig.align = 'center', fig.show="hold"}
# fig.cap = "(ref:participants)", dev='png', fig.ext=c('png', 'large.png'), fig.height=c(2, 2), fig.width=c(2, 2), fig.show="hold"

# ggplot(subjects_main, aes(x="",y="",fill=subject_age)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) + pie_theme
# 
# ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) +
#   scale_fill_brewer(palette="Dark2") + pie_theme

```

###		Participants {#sec:participants1}
The exploratory study consisted of 12 subjects (two males and ten females), all native German-speaking students from the Technische Hochschule Köln, who volunteered to participate in the study. The experiment was administered in a quiet room at the institute's facility in Cologne. The mean age of participants in the exploratory study was 25 (21--30).

### Results of the exploratory study {#sec:results1}

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp_pil <- results_txt(m_pilot$models$SSP, "moSSP")
est_MSD_exp_pil <- results_txt(m_pilot$models$MSD, "moMSD")
est_SSP_col_pil <- results_txt(m_pilot$models$SSP_obs, "moSSP_obs")
est_MSD_col_pil <- results_txt(m_pilot$models$MSD_obs, "moMSD_obs")
est_NAP_td_pil <- results_txt(m_pilot$models$NAP_td, "moNAP_td")
est_NAP_bu_pil <- results_txt(m_pilot$models$NAP_bu, "sNAP_bu")

```
<!-- #### Estimation {#sec:estimation1} -->
For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

* For *SSP~col~*: `r est_SSP_col_pil`.
* For *SSP~exp~*: `r est_SSP_exp_pil`.
* For *MSD~col~*: `r est_MSD_col_pil`.
* For *MSD~exp~*: `r est_MSD_exp_pil`.
* For *NAP~td~*: `r est_NAP_td_pil`.
* For *NAP~bu~*: `r est_NAP_bu_pil`.

```{r}
scores <- data_pilot %>% filter(!is.na(NAP_bu)) %>% 
  distinct(stimulus, NAP_bu) %>% 
  arrange(NAP_bu)

lpal_nap <- scores %>% filter(stimulus == "lpal") %>% 
  pull(NAP_bu) %>% round(0)
lkal_nap <- scores %>% filter(stimulus == "lkal") %>% 
  pull(NAP_bu)%>% round(0)
spal_nap <- scores %>% filter(stimulus == "spal") %>% 
  pull(NAP_bu)%>% round(0)

```

Notice that the posterior of the effect of well-formedness, $\hat\beta$, is not comparable accross models. For the ordinal models, it represents the distance between two adjacent categories had they been equidistant, or in other words, $\hat\beta$ multiplied by the number of categories minus one represents the increase in log-scale between the first and the last category. This means that it is highly affected by the number of categories. For the continuous model, $NAP_{bu}$, $\beta$ represents, the increase in log-scale for one unit in the well-formedness scale. To make it concrete, between, /lpal/ and /lkal/ there are `r lkal_nap -lpal_nap` unit (`r lkal_nap` and `r lpal_nap` respectively); and between /lkal/ and /spal/ there are `r spal_nap  -lkal_nap` unit (since their NAP scores are `r spal_nap` and `r lkal_nap` respectively). However, for all the models, $\hat\beta$ is negative  indicating that well-formedness is associated with faster responses. See the Appendix for the complete output of the models.



<!-- ####  Descriptive adequacy {#sec:posterior1} -->
<!-- create new log(rt) based on the estimates of the model, and we compare the actual log(rt) vs the distribution of "predicted" values. -->
<!-- only misfit is informative -->
<!-- approximation of leave one out pointwise crossvalidation -->
<!-- we compare the $predictive$ accuracy of two models, acc of model1 - acc of model2. a positive difference indicates that the model1 is better than model2. the stimulus text indicates the mean difference in predictive accuracy for each stimulus type -->
<!-- theme(axis.title.x=element_text(size = 12), axis.title.y=element_text(size = 12), axis.text.x=element_text(size = 10), axis.ticks.x=element_blank(), axis.ticks.y=element_blank(), axis.text.y=element_text(size = 10), axis.line.x=element_blank(), axis.line.y=element_blank(), panel.background=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank()) +  -->
<!-- axis.text.x=element_blank(),axis.text.y=element_blank(), -->
<!-- ggtitle("xxx") + xlab("Competition measures") + ylab("NAP(+) vs. xxx(-)") +  -->

<!-- #### Model comparison {#sec:ModelComparison1} -->

### Discussion {#sec:discussion1}
The exploratory study was not sufficient for distinguishing between the models, nor was it designed to do so. The fact that none of the models failed in the estimation results shown above was used to confirm our data collection techniques (subsection \@ref(sec:procedures)) and to finalize our model implementations (section \@ref(sec:modelimp)), with emphasis on the two novel NAP-based models. Importantly, the results of the following confirmatory study (Experiment 2) are consistent with those of Experiment 1.

## Experiment 2: Confirmatory study {#sec:experiment2}
Experiment 2 was designed as the main confirmatory study that we conducted after finalizing our hypotheses and methodologies with the data from Experiment 1. Each experimental block in Experiment 2 consisted of two repetitions of the target words (2 $\times$ 29 $=$ 58) and one trial of each filler word (1 $\times$ 58). The experiment consisted of two blocks with randomized trials, generating altogether four repetitions of the target words (4 $\times$ 29 $=$ 116) and two repetitions of the filler words (2 $\times$ 58 $=$ 116), yielding a total of 232 data points per subject. 

### Participants {#sec:participants2}
Fifty-one native German speakers (29 female, 19 male and 3 "other") participated in the experiment, of which 48 were monolingual (the 3 bilingual speakers had Polish, Low German and Hebrew as their heritage language). The vast majority (30 out of 51) were between the ages 19-29. Eight participants were between 30-39, and 11 participants were between 40-59 years old. There were also two younger participants, between 13 and 18 years old. The vast majority (49 of 51 participants) were right-handed. Of the 51 participants, 34 were students at the University of Cologne who took part in the experiment at the sound attenuated booth of the phonetics laboratory. The other 17 participants took part in the experiment at three different locations---all small quiet rooms within private apartments. All subjects were paid five Euros for their participation. 

We excluded the responses from one participant who failed in our participant inclusion criterion requiring accuracy of at least 75% with bi-vocalic fillers. The bi-vocalic fillers of the forms /CəCal/ and /əCCal/ link correct reponses to the disyllabic choice ("2"), and we expect relatively few monosyllabic choices ("1") in response to stimuli with two separate vowels. 
Indeed, the overall average accuracy of all the 51 participants, when responding to bi-vocalic filler stimuli, was 96%. The excluded participant achieved a much lower accuracy score for bi-vocalic fillers, nearing chance-level with 65%.

### Results of the confirmatory study {#sec:results2}

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp <- results_txt(m_german$models$SSP, "moSSP")
est_MSD_exp <- results_txt(m_german$models$MSD, "moMSD")
est_SSP_col <- results_txt(m_german$models$SSP_obs, "moSSP_obs")
est_MSD_col <- results_txt(m_german$models$MSD_obs, "moMSD_obs")
est_NAP_td <- results_txt(m_german$models$NAP_td, "moNAP_td")
est_NAP_bu <- results_txt(m_german$models$NAP_bu, "sNAP_bu")

```

#### Estimation {#sec:estimation2}

For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

* For *SSP~col~*: `r est_SSP_col`.
* For *SSP~exp~*: `r est_SSP_exp`.
* For *MSD~col~*: `r est_MSD_col`.
* For *MSD~exp~*: `r est_MSD_exp`.
* For *NAP~td~*: `r est_NAP_td`.
* For *NAP~bu~*: `r est_NAP_bu`.

See section \@ref(sec:results1) for the interpretation of $\hat\beta$, and the Appendix for the complete output of the models. 

####  Descriptive adequacy {#sec:posterior2}

The null model is shown in in Figure \@ref(fig:NullFit) as a baseline. The slight differences in predictions for different clusters are due to individual differences in the accuracy. Recall that  we subset the response times conditional on the monosyllabic response ('1') to the forced-choice task. This means that when participants gave more monosyllabic answers for a specific  cluster, their adjusted intercept will have a greater influence on the predictions of the model for that cluster. In addition, clusters with fewer monosyllabic responses show more variability in their predictions (e.g., /lf/ vs. /fl/).

The model fit of the six sonority models is shown in Figures \@ref(fig:SSPcolFit)--\@ref(fig:NAPbuFit). The plots in these figures present the dispersion of the average response time results, depicted as red points for related CC clusters, vis-à-vis each models' predictions in the form of distributions, depicted with blue violins. The order of the stimuli, from left to right, follows from the models' scores such that predictions for better-formed clusters appear further to the right. <!-- We discuss these model fits further in the discussion of the results, in subsection \@ref(sec:discussion2). -->

(ref:NullFit) Observed mean log-transformed response times are depicted with red points, distribution of simulated means based on the null model are depicted with blue violins. Stimuli ordered from left to right according to their ascending well-formedness score in *NAP~bu~* (here in forced-ordinal scale for exposition purposes).
```{r NullFit,fig.cap = "(ref:NullFit)", warning=FALSE, fig.width=7, fig.asp=.4}
predictions$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Times") +
    scale_x_discrete("Stimuli", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 8, family = "Times"), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times")) +
    ggtitle("Null model fit")
```

#### Traditional sonority models fit {#sec:traditionalModelfit}

(ref:SSPcolFit) *SSP~col~* model fit: Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness.
```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, fig.width=8, fig.asp=.4}
# ```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, fig.width=c(7,7), fig.asp=c(.4,.4), fig.show="hold"}

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1.2) +
    xlab(bquote(italic(SSP[col])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 8, family = "Times"), axis.ticks.x = element_blank(), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times")) +
        ggtitle(bquote(italic(SSP[col])~fit))
   }
```
(ref:SSPexpFit) *SSP~exp~* model fit (plot details are same as above).
```{r SSPexpFit,fig.cap = "(ref:SSPexpFit)", warning=FALSE, fig.width=8, fig.asp=.4}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(SSP[exp])~scores)) +
       ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 8, family = "Times"), axis.ticks.x = element_blank(), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times")) +
        ggtitle(bquote(italic(SSP[exp])~fit))
     }
```
(ref:MSDcolFit) *MSD~col~* model fit (plot details are same as above).
```{r MSDcolFit,fig.cap = "(ref:MSDcolFit)", warning=FALSE, fig.width=8, fig.asp=.4}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[col])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 8, family = "Times"), axis.ticks.x = element_blank(), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times")) +
        ggtitle(bquote(italic(MSD[col])~fit))}
```
(ref:MSDexpFit) *MSD~exp~* model fit (plot details are same as above).
```{r MSDexpFit,fig.cap = "(ref:MSDexpFit)", warning=FALSE, fig.width=8, fig.asp=.4}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[exp])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 8, family = "Times"), axis.ticks.x = element_blank(), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times")) +
        ggtitle(bquote(italic(MSD[exp])~fit))}
```

We consider a good fit in the case of the ordinal models to be roughly characterized by the following three criteria:
(i) the data is contained within the predictions, i.e., the red points appear within the respective violins;
(ii) the data is consistent within each predicted level, i.e., the vertical dispersion of red points pattern together around the same area within each level (preferably in the middle of the distribution), and;
(iii) the model predictors are not redundant, i.e., the violins of the different model levels show little overlap between them.

A quick observation at the four Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) reveals a common failure of all the traditional sonority models to contain the nasal plateaus (/mn/ and more so /nm/) within their predicted distribution alongside all the other plateaus ('0' model score in all these figures). Furthermore, the data within the plateau level appears to be widely dispersed. 

A closer observation at the two left-most columns, reflecting the onset fall and onset plateau levels, presents a comparison between the two sonority hierarchies---*col* (*SSP/MSD~col~*) and *exp* (*SSP/MSD~exp~*). The difference between these two sonority hierarchies results in different allocation of the fricative-stop clusters /\textesh p, sp, ft/, as plateaus in the *col* hierarchy ('0' in Figures \@ref(fig:SSPcolFit) and \@ref(fig:MSDcolFit)) and as falls in the *exp* hierarchy ('-1' in Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). The latter case of the *exp* hierarchy, whereby the fricative-stop clusters are treated alongside the most ill-formed onset falls, leads to two distinct patterns in the vertical dispersion of data at the left-most column of onset falls (Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). 
This suggests that the *col* hierarchy (where all obstruents are grouped into one class on the sonority hierarchy such that fricative-stop clusters are considered plateaus) is better than the *exp* hierarchy in treating fricative-stop clusters, as reflected in the better model fits for onset falls and plateaus when the *col* hierarchy is applied (*SSP/MSD~col~* vs. *SSP/MSD~exp~*). However, the two sonority hierarchies also lead to differences in the grouping of onset rises when the MSD models are taken into account, as we observe next.

The right side of these plots, i.e., the columns that reflect well-formed onset rises with positive model scores, present three types of grouping across the four models. The two SSP models (*SSP~col/exp~*) make identical predictions with respect to onset rises, lumping all rises into one category ('1' in Figures \@ref(fig:SSPcolFit)--\@ref(fig:SSPexpFit)). The vertical dispersion of data in the right-most column of the SSP models appears very wide, with voiced-initial clusters like /ml, vn, vl/ appearing to pattern separately from voiceless-initial clusters like /\textesh m, sm, fl/.
In contrast, the MSD models present multiple levels of well-formedness for onset rises. *MSD~col~* exhibits two levels of rises ('1--2' in Figure \@ref(fig:MSDcolFit)) while *MSD~exp~* exhibits four levels of rises ('1--4' in Figure \@ref(fig:MSDexpFit)). The grouping in *MSD~col~* appears to result in a relatively high overlap between the distributions of the three right-most columns (including onset plateaus), suggesting redundancy in the model. In comparison, the grouping in *MSD~exp~* seems to capture the distinct patterns of onset rises more accurately, with more levels yet less overlap. Therefore, it appears that the combination of the *exp* hierarchy with the MSD (*MSD~exp~*) has the best fit with respect to onset rises.

To conclude, an observation of the model fit of the four traditional sonority models brings up a mixed picture: the *col* hierarchy (*SSP/MSD~col~*) appears to result in a better fit with onset falls and plateaus, while *MSD~exp~* seems to have the best fit with onset rises. The advantage of *col* with respect to onset falls and plateaus is related to the mistreatment of voiceless fricative-stop clusters as ill-formed. The advantage of *MSD~exp~* with respect to onset rises is related to its ability to separate the voiceless-initial from the voiced-initial onset rises, such that voiceless-initial rises are better-formed. These advantages, as we detail next, are built-in into the logic of the NAP models.

#### NAP~td~ model fit {#sec:NAPtdModelfit}

(ref:NAPtdFit) *NAP~td~* model fit (plot details are same as above).
```{r NAPtdFit,fig.cap = "(ref:NAPtdFit)", warning=FALSE, fig.width=8, fig.asp=.4}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", size =1.2)+
  xlab(bquote(italic(NAP[td])~scores)) + ylab("Response time (log scale)") +
      ylim(600,1200) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text.y = element_text(size = 8, family = "Times"), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times"))+
      ggtitle(bquote(italic(NAP[td])~fit))
  }

```

Although *NAP~td~* is an ordinal model like all the traditional sonority models, it follows a different rationale (see subsection \@ref(sec:naptdmodel)), whereby the distinct categories of the model estimate nucleus competition to reflect well-formedness. Figure \@ref(fig:NAPtdFit) shows that *NAP~td~* is the only model that succeeds in containing all the data (points) within the respective predictions (violins). 

However, *NAP~td~* appears to exhibit some redundancy, as suggested by the large degree of overlap between some of the predictive distributions of the model. This is apparent from the overlap between distributions (blue violins) in the two right-most columns as well as the three left-most columns in Figure \@ref(fig:NAPtdFit).

#### NAP~bu~ model fit {#sec:NAPbuModelfit}

(ref:NAPbuFit) *NAP~bu~* model fit (plot details are same as above).
```{r NAPbuFit,fig.cap = "(ref:NAPbuFit)", warning=FALSE, fig.width=7, fig.asp=.4}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Times") +
    scale_x_discrete(bquote(italic(NAP[bu])~scores), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 8, family = "Times"), axis.title = element_text(size=11, family = "Times"), plot.title = element_text(size=14, family = "Times"))+
    ggtitle(bquote(italic(NAP[bu])~fit))
```

*NAP~bu~* is different from all the other models in that it presents scores that are specific to each token in a continuous rather than ordinal scale (i.e., the distances between scores in the model are also predicted). See Figure \@ref(fig:NAPbuFit) where the expected negative correlation between response time and well-formedness appears to generally hold for both the predictions and the data of the model fit of *NAP~bu~*.
Our criteria for goodness of fit based on the plots (see opening subsection \@ref(sec:traditionalModelfit)) are not all valid when evaluating *NAP~bu~* since we have no classes and no vertical dispersion of data (red points) within levels, and since the horizontal overlap of predictions (blue violins) between levels requires different interpretation. However, the criterion for inclusion of data within the model's predictions naturally also holds for the *NAP~bu~* fit, which fails to include the data for the nasal plateaus /nm/ and /mn/ within their respective predictive distribution (a failure that is shared by the traditional models; subsection \@ref(sec:traditionalModelfit)). Furthermore, *NAP~bu~* also fails to include the /z/-initial clusters---/zm/ and /zv/---within their respective predictive distribution.

The failures in the fit of the *NAP~bu~* model can be split into two types:
(i) nasal-initial clusters---*nval*, *nmal* and *mnal*---which received results on a par with the slowest responses in the data, reflecting an overestimation of well-formedness by the model, and;
(ii) syllables beginning with a voiced sibilant---*zval* and *zmal*---which received results that pattern with faster responses, reflecting well-formedness underestimation by the model. 
These results may be taken to suggest that the distinctive high-frequency aperiodic energy of sibilants has a top-down repeller effect on the nucleus, while nasals, that can attract the nucleus in German (syllabic nasals occur in German), have a top-down attraction effect on the nucleus.[^cf_td]  

#### Model comparison {#sec:ModelComparison2}

```{r resultsmodels, results = "asis"}

comparison <- loo::loo_compare(x=kfold_german)
ll_matrix <- map2_dfc(kfold_german, names(kfold_german), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_german)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison",note= "The table is ordered by the expected log-predictive density (elpd) score of the models, with a higher score indicating better predictive accuracy. The highest scored model is used as a baseline for the difference in elpd and the difference standard error (SE). The column weight represents the weights of the individual models that maximize the total elpd score of all the models.")
```

Results of the model comparisons (see Table \@ref(tab:resultsmodels)) reveal a very clear advantage of *NAP~bu~* in making out-of-sample predictions using cross-validation (i.e., predicting unseen items) over all the other models considered. The difference in expected log-predictive density from the second-best model, *NAP~td~* (*Difference in elpd* = -95.37), is more than four times larger in absolute terms than the difference in standard error (*Difference SE* = 22.78), and this ratio only increases with all the other comparisons, giving *NAP~bu~* a strong lead in the model comparison.

Model averaging via stacking of predictive distributions (model *weights*) reveals that the combination of both *NAP~td~* and *NAP~bu~* adds up to 87% of the combined ability of all models to maximize the *elpd* score together (i.e., make out-of-sample predictions). Importantly, while *NAP~bu~* contributes the lion's share with 56%, the contribution of *NAP~td~* is quite substantial with 31%, suggesting that the two models are, indeed, complementary to a large extent. The added contribution of the traditional models is comparatively negligible with mere 12% contributed solely by *MSD~exp~*.

```{r resultsmodelsohnenaptd, results="asis", collapse=FALSE}
## NAPbu vs. trad
kfold_classic <- kfold_german[!names(kfold_german) %in% c("NAP_td")]
comparison <- loo::loo_compare(x=kfold_classic)
ll_matrix <- map2_dfc(kfold_classic, names(kfold_classic), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_classic)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{bu}$ and traditional models comparison",note= NULL)
```
```{r resultsmodelsohnenapbu, results="asis"}
## NAPtd vs. trad
kfold_ordinal <- kfold_german[!names(kfold_german) %in% c("NAP_bu")]
comparison <- loo::loo_compare(x=kfold_ordinal)
ll_matrix <- map2_dfc(kfold_ordinal, names(kfold_ordinal), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_ordinal)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{td}$ and traditional models comparison",note= NULL)
```
To verify the superiority and relative complementarity of the two NAP models, we also ran the model comparison without one of the NAP versions. We did this once without *NAP~td~* (Table \@ref(tab:resultsmodelsohnenaptd)) and once without *NAP~bu~* (Table \@ref(tab:resultsmodelsohnenapbu)). We observe the *weight* column in these tables to see how the different models maximize the ability to make out-of-sample predictions using cross-validation with only one type of NAP alongside the four traditional models. When *NAP~bu~* is our only NAP model (Table \@ref(tab:resultsmodelsohnenaptd)), its contribution to maximizing the collective ability of the models to make out-of-sample predictions rises to 77%, such that the combined contribution of the traditional models (excluding *SSP~exp~*) only rises to 23%.
When *NAP~td~* is our only NAP model (Table \@ref(tab:resultsmodelsohnenapbu)), its contribution to maximizing the collective ability of the models to make out-of-sample predictions using cross-validation rises to 59%, such that *MSD~exp~* contributes only 15% and the rest (25%) is attributed to the null model. 
The consistent picture that emerges is that while the two NAP models appear complementary to a large extent, *NAP~td~* alone completely subsumes all the traditional models, excluding *MSD~exp~* (Table \@ref(tab:resultsmodelsohnenapbu)), and *NAP~bu~* alone considerably reduces the contribution of the traditional models, where they make only relatively small contributions (Table \@ref(tab:resultsmodelsohnenaptd)). *MSD~exp~* is the only traditional model that remains complementary to some degree in all scenarios, yet with a relatively small contributions of 10-15%.

Tables \@ref(tab:resultsmodelsohnenaptd)--\@ref(tab:resultsmodelsohnenapbu) also demonstrate the superiority of *NAP~bu~* compared to all other models. Note how the *Difference in elpd* and *Difference SE* columns in Table \@ref(tab:resultsmodelsohnenapbu) are rather inconclusive when *NAP~td~* serves as winning baseline: the absolute size of the *Difference in elpd* values is almost equal to the *Difference SE* of the second best model, *MSD~exp~* (-24.63 vs. 23.9), and less than twice as big from the third best model, *SSP~exp~* (-43.41 vs. 24.34). In contrast, when *NAP~bu~* is compared alone against the four traditional models (Table \@ref(tab:resultsmodelsohnenaptd)) the absolute size of *Difference in elpd* of the second best model, *MSD~exp~*, is already more than six times the size of the *Difference SE* (-120 vs. 18.83).

```{r resultsmodelsohnenap, results="asis"}
## trad vs. trad
kfold_classic <- kfold_german[!names(kfold_german) %in% c("NAP_bu","NAP_td")]
comparison <- loo::loo_compare(x=kfold_classic)
ll_matrix <- map2_dfc(kfold_classic, names(kfold_classic), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_classic)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="Traditional models comparison (without NAP)",note= NULL)
```
Finally, we ran the model comparison without any of the NAP models, observing the results for the four traditional models (Tables \@ref(tab:resultsmodelsohnenap)). *MSD~exp~* leads among the traditional models we tested, but this lead is only conclusive with regards to *SSP~exp~*, where the absolute size of the *Difference in elpd* values is more than three times the *Difference SE* (-18.78 vs. 5.58). The individual contribution of the models to maximizing their combined predictive accuracy gives 44% weight to *MSD~exp~* and 43% weight the the two models that used the *col* hierarchy, *SSP~col~*  (25%) and *MSD~col~* (18%), in line with our analysis of the traditional models fit in subsection \@ref(sec:traditionalModelfit).

### Discussion {#sec:discussion2}
<!-- In the following subsections we overview the fit of the six sonority models and we point at apparent strengths and weaknesses of the models, based on the plots in Figures \@ref(fig:SSPcolFit)--\@ref(fig:NAPbuFit). Smaller and less-detailed versions of these plots are also repeated here for easier overview in Figure \@ref(fig:SonFitAll). -->

The results of the confirmatory study (Experiment 2) can be summarized as follows: 
(i) all of the sonority models we tested are capable of explaining the response time data of the different consonant clusters to some extent; 
(ii) the two NAP models, *NAP~td/bu~*, have a much better predictive accuracy than all the traditional models, *SSP/MSD~col/exp~*; 
(iii) the continuous bottom-up NAP model, *NAP~bu~*, outperforms all the the other ordinal models (including *NAP~td~*) in making out-of-sample predictions using cross-validation; 
(iv) the combined contribution of the top-down and bottom-up NAP-based models (*NAP~td/bu~*) is complementary to a considerable degree.

The success relative to the traditional model fits (discussed in subsection \@ref(sec:traditionalModelfit)) can be mainly attributed to the following traits of NAP:
(i) all the voiceless-initial onset clusters, including onset falls and plateaus (e.g., /sp/ and /sf/), are relatively well-formed in NAP<!-- (two right-most columns in Figure \@ref(fig:NAPtdFit)) -->, correctly predicting the patterning together of such data at the low-right parts of the plots given faster response times;
(ii) onset rises like /ml/, nasal plateaus (/nm/ and /mn/) and onset falls like /lm/ pattern together as relatively ill-formed in NAP<!-- (third column from left in Figure \@ref(fig:NAPtdFit)) -->, correctly predicting the data, whereby sonorant-initial plateaus and rises do not pattern with (better-formed) obstruent-initial plateaus and rises.

(ref:SonFitAll) Combined sonority models fit (small reduced versions, see detailed versions above). Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness. 
```{r SonFitAll,fig.cap = "(ref:SonFitAll)", warning=FALSE, out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.5,.5,.5,.5,.5,.5), fig.show="hold", fig.align = "default"}
# fig.ncol=2

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1) +
    xlab(bquote(italic(SSP[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_text(size = 8, family = "Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[col])~fit))}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_text(size = 8, family = "Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[col])~fit))}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(SSP[exp]))) +
       ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_text(size = 8, family = "Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[exp])~fit))}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[exp]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text.x = element_text(size = 8, family = "Times"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[exp])~fit))}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    # geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha = .7, size =1)+
  xlab(bquote(italic(NAP[td]))) + ylab("Response time (log scale)") +
      ylim(600,1200) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
      ggtitle(bquote(italic(NAP[td])~fit))}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1) +
    # geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "S", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Times") +
    scale_x_discrete(bquote(italic(NAP[bu])), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
    ggtitle(bquote(italic(NAP[bu])~fit))

```

####		Sonority distance  {#sec:distance}
The notion of sonority distance is crucial to MSD-based models, and it also plays a major role in NAP models, although with different assumptions and consequent implications than the MSD implies. In both cases, the results of the experiment seem to only weakly support the different distance hypotheses put forward by the MSD and by our NAP models. 

##### Sonority distance in MSD models {#sec:msdistance}
The MSD was developed to elaborate on the mechanism of the SSP and to allow the distinction between different types of rising sonority slopes in onsets. The MSD is essentially used to differentiate clusters with small sonority rises, like *psV* or *bnV*, from less marked (more common) clusters with a larger sonority distance, like *plV* or *brV* (see Figure \@ref(fig:slopes-pl-bn)). In formal terms, the MSD assumes that rising onset slopes are preferred with a steeper angle (see subsection \@ref(sec:principles)).

Our discussion of the traditional sonority models' fit in subsection \@ref(sec:traditionalModelfit) established that the *MSD~exp~* (see Figure \@ref(fig:MSDexpFit)), seems to have the best fit with rising sonority onsets. This, however, is not a strong support of the distance hypothesis in the MSD, as the MSD appears to be highly contingent on the given sonority hierarchy (e.g., *MSD~col~* seems to separate rises less effectively, see Figure \@ref(fig:MSDcolFit)). Moreover, the MSD view, which is restricted to rising slopes, obscures the lack of evidence for distance effects with other voiceless-initial clusters such as *spV*, *ftV* and *fsV*, that are technically not rising yet appear to pattern with well-formed combinations.

Indeed, while there are some phonotactic preferences in some languages that are correctly predicted by the MSD, e.g., when following a stop consonant, English allows liquids while banning obstruents and nasals [although see @easterday2019highly 9 for divergent patterns]. However, this trend appears to break down when *C*~1~ is a sibilant rather than a stop. Thus, when *C*~1~ is /s/, an obstruent in *C*~2~ is cross-linguistically less marked than a sonorant, to the extent that following stops that result in /s/-stop clusters are the most common sibilant-initial clusters [e.g., @morelli2003relative; @enochson2014l2sk; @goad2016sonority]. Thus, classic sonority distance accounts may successfully predict stop-initial phonotactics in some of the languages that permit stop-initial clusters (not necessarily due to sonority), while they contradict the sibilant-initial phonotactics of the vast majority (if not all) of the languages that permit sibilant-initial clusters (again, for reasons not directly related to sonority). We therefore argue that the notion of sonority distance, as it was developed in MSD models, is wrongly placed in the framework of sonority. 

##### Sonority distance in NAP models {#sec:napdistance}
Our view of sonority distance under NAP depends on configurational details that---taken together---may or may not influence nucleus competition. For example, if *C*~1~ is a voiceless obstruent, the amount of sonority distance between *C*~1~ and *C*~2~ should matter very little in terms of competition because a voiceless *C*~1~ has no periodic mass to compete for the nucleus. Thus, when *C*~1~ is voiceless, the build-up of periodic mass can only start from *C*~2~ and it will continue uninterrupted towards the following vowel, much like in simple CV syllables, where nucleus competition and sonority slopes are practically irrelevant. This view is supported by our data, in which all voiceless-initial clusters generally pattern together, with no apparent sonority distance effects differentiating between them.

Sonority distance is however expected to matter under NAP with certain configurations in which the distance between *C*~1~ and *C*~2~ is expected to affect the nucleus competition. Consider, for example, an onset cluster that begins with a highly sonorous consonant (e.g., the liquid /l/ in *C*~1~). The further we decrease the sonority of *C*~2~, the larger will be the sonority distance within the cluster (e.g., *lmV* $<$ *lvV* $<$ *lfV*). This scenario is expected to increase the competition between portions of the syllabified material as sonority distance increases, because in this configuration a larger sonority distance entails a more pronounced discontinuity between the sonority mass of *C*~1~ and that of the vowel. Interestingly, the results of our experiment do not provide a strong support for this prediction of NAP, as most sonorant-initial clusters generally patterned together, with no apparent sonority distance consistently differentiating between them.

Note also that the symbolic version of NAP in its current state is incongruent with some of the above-mentioned NAP assumptions. *NAP~td~* in fact takes sonority distance into account equally for both rising and falling slopes, by measuring the slope in one part of its formula, *C*~2~ – *C*~1~, such that larger values are gradually better and smaller values are gradually worse (see subsection \@ref(sec:ordinalmodels)). Looking at the model fit of *NAP~td~* (Figure \@ref(fig:NAPtdFit)), it makes sense to conclude that the lack of evidence for distance effects in neither rising nor falling sonority slopes in our data is the source of the large overlap between the two most well-formed levels (right side), featuring mostly voiceless-initial clusters, and the three most ill-formed levels (left side) that feature only sonorant-initial clusters.

#####		Intercepts matter {#sec:interceptsmatter}
The idea that we should look at the sonority distance (i.e., the slope) with no reference to underlying energy (i.e., the intercept of the cluster) is a direct outcome of the formal geometric stipulation of sonority in phonological traditions (see subsection \@ref(sec:slippery)). Distance alone should be taken to mean that a fricative-nasal cluster (e.g., *zmV*) is equal to a less-common nasal-liquid cluster (e.g., *mlV*) as in both cases they may be considered one level apart according to most standard sonority hierarchies. This claim is hard to substantiate since the values are drawn from an ordinal sonority scale that does not imply equal intervals. However, the distance between members of plateaus is similar (zero) at all levels of the sonority hierarchy and we can methodologically use plateaus to see if sonority intercepts do matter. Our findings show that such systematic difference, indeed, exists: participants responded much slower when they had to syllabify nasal plateaus in a single syllable, compared to voiced fricative plateaus, and further more when compared to voiceless fricative plateaus (although note that responses to /zval/ patterned with the voiceless plateaus). 
This is also reflected in cross-linguistic patterns, where sonorant plateaus are more marked than obstruent plateaus [@greenberg1978some; @kreitman2008phoneticssk].

To conclude, it seems that sonority distance effects within clusters are negligible compared to the sonority intercepts of onset clusters, that seem to reliably predict fast vs. slow responses in our data. In symbolic terms, we suggest that the sonority level at *C*~1~ position has a stronger effect on nucleus competition than the sonority distance between *C*~1~ and *C*~2~, meaning that the part of the *NAP~td~* formula that computes the slope (*C*~2~--*C*~1~) is not as important as the part that computes the intercept (*V*--*C*~1~). In dynamic terms, we suggest that the amount of periodic energy mass at the start of a unit affects competition and syllabic parsing more than discontinuities between that initial mass and the main periodic energy mass that follows. This implies that the our methodology to compute competition from the signal may still require calibration that would account for potential discontinuity compensation effects in perception.

#		General discussion {#sec:genDiscussion}
Our experimental results provide strong support for our synergy of proposals, including our choice of sonority's perceptual basis and acoustic correlate, the incorporation of continuous entities and dynamic procedures in phonological models, and the dual-route modeling strategy that accounts for both top-down and bottom-up inferences with separate compatible machinery. The following subsections discuss some important implications that are borne out of our interpretation of the results. 
In subsection \@ref(sec:division) we discuss a potential phonotactic division of labor, demonstrated with a holistic account of /s/-stop clusters. 
Subsection \@ref(sec:projection) is devoted to the *nature vs. nurture* debate regarding sonority, raising questions as to the source of the universality of sonority-based restrictions. 
Finally, in subsection \@ref(sec:dichotomies) we discuss the complementarity of discrete and continuous modes in cognitive modeling, suggesting the top-down/bottom-up distinction instead of the classic phonetics/phonology dichotomy. 
<!-- We conclude the general discussion in subsection \@ref(sec:future) where we point at some future directions of this research program. -->

##		Phonotactic division of labor: Towards a holistic account of /s/-stop clusters {#sec:division}
Our account of /s/-stop clusters with NAP does not suffice to explain this phonotactic phenomenon since there is nothing in NAP that is specific to sibilants or stops that would make the specific combination of a sibilant and a stop consonant stand out. In fact, any voiceless element is practically invisible to NAP as it is only sensitive to portions of the speech signal that contain sufficient periodic energy. Indeed, the predictions of NAP, which were corroborated in the experiment, expect non-sibilant counterparts of /s/, like /f/ in the cluster *ftV*, to pattern with *spV* and *\textesh pV*. Furthermore, NAP successfully predicts that all the voiceless-initial clusters in the experiment---including the /s/-stop clusters---generally pattern together as well-formed from sonority's point of view. This may suffice to explain why /s/-stop clusters are tolerated, but not why they are so often preferred over other obstruent combinations. The complete phonotactic story of /s/-stop clusters thus requires an integrative explanation, in which sonority only plays a certain role. In what follows we sketch a more holistic account of /s/-stop clusters that illustrates a potential division of labor between sonority and other major functional phonotactic pressures.<!-- [^cfdesign] -->

Consider the following three types of voiceless obstruent onset clusters:
(i) an /s/-stop cluster, *spV*; (ii) a fricative-stop cluster, *ftV* and;
(iii) a stop-stop cluster, *tpV*. Given NAP's interpretation of sonority, all of these clusters are relatively well-formed to a similar degree.
However, there should be little dispute over the fact that cross-linguistically *spV* is less marked (more common) than *ftV* (which has a non-sibilant fricative in *C*~1~), as well as from *tpV* (which has a stop in *C*~1~). Since these differences are not accounted for by sonority, we sketch below a more complete account of the phonotactics of /s/-stop clusters, which is based on at least the following three different sources:

### (1) Sonority {-}
Voiceless clusters are not ruled-out or disfavored by nucleus competition---they are well-formed according to NAP.  
(well-formedness summary: *spV* $\approx$ *ftV* $\approx$ *tpV*).

### (2) Articulation {-}
Fricative-stop combinations are articulatory preferred over stop-stop combinations in onset clusters. The closure phase of the stop in *C*~2~ occurs mid-cluster, thus considerably damping the release burst of preceding stops in *C*~1~ as this early burst is coordinated with an immediately following airflow blockage (note that a stop in *C*~1~ relies almost entirely on its release burst). In contrast, the constricted airflow of fricatives in *C*~1~ is not compromised when followed by airflow blockage mid cluster. Thus, fricative-stop clusters are relatively better-formed in terms of airflow discontinuity.  
(well-formedness summary: *spV* $\approx$ *ftV* $>$ *tpV*).

### (3) Cue robustness {-}
Sibilants uniquely stand out in perception due to their distinctive high-frequency aperiodic energy that retains robust acoustic cues to their identity in weak positions compared to other voiceless fricatives like /f/ [@wright2004review]. Thus, sibilants are perceptually much more recoverable from marginal (weak) positions in a syllable.  
(well-formedness summary: *spV* $>$ *ftV* $\approx$ *tpV*).

The first source, *sonority*, is a perceptual account that is related to the syllable. The second source, *articulation*, concerns airflow discontinuities from the perspective of articulation, echoing the formal OCP constraint in many phonological accounts [@leben1973suprasegmental; @mccarthy1979formalsk], banning two consecutive segments of the same type. The third source, *cue robustness*, is also perceptual, like sonority, yet it does not require syllables as it is based on adjacency patterns. These perspectives are complementary, and although they do not represent an exhaustive list of phonotactic pressures, we need at least these three in order to properly appreciate the phonotactic phenomenon of /s/-stop clusters.

##		Universality of sonority {#sec:projection}
A consistent interest within theoretical phonology concerns the universality of sonority-based principles. An impressive volume of publications devoted to this question can be found in the works of Iris Berent and her colleagues, starting with @berent2007we, and followed by many subsequent studies [e.g., @berent2008language; @berent2011syllable; @berent2012language; @berent2012universalsk; @berent2013phnological; @berent2014language; @gomez2014language; @zhao2015universalsk; @berent2015role; @lennertz2015onthesonority; @berent2017origins]. Berent et al. collected mostly behavioral data from perception tasks, where subjects of various different language backgrounds were found to adhere to the SSP, even when presented with combinations that are not attested in their language. The patterns under Berent's consistent scrutiny are usually limited to a set of initial clusters with an onset rise (e.g., *blif*), an onset plateau (e.g., *bdif*) and an onset fall (e.g., *lbif*). Since /s/-clusters and sonorant plateaus are absent from these studies, Berent's experimental results with SSP-based models are largely compatible with NAP, where the hierarchy *blif* (3) $>$ *bdif* (2) $>$ *lbif* (0) is maintained in *NAP~td~* (model scores in brackets).[^cf22] 

Berent and her colleagues present these findings in support of the innateness hypothesis, assuming that all humans share a universal linguistic knowledge, which is genetically encoded (often refereed to as *Universal Grammar* in generative traditions). Importantly, the universal grammar in the generative tradition is not dependent on functional descriptions of the sensorimotor system, allowing a grammar that is composed of formal rules and constraints that do not require a functional motivation. In respect to Berent's claims, the universality of sonority principles thus implies innate knowledge of ordinal sonority hierarchies and mechanisms that compute the sonority slopes within syllables to determine well-formedness.

The interpretation of Berent's findings has been a matter of interest in the literature. Some responses, like @daland2011explaining and @hayes2011interpreting, have argued that the apparent universal phonotactic behaviors that Berent et al. present can be shown to result from speakers' ability to generalize categories and distributions, and use analogy and probabilities to predict unattested forms. For example, if an English speaker encounters unattested *ltV* as well as *tlV*, they can recognize that the latter, *tlV*, is better-formed given that English supplies ample evidence for CC onset clusters in which /t/ is in *C*~1~ and /l/ is in *C*~2~, but no evidence for /l/ in *C*~1~.
Such models can successfully apply statistical learning methods based on the lexicon, without a requirement for prior formal knowledge of sonority [e.g., @jurafsky2009speech; @coleman1997stochastic; @vitevitch2004webbasedsk; @hayes2008maximum; @hayes2011interpreting; @bailey2001determinants; @albright2009feature]. 
@daland2011explaining overview these models and conclude that many of them are capable of making correct sonority-related generalization based on the lexicon. They find that the best performing statistical learners in their review [@hayes2008maximum; and @albright2009feature] incorporate formal linguistic distinctions using *phonological features* that essentially define classes across segments, promoting distinctions along the sonority scale. 
Similar conclusions about the role of abstract phonologial features in models of phonotactic learners can be also found in @futrell2017generative and @jarosz2017inputsk.
However, more recent findings in @mayer2019phonotactic showed improved performance of phonotactic learners based on Recurrent Neural Network Language Models that do not require phonological features, and @mirea2019usingsk found that recurrent neural networks without phonological features outperform phonotactic learners that include them.

<!-- Another possibility for the universality of sonority that @daland2011explaining mention is in line with @blevins2004evolutionary's *Evolutionary Phonology* program, whereby universal pressures design the grammar of speakers over evolutionary time scales that are internalized in terms of phonetics (i.e., not a formal phonological entity). -->
<!-- Blevins' [-@blevins2004evolutionary] *Evolutionary Phonology* program  -->
In contrast to traditional sonority principles, we designed NAP to be compatible with general cognitive processes and the sensorimotor system of humans, not requiring any further assumptions about innate formal knowledge of sonority.
Sonority-based patterns in NAP arise from the general cognitive process that underlies the parsing of the stream of speech into syllables with a pitch-bearing nucleus (i.e., nucleus competition). This requirement for pitch-bearing units may be explained in evolutionary time scales [echoing @blevins2004evolutionary; and @christiansen2016creating] as the inevitable result of the important role of pitch in speech communication [@bolinger1978intonation; @cutler1997prosody; @roettger2019tune] and the observation that tune-text integration occurs with syllable-sized units [e.g., @goldsmith1976autosegmental; @liberman1975intonationalsk; @pierrehumbert1980phoneticssk; @ladd2008intonational]. 
It is of interest to note in this respect that in non-speech voicing, as in singing voices, even a steady pitch is naturally subject to some vibrato effects, i.e., small fluctuations in frequency at around 6Hz, corresponding to roughly 170 ms long intervals, much like syllables in speech [see @cook2001pitch].

Models of statistical learning of phonotactic generalizations based on the lexicon, such as those mentioned above, are valuable in explicating models of top-down inferences based on symbolic representations and probabilistic distributions. Such top-down models should be, however, more restricted in the type of symbolic data that they should be able to access, to the exclusion of formal abstract entities, in line with more recent RNN-based phontactic learners such as those in @mayer2019phonotactic and @mirea2019usingsk. 
The abilities to abstract, generalize, learn and replicate recurring dynamic events in language requires symbolic and discrete (even if fuzzy) categories. A conservative approach to top-down models in the framework we present considers only this type of categorizable entities as viable symbols for top-down inferences, under the assumption that speakers derive top-down inferences from symbolic representations and their distributions in the system.

To conclude, bottom-up NAP subsumes the innateness claims for formal sonority universals with a more general explanation that is based on the workings of the sensorimotor system and the evolution of language systems as pitch-bearing communication systems. At the same time, top-down NAP is in-line with models of a *statistical learner*, which require symbolic interpretation of the signal in order to abstract variable dynamic events with stable forms and generalize over their distributions.

##		Reshuffling dichotomies  {#sec:dichotomies}
This work rejects the classic dichotomy between phonetics and phonology, whereby continuous phenomena are considered phonetic, while phonology is modeled in discrete terms. This dichotomy implies that perception and articulation are continuous, while cognition is discrete, a perspective that, indeed, characterized many areas of cognitive psychology from the second half of the twentieth century.
The antithesis of this discrete view of the mind motivated a shift in cognitive sciences towards more continuous and dynamic models, a shift that has already amassed a compelling body of empirical and theoretical work to support it [e.g., @barlow1972single; @rosen1992temporal; @laks1995connectionistsk; @port1995mind; @case1995evaluation; @kelso1997dynamic; @pouget2003inference; @greenberg2003temporal; @spivey2007continuity; @ghitza2011linking; @giraud2012cortical; @lancia2013interaction; @poeppel2014current; @roessig2019dynamics].
A cognitive system is however incomplete without the notion of a category, and, hence, without any symbols. A synthesis of discrete and dynamic models within a single framework is required for the appropriate description of language systems, allowing dynamic events to be recognized and replicated via stable forms [@pattee2012lawssk]. 
As we demonstrate in this study, speech perception models should more appropriately link the distinction between dynamic and discrete descriptions to the distinction between top-down vs. bottom-up processes, rather than the classic phonetics vs. phonology distinction.
Bottom-up routes in perception are continuous and functional since they adhere to the laws of physics and to the limitations of the sensorimotor system. In contrast, top-down inferences in perception are based on the history of symbolic representations in the language. These symbols and their distributions are learned and constantly updated, and they reflect probabilistic knowledge about the distribution of categorically analyzable units of speech.

In this paper we modeled the notion of sonority and its contribution to language sound systems with the assumption that the two different routes---bottom-up and top-down---are both active when speech inferences occur. Our bottom-up model uses continuous data (periodic energy), dynamic principles (competition) and functional motivation (syllables carry pitch information) to model sonority. Our top-down model is based on generalizations over the discrete categories in the system (although note that our top-down model is not a real statistical learner for reasons that we explain in subsection \@ref(sec:complementary)). The results of model stacking based on our experimental data (see Table \@ref(tab:modelstacking)) support this move, showing that the combined contribution of the two NAP models is largely complementary (56% + 31%), and, when taken together, they contribute 87% of the combined contribution of all the six models in maximizing the expected log-predictive density (elpd) score, reflecting the combined ability of the models to make out-of-sample predictions using cross-validation (see subsection \@ref(sec:ModelComparison2)).

# Conclusions  {#sec:conclusions}
<!-- ##		Future directions  {#sec:future} -->
This project suggests a novel synergy of current theoretical and methodological appraoches in attempt to shed new light on old problems in linguistics. Naturally, the paradigm shift that we propose here for models of sonority, and more generally, for models of phonology, will need to amass more supporting evidence from multiple sources in order to be widely considered, adopted and cosequently developed further. 
In this paper we therefore lay the foundation for such potential long-term contribution. We demonstrated how our set of proposals results in a model of sonority that can account for some of the most persistent problems in phonological theory. 
Importantly, our NAP-based models not only present clear advantages over traditional sonority models in terms of empirical coverage, they are also providing good explanations as to the source and cause of sonority phenomena, linking sonority to pitch intelligibility at the level of the syllable as the most basic utterable unit of sppech. Furthermore, our dual-route strategy to modeling (see subsection \@ref(sec:dichotomies)) makes the important distinction between (bottom-up) signal-based inferences and (top-down) symbol-based inferences, thus appropriately predicting the complamentary contribution of these two essentially different inference routes of the same linguistic phenomena.

<!-- which is organized  -->
<!-- and to syllable -->
<!-- via organization of the basic linguistic units -->
<!-- to some of the underlying questions -->
<!-- providing answers not only  -->
<!-- In this paper we provide strong support for the incorporation of continuity in phonology and the integration of continuous and discrete models of linguistic processing. We demonstrate the advantages of our prposed approach with a new account of sonority (NAP), which we model with theoretical, methodological and empirical advantages over traditional models. -->
Lastly, this study also provides compelling motivations and strong evidence that support our proposal to link the notion of sonority with periodic energy in the acoustic signal. This proposal entails that the role of periodic energy is of great importance to linguistic analysis of speech, beyond the scope of the current proposal. A partial list of relevant topics includes:
(i) acoustic descriptions of prosodic *weight* and prosodic *prominence* in terms of periodic energy mass;
(ii) automatic syllabification procedures based on the main fluctuation rate of the periodic energy curve [useful for a myriad of tasks, including speech rhythmicity studies; see, e.g., @galves2002sonoritysk; @tilsen2013speech; @rasanen2018pre] and;
(iii) improved visualization and analysis of pitch contours in intonation research based on the interaction of the periodic energy and the F0 trajectories [see @albert2018using; and @cangemi2019modellingsk for continuous visualization and quantification procedures in intonational phonology using periodic energy].

Future endeavors within the framework we presented here promise to deliver many more valuable insights into old and new problems in phonology and linguistic theory.
<!-- Alongside such developments from theoretical and methodological perspectives, this study also emphasizes the requirement for more freely-available state-of-the-art tools for acoustic measurements of periodic energy. Current developments that we are aware of attempt to achieve this goal with freely-available (and widely-used) programs like Praat [@boersma2019praat], alongside open-source programming platforms like R [@R-base]. We are certain that these combined theoretical, methodological and technological efforts can make an important contribution to the phonetician/ phonologist toolkit. -->

<!-- # Conclusions  {#sec:conclusions} -->
<!-- NAP rocks! -->


<!-- footnotes -->

<!-- [^cf1]: See @pattee2012lawssk for an overview of Pattee's classic works with contemporary commentary. -->

<!-- Early versions of sonority hierarchies often date back to @sievers1893grundzugesk; @jespersen1899fonetik and @whitney1865relation. @ohala1992alternatives finds earlier mentions in @debrosses1765traite and   -->
[^cf2]: @Donegan1978onthenatural notes that Pāṇini and the Sanskrit grammarians used the term svara to imply some kind of harmonic musical quality which applies mainly to vowels. @parker2002quantifying [p. 58] notes further that the Sanskrit grammarians observed natural classes for speech sounds that are "grouped according to their degree of 'opening' (vivāra)".

[^cf2b]: Note that a related notion of *strength hierarchies* makes similar distinctions, yet in the opposite direction (stronger = less sonorant). Strength hierarchies are mostly evoked in relation with lenition processes rather than syllabic phenomena. 

[^cf3]: The class of vowels is often also divided into (in rising sonority order) high, mid and low vowels [sometimes also with distinctions between central and peripheral vowels; see @gordon2012sonority], but these distinctions will be irrelevant in the context of this paper.

[^cf4]: The *Sonority Dispersion Principle* [SDP; @clements1990role; @clements1992sonority] is a slightly different yet related principle that prefers onset rises with large distance and equal dispersion of sonority index values across the consonantal sequence and the following vowel. The results of the SDP are highly contingent on the given sonority hierarchy and it is not very clear how to apply the SDP with onset sonority falls [among other problems listed in @parker2002quantifying 22--24]. The SDP is therefore mostly invoked in relation to other generalizations that it makes about the status of the onset versus the coda (not directly related to consonantal clusters), by assuming that onsets prefer to maximize sonority distance from the following nucleus while codas prefer to minimize it.

[^cf5]: There are also perspectives, such as @foley1977foundations, @rice1992onderiving and @harris2006phonology [see @blaho2008syntax for an overview], where an abstract definition of sonority is not grounded in the nature of the speech signal, and is therefore not expected/required to have a phonetic basis.

[^cfartic]: It can be argued that implicitly many linguists adopt an articulatory-based account since the main categorical distinctions employed in standard phonological sonority hierarchies reflect distinctions in line with  *manners of articulation*, which can be taken to reflect a gradual articulatory change in degree of opening, going from occlusion (stops) to frication (fricatives), to nasal release (nasals) to partial-constriction (liquids) to no constriction (from glides to low/open vowels).

[^cf6]: Note that some of these issues are not necessarily unique to intensity measurements as they should be more generally related to the problematic assumption that the abstract representation of non-overlapping consonants and vowels can be found in the continuous stream of natural speech.
<!-- Typical issues include the following: Where to set the boundaries of each segment? Should we measure peaks or averages of selected portions? How to deal with the unique profile of stop consonants that include a soft closure portion and a loud transient burst? How to normalize values within and across measurements? -->

[^cf7]: A partial list of some prominent examples includes @sigurd1955rank; @jakobson1956fundamentals; @chomsky1968spesk; @foley1972rule; @ladefoged1971preliminaries; @allen1973accentsk; @fujimura1975syllable; @Donegan1978onthenatural; @ultan1978typological; @price1980sonority; @lindblom1983production; @anderson1986suprasegmental; @vennemann1988preferencesk; @levitt1991syllable; @pierrehumbert1992lenition; @fujimura1997acoustic; @stemberger1997handbook; @boersma1998functional; @kingston1998class; @zhang2001effects; @howe2004harmonic; @clements2009does; @sharma2018significance

[^cf8]: Moras are used to represent quantitative diffrences between light and heavy syllables (weight sensitivity), such that light syllables contain one mora while heavier syllables contain two (and sometimes even three) moras [see @hyman1984atheory; @mccarthy1990footsk; @hayes1989compensatory; @ito1989prosodic; @zec1995sonority; @zec2003prosodic].

[^cf9]: This view of sonority is explicitly perceptual. However, it does not contradict and it should be considered as complementary to syllabic descriptions from an articulatory point of view, such as timing coordianation and phase relations that characterize syllabic organization in Articulatory Phonology (@browman1992articulatory; @macneilage1998frame; @goldstein2007syllablesk; @hermes2013phonologysk; @gafos2014stochastic).

[^cfstress]: It is important to note that weight sensitivity is not a universal process, as stress assignment patterns vary from language to language, and not all languages even have stress to begin with. However, it is one of the naturally occurring stress assignment patterns that many unrelated languages exhibit, e.g., Arabic, Tibetan (Lhasa), Wolof, Finnish and Latin [see @goedemans2013weight; and @gordon2006syllableweight 23 for more exhaustive lists].

[^cfglides]: We expect only vocoids (i.e., glides and vowels) to be able to compete for the nucleus from this *C*~2~ position, and we excluded glides from this study.

<!-- [^cf10]: There is, in fact, a more intricate text-tune interaction that can also lead to local "heaviness" of stressed syllables (i.e., on-line prosodic enhancements of nuclei) in order to accommodate certain tonal events in post-lexical intonation via extended duration and/or increased acoustic energy [@roettger2019tune]. -->

[^cf11]: We apply Tukey's (Running Median) Smoothing "3RS3R" to the periodic energy data. The threshold of the log transform is calculated as the maximal value obtained for voiceless portions in the set, less than 5% of the scale before transformation.

[^cf12]: The periodic energy curve is fitted with Local Polynomial Regression Fitting (*loess*) in the figures. This is only used for visual clarity---all the data points and calculations are based on the periodic energy curve before this smoothing and after initial smoothing and log transform (see details in subsection \@ref(sec:obtaining)).

[^cf13]: Note that some rhotics, which are traditionally considered liquids, may in fact belong with the vocoid consonants (e.g., most of the English rhotics). However, we ignore this issue here since we do not include rhotics in this paper.

[^cf13a]: Note that the addition of the articulatory contact distinction to the symbolic sonority hierarchy in NAP  does not have any implications on the following predictions of *NAP~td~*.

[^cf14]: The symbolic sonority hierarchy in NAP reconciles perceptual and articulatory approaches to sonority by modelling their mutual contribution to enhancing pitch intelligibility (or periodic energy mass in acoustic terms). This hierarchy is similar to a few proposals for sonority hierarchies that combined levels of voicing/periodicity with degree of vocal tract opening [e.g., @lass1988phonology; @miller2012sonority; and @sharma2018significance]. Such hierarchies may be also seen as compatible with source-filter models of speech [e.g., @fant2000source], where the source controls voicing and the filter controls opening.

[^cf15]: A slightly similar calculation can be found in Fullwood's [-@fullwood2014perceptual] *Sonority Angle*, although with different grounds, applications and outcomes.

<!-- [^cf16]: The more relevant range when glides are excluded is from -1 to 5. -->

<!-- [^cf17]: For more accurate values of the CoM distance in these examples: sfal = 48.85 ms; spal = 50.85 ms; nmal = 93.68 ms; and npal = 167.74 ms. -->

[^cf18]: Depending on whether fricatives are considered higher or similar in sonority to stops, clusters of the type fricative-stop may be considered as either an onset fall or an onset plateau.

[^cf19]: Based primarily on @van2012sonority.

[^cf20]: The schwa in this case represents a weak (unstressed) /e/ vowel from the 5-vowel inventory of Modern Hebrew.

[^cf21]: Participants used their left index finger to choose '1' and their right index finger to choose '2', using ‘F' and ‘J' buttons respectively. The respective buttons were covered with salient red-on-white ‘1' and ‘2' stickers (the rest of the laptop's keys are all white-on-black by design).
<!-- ‘F' and ‘J' buttons denote the resting place of the two index fingers in a QWERTY keyboard layout when using standard typing methods (e.g., Touch typing).  -->
<!-- In the experiment, these buttons were covered with salient red-on-white ‘1' and ‘2' stickers. -->

[^cfpilot]: The exploratory study included also nine clusters with glides, which we later discarded from our confirmatory study. The actual number of data points per subject was therefore higher, with 76 data points per subject (adding nine targets and nine fillers that we later discarded from analysis).

[^cf_td]: Note that language-specific top-down biases, such as the question of syllabic consonants in a given language, are not covered by our top-down model, *NAP~td~*, which is limited to the universal aspects of NAP. That said, *NAP~td~* succeeded in fitting the data for /z/-initial and nasal-initial clusters within its predictions.

<!-- [^cfdesign]: Recall that the experiment is designed as a syllable count task which allows us to interpret response times as evidence of the processing cost that is associated with parsing incoming speech into a single syllable. This design has the advantage of a relatively percise targeting of sonority's contribution to phonotactics, by observing syllabic parsing behaviour while controlling speech sound combinations. These results should not be interpreted as reflecting well-formedness in a grammar-holistic manner thuough. This would not be an appropriate interpretation of our experimental results which only target the cognitive cost of syllabic parsing. Other functional pressures on phonotactics are always at work at the same time, and they must be taken into account separately, and inegrally, when explaining phonotactics from a holistic point of view.  -->

[^cf22]: *NAP~bu~* cannot make such determinations based on symbolic representations, but it should be expected to generally follow the same trends in the vast majority of cases.

\newpage

# References
```{r create_r-references}
r_refs(file = "bibs/r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup



```{r echo = FALSE, results = 'asis', cache = FALSE}
render_appendix('./appendix.Rmd')
```

