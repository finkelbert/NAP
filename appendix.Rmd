<!-- # Appendix {-} -->

## Experimental procedures {#sec:procedures}
<!-- {#sec:materials} -->
<!-- ## Materials  -->
### Stimulus list considerations {#sec:considerationlist}
The following list summarizes concerns that were taken into consideration when constructing the stimulus set (see the full set in Table \@ref(tab:targetlist)):

*	For the class of liquids we consider only the lateral /l/, disregarding the sub-class of rhotics that are phonetically varied and inconsistent across different languages. We therefore present no liquid plateau in the experimental set.

*	We use the alveolar /s/ for the class of voiceless sibilants (voiceless coronal fricatives). In *C*~1~ positions we also use the post-alveolar /\textesh/ to monitor language-specific effects that may appear due to restrictions in German, in which /\textesh C/ onset clusters can be licit, while /sC/ onset clusters occur only marginally in loanwords.
<!-- should not be allowed. -->

*	We use stops only in *C*~2~ position. We avoid stops in *C*~1~ position since it is also the phrase-initial position of the stimuli, which is practically devoid of acoustic cues for the closure phase of the stop. Furthermore, we use only voiceless stops in order to keep the amount of stimuli at a reasonable low.

* We use one instance of the dorsal consonant /k/ instead of the coronal /t/ as an alternative to a labial-coronal cluster with a liquid in *C*~1~, thus retaining the same direction of a labial-coronal cluster (i.e., both labial-coronal and coronal-dorsal are front-to-back in terms of their places of articulation). We do not use other dorsals for fricative, nasal or liquid consonants, as these tend to be relatively more marked and more inconsistent between languages.

*	Lastly, we avoid sequences of obstruents that differ in voicing due to the strong cross-linguistic tendency of obstruent clusters to agree in voicing (although note that German allows /\textesh v/ and /\texttslig v/ clusters while banning /\textesh f/).

<!-- {#sec:audio} -->
### Audio recordings {#sec:audio}
Audio stimuli for the experiment were recorded by a phonetically trained monolingual Hebrew speaker (the first author) in a sound attenuated booth at the phonetics laboratory of the University of Cologne. Speech was recorded via head-mounted headset condenser microphone (AKG C420), capturing mono digital files at a resolution of 44.1kHz sample-rate and 24 bit depth with a Metric Halo MIO 2882 audio interface. Selected audio takes were treated in the original high resolution for DC offset correction and compression of ultra low frequencies under 52Hz (to comensate for some room reverbration effects). Audio was then downgraded from 24 to 16 bit depth with Goodhertz Good Dither dithering to be used in the perception task running on OpenSesame 3.1.9 [@mathot2012opensesame]. The audio that was submitted to analyses by the APP Detector (see Subsection \@ref(sec:obtaining)) was also downgraded in sample-rate to 16kHz. Finally all audio takes, at all resolutions, were normalized to the same RMS target of -20dBFS.

To record the stimuli, the combined 87 word-like tokens were embedded within carrier sentences, appearing in a non-final position with broad-focus intonation in order to maintain consistent prosody. Carrier sentences were also designed to minimize potential effects of resyllabification as well as co-articulation by controlling the segmental makeup immediately preceding and following target words. 

<!-- Note that since the bottom-up predictions of NAP are derived via measurements of acoustic signals of particular productions rather than from fixed symbolic predictions, the assumption that all things other than the controlled variable are equal in the experimental stimuli should hold also for a large degree of variation that occurs in natural speech. Thus, if a certain segment in one item is slightly longer, shorter, louder or softer than in other comparable tokens, bottom-up NAP is designed to directly account for this variation, while the other symbol-based ordinal models need to assume that such variation is mostly negligible. This allows us to opt for a slightly more ecologically valid experimental paradigm, by using natural speech recordings that were designed and selected to sound as similar as possible, rather than using synthesized speech. -->

<!-- {#sec:obtaining} -->
###		Obtaining periodic energy data {#sec:obtaining}
We extract continuous measurements of periodic energy from acoustic signals using the *APP Detector*, a computer code that was introduced in @deshmukh2003detectionsk and developed in subsequent publications [@deshmukh2005use; @vishnubhotla2007detection], with the ability to measure the spectral distribution of periodic energy from digital audio files with a 16kHz sample-rate, effectively measuring periodic energy up to 8kHz [consider that our pitch perception range is limited to periods up to approximately 4kHz, reflecting a ceiling effect of the phase-locking of neural firing rates; @wever1937perception; @attneave1971pitch; @pressnitzer2001lower; @moore2013anintro 46]. We export the periodic energy data from the APP Detector's analysis into R [@R-base], for further data manipulation, visualization, modelling and statistics. 

We sum over the different frequencies that the APP Detector measures to obtain a time series  with the sum of periodic energy over different frequencies at 10 ms intervals.
We fit a smoothed curve to the periodic energy time series, to eliminate small-scale fluctuations in the curve's trajectory, and, finally, the sum of periodic energy is log transformed, where it is divided by a value that reflects the threshold of effective voicing periodicity, thus setting a meaningful zero for the periodic energy floor.[^cf-smog]

<!-- {#sec:ptask} -->
###		Perception task {#sec:ptask}
The experiment was designed as a forced-choice 2-alternative perception task, prompting meta-linguistic syllable count judgements and recording accuracy and response time information. 
Since all stimuli share the form /CCal/ where the rime *al* is predictable (this becomes clear to participants during the training session), we determined the zero time for valid response times at the the mid point of transition from *C*~2~ to /a/, individually for each one of the 87 stimuli. We used manual segmentation to determine this point for each target. We then excluded response times shorter than 100ms (after the mid point of transition from *C*~2~ to /a/) as too fast to be considered as valid. This led to no exclusions from Experiment 1 and to only one observation being excluded from Experiment 2.

Participants were seated in a quiet room in front of a laptop computer (a MacBook Air 13-inch, Early 2014) running the experiment on OpenSesame 3.1.9 [@mathot2012opensesame], where they listened to the stimuli through a set of closed headphones (Sennhieser HD 201), fed directly from the laptop's internal audio interface. After verifying that participants share a standard understanding of the notion of the syllable with a few German examples of words with one and two syllables, they were instructed to listen to nonce words in an "unknown" foreign language (the use of a speaker with a Hebrew accent in the stimuli that were presented to German listeners was intended to further support a "foreign" type of perception). Participants were instructed to respond quickly and accurately on whether they hear one or two syllables by clicking '1' or '2' on the computer keyboard.[^cf-indexfinger] 
A training session of ten trials preceded the experimental trials, allowing the participants to familiarize with the task, and allowing the experimenter to adjust listening volume and monitor potential problems and any misunderstandings regarding the task.



## Complete output of the Bayesian models

The complete results of the exploratory (Experiment 1) and confirmatory models (Experiment 2) are presented in Tables \@ref(tab:exploratory-NAPbu)-\@ref(tab:confirmatory-NAPtd). 

Notice that the parameters are not entirely comparable accross models: (i) The intercept, $\alpha$, represents the mean log-RT of the first category for the ordinal models, but it is the grand mean for the  continuous model, $NAP_{bu}$. (ii) The size of the effect of well-formedness, $\beta$, represents the distance between two adjacent categories had they been equidistant for the ordinal models but it is the increase in log-scale for one unit in the well-formedness scale for the continuous model, $NAP_{bu}$.  (iii) The parameter vector, $\zeta$, (present only in ordinal models) represents the normalized distances between consecutive predictor categories, so that the distance between the first and last category is $1$. (iv) For all models, the variance components are comparable: $\sigma$ represents the scale of the log-normal likelihood (or standard deviation of the distribution on the log scale), $\sigma_\alpha$ and $\sigma_\beta$ represent the by-participant adjustment to the intercept and slope respectively, and $\rho_{\alpha,\beta}$ represents the correlation between by-participant intercept and slope. 

For each parameter, `Bulk ESS`
and `Tail ESS` are effective sample size measures, and `Rhat` is the potential
scale reduction factor on split chains (at convergence, `Rhat`= 1, and `ESS` $> 10\%$ of post-warmup samples $=1200$).

```{r}

model_names <- c("NAP_bu" = "NAP_{bu}", "null"="Null", "SSP"="SSP_{exp}", "SSP_obs" = "SSP_{col}", "MSD" = "MSD_{exp}", "MSD_obs" = "MSD_{col}", "NAP_td" = "NAP_{td}")

model_tab <- function(models, type) {
   imap_chr(models, ~ {
  label = paste0("\\label{tab:",type,"-",str_remove(.y,"_"),"}")
  .x %>% {rbind(summary(.)$fixed,
                 summary(.)$mo,
                summary(.)$spec_pars,
             summary(.)$random$subj)} %>%
    round(2) %>%
  as_tibble(rownames= "Parameter") %>%
    mutate_at(vars(ends_with("ESS")), as.integer) %>%
  select(-Est.Error) %>%
    rename_all(~ str_replace_all(., "%","\\\\%")) %>% 
        rename_all(~ str_replace_all(., "_"," ")) %>% 
  mutate(Parameter = case_when(Parameter=="Intercept" ~ "$\\hat\\alpha$",
                          str_detect(Parameter,".*?\\[.\\]") ~ 
str_replace_all(Parameter,c(".*?\\[" = "$\\\\hat\\\\zeta_{","\\]" = "}$" )),
Parameter=="sigma" ~ "$\\sigma$",
Parameter =="sd(Intercept)" ~"$\\hat\\sigma_{\\alpha}$",
str_detect(Parameter,"sd.*?") ~"$\\hat\\sigma_{\\beta}$",
str_detect(Parameter, "cor.*?") ~ "$\\hat\\rho_{\\alpha,\\beta}$",
                          TRUE ~ "$\\hat\\beta$")) %>%
  papaja::apa_table(escape= FALSE,
                     caption = paste0("Results from the ",type," model examining the results of the $",model_names[.y],"$ model. ",label= label, note= "See the Appendix for the interpretation of the parameters and column names."))}
) %>% 
  paste(collapse = "\n")
}

knitr::raw_latex(model_tab(m_pilot$models,"exploratory"))
knitr::raw_latex(model_tab(m_german$models,"confirmatory"))

```
