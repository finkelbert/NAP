---
title             : "The double life of language systems: Modelling sonority with complementary symbol and signal based models"
shorttitle        : "Modelling sonority with complementary models"

author: 
  - name          : "Anonymous"

affiliation:
  - institution   : "Affiliation"

authornote: |
  Author Note: In accordance with the Peer Reviewers' Openness Initiative (opennessinitiative.org), all materials and scripts associated with this manuscript were made available during the review process and will remain available (see anonymized link: https://tinyurl.com/5jxmjntx).

abstract: |
  *Sonority* is a fundamental notion in phonetics and phonology, central to many descriptions of the syllable and evidently useful as a predictor of phonotactics.
  Although widely-accepted, sonority lacks a clear basis in speech articulation or perception, and traditional sonority principles, which were not designed to be compatible with cognitive capacities, exhibit systematic gaps in empirical coverage. Traditional sonority models have been exclusively based on discrete and symbolic machinery.
  Against this backdrop, we propose an incorporation of symbol-based and signal-based models to adequately account for sonority with two complementary models.
  We claim that sonority is primarily a perceptual phenomenon related to pitch, driving the optimization of syllables as pitch-bearing units. We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we provide a novel principle that can account for syllabic well-formedness, the *Nucleus Attraction Principle* (NAP).
  We present two perception experiments that test our two NAP-based models against four traditional sonority models and we use a Bayesian data analysis approach to test and compare them. We show that our two NAP models retain the highest degree of complementarity while one of them is superior to all the other models we tested. 
  We interpret the results as providing strong support for our proposals:
  (i) the designation of periodic energy as sonority's correlate; 
  (ii) the incorporation of continuity in phonological models, and; 
  (iii) the dual- model strategy that separates and integrates symbol-based top-down processes and signal-based bottom-up processes.
  
keywords          : "Sonority; Periodic energy; Bayesian data analysis; Phonetics and Phonology"
wordcount         : "X"

bibliography      : ["bibs/r-references.bib", "bibs/methods.bib", "bibs/phon.bib", "bibs/phon_sk.bib"]
appendix:
  - "./appendix_a.Rmd"
  - "./appendix_b.Rmd"
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : yes

documentclass     : "apa6"
classoption       : "doc"
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex
    includes:
      in_header: load.tex
keep_tex: yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r , cache=FALSE,include=FALSE}
# global chunk options
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE,fig.path='figure/graphics-', fig.align='center')#, dev="cairo_pdf")
```

```{r libraries, message = FALSE}
library(R.matlab)
library(ggplot2)
library(dplyr)
library(Cairo)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores =  parallel::detectCores())
library(stringr)
library(readr)
library(purrr)
library(tidyr)
library(loo)
library(brms)
library(ggrepel)
```

# Introduction
The study of the sound system of human languages has been one of the longest-standing intersections of symbol-based categorical analyses on the one hand, and signal-based continuous descriptions on the other. These two different types of analysis stand at the core of the distinction many researchers make between *phonetics* and *phonology*, where the former addresses continuous and measurable aspects of the speech signal (e.g., articulation, perception and acoustics), while the latter addresses categorical aspects of the speech signal using discrete and symbolic units (e.g., consonants, vowels, and syllables).

The following study proposes an incorporation of symbol-based and signal-based models to adequately account for the notion of *sonority*, which has played a pivotal role in the vast linguistic literature on syllables, most often invoked as a powerful tool to predict phonotactics, i.e., the propensity of different consonants and vowels to combine via concatenation.

We claim that sonority is primarily a perceptual phenomenon related to the strength and quality of pitch perception, postulating a universal drive to optimize syllables as pitch-bearing units (see Section \@ref(sec:dynamic)). We suggest a measurable acoustic correlate for sonority in terms of *periodic energy*, and we provide a novel principle that can account for syllabic well-formedness based on general principles of competition in real-time; the *Nucleus Attraction Principle* (NAP).

We implement NAP with two complementary models (see Section \@ref(sec:modelimp)): (i) a bottom-up model that accepts continuous information and links it to predicted categorical responses, and; (ii) a top-down model that is based on discrete segments---consonants and vowels---and their potential distribution within syllables. In suggesting two different models for a single phenomenon we follow the pioneering works of Howard Pattee [see @pattee2012lawssk] rather than the more commonly assumed *Occam’s razor*,
which requires theories to be as simple as possible, thus creating a strong preference for a single model over two different models in explaining a certain phenomenon.
In contrast, Pattee views the link between continuous dynamic modes on the one hand, and discrete symbolic modes on the other hand, as a crucial characteristic of all language systems, yet he argues further that no single model can adequately describe both the continuous and discrete modes of the system, as they are not reducible to a single mathematical representation.

In section \@ref(sec:experiments) we present two perception experiments---exploratory and confirmatory---that we designed in order to test our two NAP-based models (applying NAP with bottom-up and top-down approaches) against four traditional sonority models, considering two types of common sonority hierarchies (with and without distinctions between obstruents), and two types of common sonority principles---the *Sonority Sequencing Principle* (SSP) and the *Minimum Sonority Distance* (MSD).
We use a Bayesian data analysis approach to test and compare the six different sonority models. Whereas all the different models are found to be capable of predicting the experimental results to some extent, the symbolic top-down version of NAP is shown to be superior and appears to exhibit the highest degree of complementarity with the bottom-up model of NAP.

Our synergy of proposals has many advantages over traditional sonority accounts, including methodological aspects, theoretical perspectives, and, most importantly, a better empirical coverage. In Subsection \@ref(sec:discussion2) we interpret the results of the main confirmatory experiment and in Section \@ref(sec:genDiscussion) we address some relevant implications, 
where we discuss the usefulness of the top-down vs. bottom-up distinction instead of the common use of the phonology--phonetics dichotomy to express the distinction between discrete and continuous aspects of linguistic sound systems (Subsection \@ref(sec:dichotomies)).
We also discuss the division of labor between sonority and other phonotactic factors, demonstrated with a holistic account of the phenomenon of /s/-stop clusters (Subsection \@ref(sec:division)), before we present our concluisons in Section \@ref(sec:conclusions). 
<!-- and we provide the technical details on the procedures used in the experiments in Section \@ref(sec:methods). -->

In the remainder of this Introduction, we briefly present the relevant background on traditional sonority hierarchies and sonority principles, emphasizing their rationale, their application, and their inherent flaws (Subsections \@ref(sec:hierarchies)-\@ref(sec:failures)). 

## Sonority hierarchies {#sec:hierarchies}

A sonority hierarchy is a single scale on which all consonant and vowel types can be ranked relative to each other.
<!-- [^cf-strength] -->
Such hierarchies can be traced back centuries, and concepts akin to sonority hierarchies can be found already in the pioneering works of early Sanskrit grammarians.[^cf-sanskrit] 
Early versions of current sonority hierarchies often date back to @sievers1893grundzugesk; @jespersen1899fonetik; and @whitney1865relation, while @ohala1992alternatives even goes as far back as @debrosses1765traite.

While the phonetic basis of sonority hierarchies remains controversial, phonological sonority hierarchies have been primarily based on repeated observations that revealed systematic behaviors of segmental distribution and syllabic organization within and across languages. The general consensus regarding the phonological sonority hierarchy thus stems from attested cross-linguistic phonotactic behaviors of different segmental classes, such as, for instance, the relatively high frequency of stop-liquid sequences in the onset of complex syllables (e.g., /kl/ in the English word ***cl**ean*) and the rarity of the opposite liquid-stop sequences at such onset positions, but not at the mirror-image coda position of syllables (e.g., /lk/ in the English word *mi**lk***). See examples in @zwicky1972notesk; @selkirk1984majorsk; @parker2002quantifying; @jany2007universal, and see @ohala1992alternatives for related criticism regarding the circularity that results from determining sonority hierarchies according to attested behavior without another independent (phonetic) variable.

Most common phonological sonority hierarchies group segment types into classes that primarily reflect the standard *manners of articulation* in traditional phonology. The distinct categories commonly used include *stops*, *fricatives*, *nasals*, *liquids*, *glides*, and *vowels*, often with additional distinctions such as voicing and vowel height.[^cf-liquid] Although there are many different proposals for sonority hierarchies [@parker2002quantifying found more than 100 distinct sonority hierarchies in the literature], a very basic hierarchy that seems to reach a considerable consensus, and is often cited in relation to Clements's [-@clements1990role] seminal paper is given in (\@ref(ex:scale)).

<!-- ```{=latex} -->
\begin{exe} 
\ex Obstruents $<$ Nasals $<$ Liquids $<$ Glides $<$ Vowels  \label{ex:scale} 
\end{exe} 
<!-- ``` -->

The ordering of different speech sounds along the sonority hierarchy is assumed to be universal,
in line with the common assumption that sonority has a phonetic basis in perception and/or articulation, yet the patterning of segmental classes as distinct groups along the scale is considered to be language-specific, i.e., based on phonological categorization. 
For example, voiceless stops may be considered universally lower than voiced fricatives on the sonority hierarchy, yet for some languages and analyses they may constitute a single level of *obstruents*. Classes along the sonority hierarchy are most commonly modeled as a series of integers (often referred to as sonority indices) reflecting the ordinal nature of phonological interpretations of the sonority hierarchy. 

The main differences that result from variation of the basic hierarchy in (\@ref(ex:scale)) concern the class of obstruents, which may contain voiced and voiceless variants of stops and fricatives (to mention just the most prominent distinctions).
<!-- [^cf-vowels]  -->
Note that vowels are often also commonly divided into subgroups along the sonority hierarchy [see @gordon2012sonority], but these distinctions will be irrelevant in the context of this paper.
It is therefore not uncommon to expand the class of obstruents, whereby stops are lower than fricatives and voiceless consonants are lower than voiced ones. 

The two variants of the sonority index values given in Table \@ref(tab:hierarchy) thus reflect two ends of a common sonority hierarchies spectrum, ranging from hierarchies that collapse all obstruents together into a single class (resulting in the same sonority index value for all obstruents), to hierarchies that expand the class of obstruents by employing voicing distinctions as well as distinctions between stops and fricatives (resulting in multiple sonority index values within obstruents). In what follows we will refer to these two versions of the sonority hierarchy as *col* for the *collapsed* sonority hierarchy, and *exp* for the *expanded* sonority hierarchy.

(ref:hierarchy-caption) (\#tab:hierarchy) Traditional phonological sonority hierarchies
(ref:hierarchy-caption2) Index values reflect the ordinal ranking of categories in sonority hierarchies. The obstruents in *col* are collapsed into one category (bottom four rows = '1'), while in *exp* they are expanded into four distinct levels.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:hierarchy-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
\multicolumn{2}{c}{\textbf{Sonority index values}} & \multicolumn{1}{l}{\textbf{Segmental classes}} & \multicolumn{1}{l}{\textbf{Phonemic examples}}\\
\multicolumn{1}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\emph{exp} hierarchy} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}\\
\midrule
5 & 8 & Vowels & \multicolumn{1}{l}{/u, i, o, e, a/}\\
4 & 7 & Glides & \multicolumn{1}{l}{/w, j/}\\
3 & 6 & Liquids & \multicolumn{1}{l}{/l, r/}\\
2 & 5 & Nasals & \multicolumn{1}{l}{/m, n/}\\
\textbf{1} & \textbf{4} & Voiced Fricatives & \multicolumn{1}{l}{/v, z/}\\
\textbf{1}& \textbf{3} & Voiced Stops & \multicolumn{1}{l}{/b, d, g/}\\
\textbf{1}& \textbf{2} & Voiceless Fricatives & \multicolumn{1}{l}{/f, s/}\\
\textbf{1}&\textbf{1} & Voiceless Stops & \multicolumn{1}{l}{/p, t, k/}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:hierarchy-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

## Traditional sequencing principles {#sec:principles}

Sequencing principles can be understood as a mapping scheme between the ranks of a sonority hierarchy and the linear order of symbolic speech segments.
Modern formulations of such principles, which use the ordinal sonority hierarchy to generalize over the phonotactics of consonantal sequences in terms of *sonority slopes* were developed mainly throughout the seventies and eighties of the twentieth century in seminal works such as @hooper1976introduction; @steriade1982greek; @selkirk1984majorsk; @harris1983syllable; @mohanan1986theory, and @clements1990role.
<!-- [^cf-sequencingclassics] -->

Sonority index values, that indicate their ranking on the sonority hierarchy, can be readily plugged into models that are able to predict distributional patterns of segments vis-à-vis syllabic organization in terms of sonority slopes. Consonants and vowels in a given string are interpreted as a sequence of discrete points in symbolic linear time. The corresponding sonority index values that are associated with these segments are then interpreted in terms of slopes that result from interpolation over the sequence of points. Thus, for instance, going from a low ranking segment to a high one is considered to be a rising slope, while two adjacent segments that share the same sonority index incur a plateau. Note that the notion of the *syllable* is required to define the ranges and types of preferred slopes, which optimally rise from the beginning of the syllable to its middle and fall from the middle of the syllable to its end. Syllabic well-formedness is therefore defined in terms of universal generalizations over the preferred and dispreferred types of sonority slopes that result from the concatenation of different consonants and vowels and their grouping into syllables.

The most basic and widely used sonority-based pribciple that derives phonotactic predictions in terms of syllabic well-formedness is the *Sonority Sequencing Principle* (SSP). The SSP is a simple yet powerful generalization about phonotactics that has been evidently useful in countless theoretical accounts. It identifies three distinct types of slopes---*rises*, *falls*, and *plateaus*---such that sequences of segments should rise in sonority from the consonant(s) in the syllabic onset to the syllable's nucleus (most often a vowel) and fall from the nucleus to the consonant(s) in the syllabic coda. In this paper we focus on syllable-initial onset consonant clusters that precede a vowel, whereby a rising sonority slope (e.g., *plV*) is considered well-formed and a falling sonority slope (e.g., *lpV*) is considered ill-formed (see Figure \@ref(fig:slopes-pl-lp)). Sonority plateaus (e.g., *pkV*) fare in between, giving way to various interpretations, depending on the language and analysis, such that plateaus may pattern as ill- or well-formed [e.g., @bat1996selecting], although they are generally interpreted as denoting a third mid-level of well-formedness, as we will treat them here.

(ref:slopes-pl-lp) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *lpV*. The red line denotes the sonority slope of the onset cluster (i.e., the two onset consonants), while the grey line denotes the slope between the second consonant and the vowel at the nucleus position (always a rise in these cases). The angle of the red lines reflects the well-formed rising sonority slope of the onset cluster in *plV* and the ill-formed falling sonority slope of the onset cluster in *lpv*.
```{r slopes-pl-lp, fig.cap = "(ref:slopes-pl-lp)", fig.asp = .45, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","l","p","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority slopes: different types",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="well-formed",y=9.5,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset rise",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="ill-formed",y=9.5,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset fall",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8, "b",
        5,6, "c",
        6,1, "c",
        6,1, "d",
        7,8, "d") %>%
  slopes_plot()
```

The *Minimum Sonority Distance* [MSD; @steriade1982greek; @selkirk1984majorsk] is a well-known elaboration on the preferred angle of sonority slopes compared to basic applications of the SSP, given that the SSP makes no distinction between different angles of rising or falling slopes. The MSD was designed to prefer onset rises with steep slopes over onset rises with shallow slopes, under the assumption that consonantal sequences in the onset are preferred with a larger sonority distance between them. For instance, *plV* has a steeper rise compared to *bnV* and it is therefore better-formed according to the MSD (see Figure \@ref(fig:slopes-pl-bn)).[^cf-sdp] 

(ref:slopes-pl-bn) Schematic depiction of the sonority slopes of two onset clusters, *plV* and *bnV* (the red solid line denotes the sonority slope of the onset clusters). The angle of the red lines reflects a steeper rise for *plV* (left) compared with *bnV* (right), due to the larger sonority distance between the consonants in *plV*.
```{r slopes-pl-bn, fig.cap = "(ref:slopes-pl-bn)", fig.asp = .4, out.width = '100%', dev="cairo_pdf"}
seg_type = c("Vowels","Glides","Liquids","Nasals","Voiced Fricatives","Voiced Stop", "Voiceless Fricatives","Voiceless Stops")
seg_token = c("p","l","V","b","n","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority rises: different slopes",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="steep rise",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="shallow rise",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 1, "a",
        3, 6, "a",
        3, 6, "b",
        4, 8,"b",
        5,3,"c",
        6,5,"c",
        6,5,"d",
        7,8,"d") %>%
  slopes_plot()
```

## Gaps in empirical coverage  {#sec:failures}

The widely-accepted use of sonority slopes in order to explain and predict phonotactic behaviors has been adopted by many researchers with only few changes such as the above-mentioned elaborations on the angle of sonority slopes as captured by the MSD. This is a strong testament to the simplicity and power of the concept of sonority slopes. 
However, systematic gaps in the empirical coverage of traditional sonority principles appear to be inherent to their simplified formal structure.

One rather well-known and well-studied consistent flaw in the empirical coverage of all traditional sonority principles concerns sequences that are often termed */s/-stop clusters*, referring to cases where a sibilant fricative---most often /s/---precedes a stop consonant, like in the English words ***st**op*, ***sk**y*, and ***sp**ort* [see e.g., @kenstowicz1994phonology; @wright2004review; @yavacs2008sonority; @vaux2009append; @olender2013acoustic; @goad2016sonority]. The sonority slope of /s/-stop clusters is either an onset fall or an onset plateau, depending on the given sonority hierarchy (see Figure \@ref(fig:slopes-sp-sp)). Thus, although /s/-stop clusters are relatively common in languages that tolerate sequences and should therefore be considered as relatively well-formed [@morelli2003relative; @steriade1999alternatives], /s/-stop clusters are predicted to be rare, or even extremely rare, by virtue of their ill-formed sonority slopes.
We return to /s/-stop clusters in the general discussion (Subsection \@ref(sec:division)), where we sketch a path towards an effective holistic division of labor between sonority and other phonotactic principles.

(ref:slopes-sp-sp) Schematic depiction of the two potential sonority slopes of the /s/-stop cluster *spV*. The red solid line that denotes the sonority slope of the consonantal clusters is falling when applied with the expanded sonority hierarchy *exp* (left), and it is a plateau when applied with the collapsed sonority hierarchy *col* (right).
```{r slopes-sp-sp, fig.cap = "(ref:slopes-sp-sp)", fig.asp = .4, out.width = '100%', dev="cairo_pdf"}
#seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("s","p","V","s","p","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Obstruents",y=1,x=4.7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL", angle=90) +
  geom_text(data=tibble(seg_token="Fricative-stop clusters: different hierarchies",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="exp (expanded)",y=9.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset fall",y=8.75,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="col (collapsed)",y=9.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_text(data=tibble(seg_token="onset plateau",y=8.75,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  # geom_text(data=tibble(seg_token="exp (expanded) hierarchy",y=9,x=3),
  #           aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  # geom_text(data=tibble(seg_token="col (collapsed) hierarchy",y=9,x=6),
  #           aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL", fontface = "italic") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 2, "a",
        3, 1, "a",
        3, 1, "b",
        4, 8, "b",
        5,2.5, "c",
        6,2.5, "c",
        6,2.5, "d",
        7,8, "d") %>%
  slopes_plot()
```

A second problem that has received much less attention in the literature [but see @baroni2014language] is the general failure of traditional sonority principles to correctly account for different types of sonority plateaus. Sonority plateaus can result from different types of consonants of the same class, regardless of the class type. Thus, a voiceless fricative plateau such as *sfV*, like in the English word ***sph**ere*, should be exactly as ill-/ well-formed as a nasal plateau like *nmV* (see Figure \@ref(fig:slopes-nm-sf)), which is, in fact, a much less common (more *marked*) cluster among the languages of the world [@greenberg1978some; @lindblom1983production; @kreitman2008phoneticssk]. 

Different sonority plateaus have the same flat angle in sonority terms, yet they differ in their apparent distribution and this difference seems to correlate with the different intercepts of the plateaus (whereby intercepts denote the starting point of the slope, reflecting the sonority level of the first item in a cluster): a plateau with a low-sonority intercept like *sfV* is less marked or better-formed, given that it is cross-linguistically more common than plateaus with higher sonority intercepts like *nmV*. 

(ref:slopes-nm-sf) Schematic depiction of the sonority slopes of two sonority plateaus, *nmV* and *sfV*. The red solid line which denotes the sonority slope of the onset clusters is higher for *nmV* (left) than for *sfV* (right) due to the higher intercept (i.e., starting point) of /n/ compared to /s/. This difference is not accounted for by traditional sequencing principles.
```{r slopes-nm-sf, fig.cap = "(ref:slopes-nm-sf)", fig.asp = .4, out.width = '100%', dev="cairo_pdf"}
#seg_type = c("Vowels","Glides","Liquids","Nasals","Vcd. Frics.","Vcd. Stop", "Vcls. Frics.","Vcls. Stops")
seg_token = c("n","m","V","s","f","V") 
slopes_plot <- function(df) {
  ggplot(df, aes(x=x,y=y, linetype=line, color=line)) + 
  #geom_segment(aes(x=0, xend=0, y=-1, yend=8.5), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=0, xend=7.5, y=-1, yend=-1), color="grey", size=.2, alpha=.5, linetype = "solid") +
  geom_segment(aes(x=4.5, xend=4.5, y=-.5, yend=8.5), color="black", size=.2, alpha=.2, linetype = "dotted") +
  geom_text(data=tibble(seg_token=seg_type,y=8:1,x=0.2),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token=seg_token,y=0,x=2:7),
            aes(label=seg_token,x=x,y=y, hjust=0),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="Sonority plateaus: different intercepts",y=10.75,x=4.5),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="high plateau",y=9.25,x=3),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_text(data=tibble(seg_token="low plateau",y=9.25,x=6),
            aes(label=seg_token,x=x,y=y, hjust=0.5),inherit.aes = FALSE, size=4, family = "Charis SIL") +
  geom_line() +
  scale_x_continuous("",breaks=2:7, labels=NULL) + 
  scale_y_continuous("",breaks=1:8, labels=NULL) +
  scale_linetype_manual(values=c("solid","solid","solid","solid")) +
  scale_color_manual(values=c("red","grey","red","grey")) +
  geom_point(color="black", size=1) +
  theme(legend.position = "none", axis.line.y = element_blank(), panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank()) + 
  coord_cartesian(xlim=c(0.2,7.5))
}
tribble(~x , ~y, ~line,
        2, 5, "a",
        3, 5, "a",
        3, 5, "b",
        4, 8,"b",
        5,2,"c",
        6,2,"c",
        6,2,"d",
        7,8,"d") %>%
  slopes_plot()
```

##		Sonority "correlusions"  {#sec:correlusions}

Given that the main evidence for sonority hierarchies comes from attested cross-linguistic patterns, the search for the phonetic basis of sonority is essentially a search for the perceptual or articulatory phenomenon that would consistently correlate with classes on the sonority hierarchy (note that correlations are often observed in the acoustic signal as a proxy for perception or articulation).[^cf-nobasis] 

@parker2002quantifying found a little under 100 different correlates of sonority in the literature, with proposals ranging between articulatory-based perspectives addressing airflow volume or pressure, or degree of jaw opening, and perceptually-based perspectives that often target acoustic intensity, duration or other parameters like the frequency of F~1~ (the lowest formant). 
@parker2002quantifying tested five leading proposals---*intensity*, *intraoral air pressure*, *F~1~ frequency*, *total air flow*, and *duration*---in laboratory conditions. The correlations that Parker obtained were partial for all potential correlates, with intensity interpreted as taking the lead, a conclusion that was repeated and elaborated upon in @parker2008sound.
Parker's conclusions are very much in line with many influential studies in phonetics and phonology that target acoustic intensity (also referred to as *amplitude* or *power*) as the phonetic correlate of sonority [e.g., @sievers1893grundzugesk; @heffner1969generalsk; @ladefoged1975acourse; @clements1990role; @blevins1995syllable; and @gordon2012sonority, to name just a few prominent examples].<!-- [^cf-articultory]  -->

Although commonly accepted as a phonetic correlate of sonority, the general acoustic intensity of the signal does not consistently correlate with any aspect of human auditory perception, not even perceived loudness [see, e.g., @fletcher1933loudness; @suzuki2004equal; @olsen2010loudness; @seshadri2009perceived; @hellman1972asymmetry]. Furthermore, the relevance of perceived loudness to syllabic organization requires some sort of functional explanation, which we are unaware of. Our current proposal for linking sonority with *pitch intelligibility* as its perceptual basis and with *periodic energy* as its acoustic correlate (Section \@ref(sec:dynamic)) is related to the intensity-based perceptual approach. 
However, instead of targeting the intensity of all the acoustic qualities in the signal, we select only the periodic components that promote the sensation of pitch. This move allows a relatively consistent and straight-forward mapping from acoustics to perception, while providing functionally reasonable explanation given a universal drive to optimize pitch transmission in the speech signal.

#		A dynamic model of sonority using periodic energy {#sec:dynamic}

##		Sonority is related to pitch intelligibility {#sec:pitchintelligibility}

The observation that sonority summarizes some essential quality that is related to vowels and their propensity to deliver a relatively steady harmonic structure, highlighting pitch and formant information, is by no means new. Previous proposals already defined sonority as either relating to vowels in some general way, or more specifically relating to voicing or glottal vibrations, or to the clarity/strength of the formants.[^cf-list] A few previous accounts went even further, by addressing the function of this evasive vowel-centric feature, suggesting that sonority may be related to periodic energy or pitch/tone [@lass1988phonology; @nathan1989preliminaries; @puppel1992sonority; @ladefoged1997linguistic; @heselwood1998unusual]. What all these proposals share, explicitly or implicitly, is a recurring insight about a strong link between the preferred type of segmental material in syllabic nuclei and a set of features that conspire to optimize pitch intelligibility, a property characterizing vowels more than consonants. 

Pitch is an inseparable communicative dimension of all linguistic sound systems [@bolinger1978intonation; @cutler1997prosody; @roettger2019tune], whether it is lexically determined as in linguistic *tone*, 
or post-lexically employed to convey intonation, i.e., the linguistic *tune* [see @jun2015prosodicsk for prosodic typology]. 
Tones are used to distinguish words while tunes are used to demarcate units as well as to express non-propositional meanings (e.g., discourse-pragmatic intention, emotional state, socio-indexical identity, and attitudinal stance). The importance of pitch to human communication cannot be overstated.
 
Crucially, linguistic pitch events are known to target syllable-sized units as their "docking site", regardless of the type of pitch event, whether they are linguistic tones or tunes. 
The latter are commonly considered to associate pitch events with Tone-Bearing Units [TBU; see @leben1973suprasegmental], that are syllables or *moras*,[^cf-mora] a hallmark of Autosegmental and Autosegmental-Metrical Phonology [e.g., @liberman1975intonationalsk; @goldsmith1976autosegmental; @pierrehumbert1980phoneticssk; @ladd2008intonational].

The functionally motivated conclusion that emerges with respect to sonority is therefore that syllables require a pitch bearing nucleus and sonority is a scalar measure of the ability to bear pitch. In other words, sonority is, very likely, a measure of *pitch intelligibility*. We hypothesize that pitch-bearing units, which are privileged in perception and essential in communication, were selected in the process of language optimization towards rich usage of linguistic pitch events in communicative manners,
making pitch intelligibility a fundamental requirement for the building blocks of prosody.[^cf-perception]

###		Pitch intelligibility correlates with periodic energy {#sec:periodicenergy}

While it is essentially pitch intelligibility in perception that we assume here to be the phonetic basis of sonority, we would be hard pressed to find simple ways to measure the scalar strength of pitch intelligibility directly as a perceptual construct. Fortunately, a tight and consistent correlation holds between the periodic content of the acoustic signal and the effective sensation of pitch in perception [see, e.g., @pierce2001intro; @de2005pitch; @oxenham2012pitch]. This remains a robust assumption even when considering that the entirety of pitch perception phenomena cannot be reduced to just a few acoustic dimensions [see, e.g., @houtsma1995pitch; @shepard2001pitch; @moore2013anintro 203; @mcpherson2018diversity].

##		The Nucleus Attraction Principle {#sec:nap}

At the heart of all sonority-based principles lies the idea that the most sonorous segment in a sequence is contained within the nucleus of the syllable, assuming a link between the amount of sonority and the nucleus position of the syllable. We adopt this fundamental insight that guides all other sonority principles in the development of the Nucleus Attraction Principle, but instead of adding further formal assumptions about non-overlapping segments with fixed sonority values and corresponding sonority slopes in symbolic time, we simply model the link between sonority and the syllabic nucleus as a dynamic process in real time, whereby all the portions of the speech signal compete against each other for the nuclei. 

Sonority is therefore the quality that is capable of *attracting* the nucleus. The varying quantities of this quality, which temporally fluctuate along the stream of speech, determine which portions of speech are destined to succeed in attracting nuclei given their superior local sonority quantity. The speech portions that fall between those successful attractors are syllabified in the margins of syllables, at onset and coda positions.

Crucially, we model the postulated link between sonority peaks and syllabic nuclei as the result of a perceptual-cognitive process in real time, rather than describing a symbolically geometric state of affairs. 
In fact, by modelling the sonority-nucleus link in dynamic terms we do not need to add further theoretical postulates about sonority slopes, or even the notion of consonants and vowels, to determine ill-/well-formedness of syllabic structures. Syllabic ill-formedness in NAP models is directly related to the degree of nucleus competition that a given syllabified portion incurs. 

Note that we are not interested in verifying the identity of the winner of the nucleus competition, which, for all instances discussed here, is always the only vowel in the structure. In order to predict ill-formedness we are essentially interested in quantifying the amount of competition that the winning vowel had to face given different consonantal clusters in the same syllable.

It should be also useful to note that we do not expect serious competition to arise from a consonant adjacent to the vowel in the same syllable, 
such that in a C~1~C~2~V syllable, only *C*~1~ is considered a competitor with winning *V* (*C*~2~ has a crucial impact on the the potential of *C*~1~ but is not, in and of itself, a competitor in the experimental sets we constructed).
<!-- [^cf-glides] -->
To elucidate this point, consider the case of simple CV syllables, whereby sonority levels are expected to continuously rise from C to V, with no discontinuities between them.
Nucleus competition, much like sonority slopes, 
has a limited impact on syllables with simple onsets and/or codas (e.g., CVC). These principles
play a role chiefly when sequences of consonants are syllabified within a single syllable as complex onset/coda clusters (e.g., **C**CVC**C**). The phonotactics of these possible sequences are determined, to a large extent, by sonority principles. We interpret this aspect of cluster phonotactics such that sequences within syllables are avoided the more they increase the potential competition for the nucleus in the process of syllabifying/parsing the stream of speech.

(ref:nap-depictions) Schematic depictions of competition scenarios with symbolic CCV structures. The purple portions denote the winning vowel in the nucleus position while the blue portions denote the losing portions in the complex onset cluster. The five examples with specified consonantal clusters exhibit their related energy depicted as the area under the curve, i.e., the integral of duration and power, which we refer to as *mass*. The nucleus competition can be understood as the competition between the blue mass and the purple mass. The two examples at the top row---*plV* and *lpV*---suggest replication of successful traditional predictions while the three examples at the bottom row---*spV*, *sfV*, and *nmV*---suggest a divergence from inherent failures of SSP-type models (see text for more details).
```{r nap-depictions, fig.cap = "(ref:nap-depictions)", fig.show='hold', out.width = c('30%', '30%', '30%'), fig.align = 'center'}

knitr::include_graphics(rep(c("extrenal_figures/NAP_CCV_small.jpg", "extrenal_figures/NAP_plV_small.jpg", "extrenal_figures/NAP_lpV_small.jpg","extrenal_figures/NAP_spV_small.jpg", "extrenal_figures/NAP_sfV_small.jpg", "extrenal_figures/NAP_nmV_small.jpg")))

```

Consider for example the pair *plV* and *lpV*, with traditional sonority slope depictions in Figure \@ref(fig:slopes-pl-lp) and schematic NAP-related depictions in Figure \@ref(fig:nap-depictions). A consonantal onset cluster with a putatively well-formed rising sonority slope like *plV* should be also considered well-formed under NAP due to the very low potential of competition between the marginal minimally-sonorous onset consonant /p/ and the non-adjacent vowel that wins the competition for the nucleus, especially given the intervening /l/ that promotes a continuous rise from /p/. Moreover, a consonantal onset cluster with a putatively ill-formed falling sonority slope like *lpV* should be also considered ill-formed under NAP due to the higher potential for competition between the marginal sonorous onset consonant /l/ and the non-adjacent winning vowel, especially given the intervening /p/ that leads to discontinuity in the sonority trajectory.

Unlike the examples above, where the rationale of NAP is expected to replicate successful predictions of the SSP with cases like *plV* and *lpV*, NAP is also expected to diverge from traditional sonority sequencing principles in those cases of inherent failures, as detailed in Subsection \@ref(sec:failures). Thus, under NAP neither /s/-stop clusters (e.g., *spV* in Figures \@ref(fig:slopes-sp-sp) & \@ref(fig:nap-depictions)) nor voiceless obstruent plateaus (e.g., *sfV* in Figures \@ref(fig:slopes-nm-sf) & \@ref(fig:nap-depictions)) are expected to incur a strong competition syllable-internally
due to the low potential for competition between the minimally-sonorous onset consonant /s/ and the non-adjacent vowel that wins the competition, especially given the intervening voiceless obstruents /p,f/ that retain a minimally sonorous trajectory throughout the whole onset. 

At the same time, a stronger competition potential of nasal plateaus (e.g., *nmV*  in Figures \@ref(fig:slopes-nm-sf) & \@ref(fig:nap-depictions)) is expected in NAP, indicating their relative ill-formedness compared to obstruent plateaus. This should be expected given the higher potential for competition between the marginal sonorous onset consonant /n/ and the non-adjacent winning vowel, especially given the intervening nasal that retains a relatively sonorous trajectory throughout the whole onset.

The impressionistic descriptions of NAP, as provided in this section, are implemented within formal models that we describe in detail in Section \@ref(sec:modelimp).

##		Complementary cognitive models {#sec:complementary}

NAP essentially models a bottom-up process, describing the parsing of the stream of speech into syllables as the end of a process that starts in perception.
This is so because NAP is designed to agree with physical rules and cognitive processing relating to our capacity to process pitch in communicative contexts.
However, in order to model the top-down contribution of sonority to the well-formedness of syllables, our NAP model should be also able to operate on abstract symbolic entities, like consonants and vowels. The mechanisms that should be available to symbol-based processes need to qualitatively differ from the dynamic models that are available to bottom-up signal-based processes
and instead be based on the distributional probability of recognized symbols, given their history in the system.

The above description of top-down inferences, which are detached from the functional aspects of the bottom-up route, echo models of the language user as a *statistical learner* [see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing]
and, more specifically, they are very much in line with models of *phonotactic learners* [see, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @jarosz2017inputsk; @mayer2019phonotactic].
<!-- [^cf-phonlearn] -->
<!-- [^cf-phonlearn]: For *statistical learners* in general see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing; @roettger2019evidential. For *phonotactic learners* see, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @futrell2017generative; @jarosz2017inputsk; @mayer2019phonotactic; @mirea2019usingsk. -->

That said, in this paper we do not explore the statistical nature of top-down inferences. Instead, we operationalize the rationale behind NAP with symbolic machinery to present what we refer to as the top-down model of NAP. 
We interpret the results of the discrete operations in the symbolic version of NAP as a good estimation of symbol-based top-down statistical inferences, guided by the rationale of NAP. This choice allows us to present a top-down model with a stronger explanatory value as it uses a similar architecture to that of standard sonority principles, helping to elucidate NAP's core ideas with familiar vocabulary (see Subsection \@ref(sec:naptdmodel)). 

Moreover, it should be noted that since a cognitively-plausible top-down architecture in our proposed framework is based on the distributional patterns of recognizable symbols, these distributions should be "blind" to their various sources, which include a host of universal and idiosyncratic phonotactic pressures. A true top-down statistical learner is thus inherently "contaminated" by all the different sources that contribute to phonotactics in a given system, without a clear distinction between sonority and other factors. Thus, it remains an open question whether top-down inferences that target only sonority-based phonotactics can be modeled in a more direct and principled way than the one we use here.

Processing of speech occurs bottom-up and top-down simultaneously, and there is no reliable way to fully tease the two apart. Our approach takes both routes into account where they stand for separate processes based on either continuous or discrete sources. Suggesting two types of models may be considered redundant by Occam's razor, yet it is very much in line with Pattee's view of cognitive modeling, where models of dynamic processes and models of their related discrete symbols are both necessary for "a complete understanding of the system", and "no one model is logically or mathematically derivable from, or reducible to, the others" [@pattee2012lawssk 18-19].

As two complementary inference routes, the top-down and bottom-up models should not be considered equal. The bottom-up route is the source of learned linguistic distinctions, it is functionally motivated by the rules of physics and the limitations of the sensorimotor system.
In contrast, the top-down route is based on linguistic experience and superficial inferences that reflect the history of the symbols in the system (i.e., the distributional probabilities of recognizable recurring patterns and their extensions by analogy). In other words, top-down inferences reflect functionally motivated behaviors only indirectly, as the outcome of learning the superficial expressions of functionally-motivated (bottom-up) dynamics.


```{r prepare-mat-per, include=FALSE}
# 60 x length of the audio file binned each 10 ms; 60 frequency bins with 10 ms for each column 
dir_mats <- c("data_tables/APPd_txt_matrices/AA/",
              "data_tables/APPd_txt_matrices/HN/")
files_mat <- list.files(path=dir_mats, pattern="*.txt",full.names=TRUE)

#creates a dataframe with 3 columns, the name of the syllable, syl, the time point, t, and the periodic energy, p
per_df <- map_dfr(files_mat, function(f){
#this is like a loop, it takes each file inside the f and does the following:
    # Read the file
    mat <- read_csv(f,col_names = FALSE,
                    col_types = cols(.default = col_double()))
    #Sum of every column of the matrix
   # vector_weights <- c(rep(1,8),rep(1.1,2),rep(1.3,3),rep(1.4,4),rep(1.5,5), rep(1.4,3), rep(1.3,5),rep(1.2,6),rep(1.1,10),rep(1,15))
    #mat <- mat * vector_weights
    per <- colSums(mat)
    #Extract the name of the syllable form the filename: looks for //(syllable).
    filename <- str_match(f,"//(.*?)_(.*?)\\.")
    syl <- filename[,2] 
    speaker <- filename[,3] 
    tibble(syl=syl,t=(0:(length(per)-1))*10,per=per,speaker=speaker)
})
# Periodic energy at every time point
per_df_full <-  per_df %>%  group_by(syl,speaker) %>% 
                mutate(smooth_per = unclass(smooth(per,"3RS3R"))) 
#head(per_df_full)

# ```
# 
# ```{r prepare-seg, include=FALSE}

dir_seg <- c("data_tables/praat_seg/AA/",
             "data_tables/praat_seg/HN/")

files_praat <- list.files(path=dir_seg, pattern="*.txt",full.names=T)
seg_df <- map_dfr(files_praat, function(f){  
#this is like a loop, it takes each file inside the f and does the following:

    # filename <- str_match(f,"/([^/]*?)_(.*?)_(.*?)\\.")
    filename <- str_match(f,"//(.*?)_(.*?)_(.*?)\\.")
    seg <- read_tsv(f, col_types =cols(
                          rowLabel = col_character(),
                          tmin = col_double(),
                          text = col_double(),
                          tmax = col_double()
                      )) %>% select(-rowLabel) %>%
        mutate(syl = filename[,2],
               speaker = filename[,3],
               text = str_extract_all(syl, ".")[[1]],
               position=row_number(),
               t = map2(tmin,tmax, ~ round(seq(.x,.y,.01)*1000))               ) %>%
        tidyr::unnest(cols = c(t)) %>%
        select(-tmin, -tmax)
})

syl_info <- left_join(per_df_full,seg_df,by = c("syl", "t", "speaker"))

## head(syl_info)

## syl       t   per speaker smooth_per text  position
## <chr> <dbl> <dbl> <chr>        <dbl> <chr>    <int>
## 1 cefal     0  0    AA            0    c            1
## 2 cefal    10  2.37 AA            2.37 c            1
## 3 cefal    20  4.97 AA            4.97 c            1
## 4 cefal    30  5.80 AA            4.99 c            1
## 5 cefal    40  4.99 AA            4.99 c            1
## 6 cefal    50  4.06 AA            4.06 c            1
# ```
# 
# ```{r log-transform, include=FALSE}

subset_voiceless_thresh <- filter(syl_info,
                                  #nchar(as.character(syl))>=4,
                                  position==1,
                                  syl %in% c("cfal","cpal","fsal","ftal","sfal","spal"))
per_thresh <- max(subset_voiceless_thresh$per)

syl_info <- group_by(syl_info,syl,speaker) %>%
    mutate(log_per = ifelse(smooth_per<per_thresh, 0,
                            10*log10(smooth_per/per_thresh)))

## print(syl_info,n=100)

# ```
# 
# ```{r, cog}

syl_info <- syl_info %>% group_by(syl, speaker) %>%
    mutate(com_syl = sum(log_per*t)/sum(log_per), # position of CoM of the whole syllable in time
           t_left_syl = ifelse(t <= com_syl,t,0),
           com_onset = sum(log_per*t_left_syl)/sum(log_per*(t_left_syl>0)),
           NAP_bu = -(com_syl - com_onset), #flipped sign
           NAP_bu_rel = -(com_syl - com_onset)/com_syl) %>%
    select(-t_left_syl)%>%
    select(NAP_bu, everything())
# ```
# 
# ```{r, monosyl, include=FALSE}

monosyl_info <- filter(syl_info,
                       nchar(as.character(syl))==4,
                       speaker == "AA")

monosyl_info$text[which(monosyl_info$text=="c")] <- "ʃ"
# monosyl_info$text[which(monosyl_info$text=="c")] <- "U+0283"
# monosyl_info$text[which(monosyl_info$text=="c")] <- "\textesh"


monosyl_info$syl <- as.factor(monosyl_info$syl)
monosyl_info <- mutate(group_by(monosyl_info,syl),
                       ##  loess smoothing
                       smog_per = predict(loess(log_per~t, data=monosyl_info$syl, span=0.19, degree = 1, na.rm=T)))

### change negatives to 0
monosyl_info$smog_per[(monosyl_info$smog_per<0)]=0

monosyl_info <- mutate(group_by(monosyl_info, syl, position),
                       pos_mid = round(mean(t),-1),
                       pos_end = ifelse(position<4, max(t), NA))

monosyl_info <- mutate(group_by(monosyl_info, syl),
                       ylim_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), smog_per, NA),
                       ylim_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), smog_per, NA),
                       x_com_ons = ifelse(t==round(com_onset,-1)&t<lead(t), com_onset, NA),
                       x_com_syl = ifelse(t==round(com_syl,-1)&t<lead(t), com_syl, NA))

```

#   Model implementation {#sec:modelimp}

In order to compare the different proposals, we consider four types of traditional sonority models alongside our two NAP models.
For the traditional models we use the two types of sonority hierarchies that we presented in Subsection \@ref(sec:hierarchies), where the class of obstruents is either *collapsed* (*col*) into a single level or *expanded* (*exp*) to include distinctions between voiced and voiceless obstruents, and between stops and fricatives. 
We apply both hierarchies on each of the two variants of traditional sonority principles, the Sonority Sequencing Principle (SSP) and the Minimum Sonority Distance (MSD). 
The four traditional sonority models we discuss are therefore a combination of a sonority principle (SSP or MSD) and a sonority hierarchy (*col* or *exp*). Accordingly, they bear the notation *SSP~col~* , *SSP~exp~*, *MSD~col~*, and *MSD~exp~*.
In our two NAP models we use periodic energy as the correlate of sonority, and we apply it either continuously with acoustics (bottom-up model), or in a discrete manner using symbols (top-down model). We refer to these two NAP models with the notation *NAP~td~* for the top-down model and *NAP~bu~* for the bottom-up one.

To demonstrate the different sonority models we focus in this paper on complex onset clusters of the general form CCV, where *C* denotes a consonant and *V* denotes a vowel. While traditional sonority models look at the sonority slope of the onset cluster to determine well-formedness of CCV syllables, NAP-based models apply the notion of *competition* to determine well-formedness.
In the following subsections we elaborate on our methods for obtaining well-formedness scores. We start with the ordinal scores obtained from the four traditional sonority models (Subsection \@ref(sec:traditionalmodels)), and the symbolic NAP model, *NAP~td~* (Subsection \@ref(sec:naptdmodel)). We then present the implementation of the continuous model, *NAP~bu~* in Subsection \@ref(sec:napbu).

## Traditional sonority models {#sec:traditionalmodels}

Implementation of traditional sonority principles like the SSP is based on a calculation of the sonority slope with a given sequence of segments. Speech segments in these frameworks have fixed index values on the sonority hierarchy, based on their class membership, as in the *col* and *exp* hierarchies (see Table \@ref(tab:hierarchy)). These sonority index values are usually expressed in terms of integers since they reflect an ordinal scale, and, for this reason, the mathematical operations that these models employ should be restricted to the most basic arithmetic functions of addition and subtraction. Sonority slopes can be therefore obtained straight-forwardly by a subtraction between the corresponding sonority indices of two adjacent consonants. In onset clusters with two consonants (CCV) this can be simply achieved by the formula $C_2 – C_1$, which yields positive results for rising sonority slopes, negative results for falling sonority slopes, or a zero for plateaus. We apply this to the two SSP models, *SSP~col~*  and *SSP~exp~* (see examples in Table \@ref(tab:ordinalscores)).

The exact same formula is also used to obtain scores for the Minimum Sonority Distance models, *MSD~col~* and *MSD~exp~*, which elaborate on the well-formedness of onset rises. 
MSD models differ from the SSP in the interpretation of positive values that reflect rising sonority slopes. While under SSP all positive scores map to a single score (i.e., all rises are well-formed to the same extent), under the MSD higher positive scores are preferred over lower positive scores to reflect the preference for a larger sonority distance (or steeper slope) in a rising onset configuration (see examples in Table \@ref(tab:ordinalscores)). 

## The top-down NAP model {#sec:naptdmodel}

The symbolic version of NAP, which we use to derive predictions for the top-down NAP (*NAP~td~*), shares a similar architecture with common SSP-based models, yet it reflects the novelties of the current proposal, both in terms of the sonority hierarchy it assumes, and in terms of the design of the sonority principle. *NAP~td~* uses a sonority hierarchy that is based on the periodic energy potential of different phoneme classes as the basis of distinct categorical patterning (see following Subsection \@ref(sec:snaphierarchy)). Furthermore, *NAP~td~* models syllabic well-formedness with the notion of nucleus competition in mind rather than the formal notion of sonority slopes, as in traditional SSP-type models (see  Subsection \@ref(sec:snapimplementation)).

### The sonority hierarchy in NAP~td~ {#sec:snaphierarchy}

The symbolic sonority hierarchy in NAP uses the basic ratio between periodic and aperiodic energy to divide all speech sounds into three distinct groups, reflecting the coarse, yet reliable differences in potential periodic energy mass of different speech sounds. To achieve that we rely on the following set of general characteristics: (i) the main source of periodic energy in speech stems from the vocal fold vibration when voicing occurs; (ii) aperiodic energy in speech is mostly the result of the turbulent airflow resulting from articulatory friction (i.e., fricatives), and from articulatory closure in oral stops, which often incurs transient bursts when released [see @rosen1992temporal].

Thus, the ratio between periodic and aperiodic components in speech sounds readily yields the following three distinct groups: (i) voiceless obstruents that consist of mostly aperiodic energy are the least sonorous type of speech sounds; (ii) sonorant consonants and vowels that consist of mostly periodic energy are the most sonorous type of speech sounds, and; (iii) voiced obstruents that consist of both periodic and aperiodic energy belong in the middle of this 3-place scale (see \@ref(ex:napscale))

\begin{exe}
\ex Voiceless Obstruents $<$ Voiced Obstruents $<$ Sonorants  \label{ex:napscale}
\end{exe}

A further distinction in NAP's sonority hierarchy is based on the general presence or absence of articulatory contact.
<!-- [^cf-hierarchynotes] -->
A free and open vocal tract contributes to a potentially stronger and longer vocalic signal that can qualitatively enhance the potential periodic energy mass.
This distinction effectively separates the sonorants into *sonorant vocoids* (glides and vowels) and *sonorant contoids* (nasals and liquids). See Table \@ref(tab:napscale).[^cf-reconciliation]

The complete 4-place hierarchy of *NAP~td~* in Table \@ref(tab:napscale) also reflects a basic typology of nucleus types, which supports the use of this scale as a qualitative measure for nucleus attraction potentials. Sonorant vocoids like glides/vowels can attract the nucleus in all languages we know (a glide is, in-fact, a vowel when syllabified in the nucleus position), while sonorant contoids like nasals or liquids can be syllabic (i.e., attract the nucleus) only in a subset of languages, of which a smaller subset may allow obstruents to attract nuclei [but see @easterday2019highly for some divergent patterns with syllabic obstruents relative to syllabic liquids].

(ref:napscale-caption) (\#tab:napscale) The symbolic sonority hierarchy in *NAP~td~*
(ref:napscale-caption2) Index values reflect the ordinal ranking of categories in the sonority hierarchy. The distinctions between categories in the symbolic NAP hierarchy are based on the characteristic ratio between periodic and aperiodic energy, and on articulatory contact, both taken to reflect the potential of the periodic energy mass, i.e., the potential for nucleus attraction.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:napscale-caption)}
\begin{tabular}{cclcclcclccl}
\toprule
Sonority index & \multicolumn{1}{c}{Segmental classes} & \multicolumn{1}{c}{Periodic:Aperiodic} & \multicolumn{1}{c}{Articulatory contact}\\
\midrule
4 & \textbf{Sonorant Vocoids} & \multicolumn{1}{c}{1:0} & $-$\\
 & (\emph{glides}, \emph{vowels}) &  & \\
3 & \textbf{Sonorant Contoids} & \multicolumn{1}{c}{1:0} & $+$\\
 & (\emph{nasals}, \emph{liquids}) &  & \\
2 & \textbf{Voiced Obstruents} & \multicolumn{1}{c}{1:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
1 & \textbf{Voiceless Obstruents} & \multicolumn{1}{c}{0:1} & $+$\\
 & (\emph{stops}, \emph{fricatives}) &  & \\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:napscale-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

### NAP~td~ implementation {#sec:snapimplementation}

When dealing with C~1~C~2~V syllables under the NAP framework we essentially want to measure the competition potential between *C*~1~ and *V* given *C*~2~. In and of itself, *C*~2~ is not considered a competitor due to its proximity to the vowel, as discussed in Subsection (\@ref(sec:nap)).
The question of competition may be thus expressed by the following questions:
(i) what is the potential periodic energy mass of *C*~1~ (i.e., how sonorous is *C*~1~, or what is the intercept of the cluster, which determines the starting point of the slope);
(ii) how much of the energy in *C*~1~ is potentially lowered, increased or maintained in *C*~2~, before peaking at the vowel (i.e., what is the sonority slope).
Assessing this relationship between *C*~1~ and *V* given *C*~2~ is thus achieved by the combination of two subtraction formulas: 
(i) a calculation of the difference between *C*~1~ and the non-adjacent vowel, to reflect the potential strength of *C*~1~ in terms of the intercept relative to the nucleus, and;
(ii) a calculation of the slope between adjacent *C*~1~ and *C*~2~, just like in the SSP model, to reflect the trend of the energy's trajectory towards the peak. 
This can be summarized with the formula in \@ref(eq:naptdeq), see examples in Table (\@ref(tab:ordinalscores)).[^cf-angle]

\begin{equation}
(V - C_1) + (C_2 - C_1)  \label{eq:naptdeq}
\end{equation}

(ref:ordinalscores-caption) (\#tab:ordinalscores) Ordinal sonority scores
(ref:ordinalscores-caption2) Well-formedness scores with ordinal models. The table demonstrates the predictions we obtain using the two traditional sonority hierarchies, *col* and *exp*, with each of the two traditional sonority principles, SSP and MSD. Numbers in brackets next to "Rise" reflect MSD's ranking of onset rises by distance---higher values indicate better-formed rises. The scores derived from *NAP~td~* on the right column are taken to directly reflect the nucleus competition potential, where higher scores are better-formed.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:ordinalscores-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Traditional sonority principles} & \multicolumn{1}{c}{Symbolic NAP}\\

\multicolumn{1}{l}{Onset} & \multicolumn{2}{c}{\emph{exp} hierarchy} & \multicolumn{2}{c}{\emph{col} hierarchy} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\

\multicolumn{1}{l}{clusters} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{exp}}}} & \multicolumn{1}{c}{$C_2-C_1$} & \multicolumn{1}{c}{\textbf{\emph{SSP(MSD)\textsubscript{col}}}} & \multicolumn{1}{c}{$(V-C_1)+(C_2-C_1)$}\\

\midrule
\multicolumn{1}{l}{\textbf{pl}V} & 6$-$1 $=$ 5 & \multicolumn{1}{c}{\textbf{Rise (5)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{fl}V} & 6$-$2 $=$ 4 & \multicolumn{1}{c}{\textbf{Rise (4)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{sm}V} & 5$-$2 $=$ 3 & \multicolumn{1}{c}{\textbf{Rise (3)}} & 2$-$1 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$1)$+$(3$-$1) $=$ \textbf{5}}\\
\multicolumn{1}{l}{\textbf{vl}V} & 6$-$4 $=$ 2 & \multicolumn{1}{c}{\textbf{Rise (2)}} & 3$-$1 $=$ 2 & \textbf{Rise (2)} & \multicolumn{1}{c}{(4$-$2)$+$(3$-$2) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{ml}V} & 6$-$5 $=$ 1 & \multicolumn{1}{c}{\textbf{Rise (1)}} & 3$-$2 $=$ 1 & \textbf{Rise (1)} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sf}V} & 2$-$2 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{zv}V} & 3$-$3 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$2)$+$(2$-$2) $=$ \textbf{2}}\\
\multicolumn{1}{l}{\textbf{nm}V} & 5$-$5 $=$ 0 & \multicolumn{1}{c}{\textbf{Plateau}} & 2$-$2 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{sp}V} & 1$-$2 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$1 $=$ 0 & \textbf{Plateau} & \multicolumn{1}{c}{(4$-$1)$+$(1$-$1) $=$ \textbf{3}}\\
\multicolumn{1}{l}{\textbf{lm}V} & 5$-$6 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$3 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(3$-$3) $=$ \textbf{1}}\\
\multicolumn{1}{l}{\textbf{mz}V} & 4$-$5 $=$ $-$1 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{lv}V} & 4$-$6 $=$ $-$2 & \multicolumn{1}{c}{\textbf{Fall}} & 2$-$4 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(2$-$3) $=$ \textbf{0}}\\
\multicolumn{1}{l}{\textbf{ms}V} & 2$-$5 $=$ $-$3 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{np}V} & 1$-$5 $=$ $-$4 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$2 $=$ $-$1 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\multicolumn{1}{l}{\textbf{lp}V} & 1$-$6 $=$ $-$5 & \multicolumn{1}{c}{\textbf{Fall}} & 1$-$3 $=$ $-$2 & \textbf{Fall} & \multicolumn{1}{c}{(4$-$3)$+$(1$-$3) $=$ \textbf{$-$1}}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:ordinalscores-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

## The bottom-up NAP model {#sec:napbu}

(ref:com-4examples-1) Smoothed periodic energy curve (black) of the four syllables from the experimental stimuli---*lpal*, *nmal*, *vlal*, and *smal*. Red vertical line denotes the center of periodic mass of the entire syllable (*CoM~syllable~*), blue vertical line denotes the center of periodic mass of the left portion (*CoM~onset~*). Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). The distance between the two CoM landmarks is indicative of the energy displacement away from the syllabic center, reflecting the nucleus competition potential within the syllable.
```{r com-4examples-1, fig.cap = "(ref:com-4examples-1)", fig.width=7, fig.asp=.6, warning=FALSE, dev="cairo_pdf"}
# CoM_ons <-  expression(CoM[ons])# %>% as_label()
monosyl_examp_1 <- filter(monosyl_info,
                          syl %in% c("smal","vlal","nmal","lpal"))
ordered_syl_abs <- unique(monosyl_examp_1[order(monosyl_examp_1$NAP_bu),]$syl)
monosyl_examp_1$syl <- factor(monosyl_examp_1$syl, levels=ordered_syl_abs)
monosyl_examp_1_plot <-
  ggplot(monosyl_examp_1, aes(x=t)) +
  xlim(0,475) + ylim(-3,19) + #ggtitle("") +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-1, yend=13), color="royalblue1", size=1.7, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-1, yend=13),color="red", size=1.7, alpha=.6, linetype = "solid", lineend = "round") +
  # geom_text(aes(x=com_onset, y=15, label=deparse(CoM_ons)), nudge_x = -20, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=com_onset,y=15,label="CoM:onset"), nudge_x = -30, color="royalblue2", alpha=1, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_text(aes(x=x_com_syl,y=15,label="CoM:syllable"), nudge_x = 30, color="red", alpha=.9, size=3, family = "Charis SIL", check_overlap=T, na.rm = T) +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-.75, yend=-.75), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.04, "npc"))) +
  geom_text(aes(x=(x_com_syl+com_onset)/2,y=-2.5,label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) + 
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=19), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  geom_text(aes(x=pos_mid,y=18,label=text), size=6, family = "Charis SIL", check_overlap=T) + 
  #
  facet_wrap(~syl, ncol=2) +
  theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="Charis SIL"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="Charis SIL"), strip.text = element_blank())
print(monosyl_examp_1_plot)
```

There are various ways to calculate an estimation of the nucleus competition potential within syllables based on the periodic energy data of the acoustic signal. The method that we present here has the advantage of not relying on segmental landmarks that 
are not unncontroversial in terms of their psychological reality and the problematic assumptions they entail regarding linearly ordered non-overlapping consonants and vowels (given that the transition between segments does not tend to occur at a single point in time). Furthermore, segmental boundaries are a construct resulting from segmentation and annotation processes, either manual, automatic, or a combination of both. Thus, the segmentation of the continuous speech into non-overlapping consonants and vowels is theoretically and methodologically arguable, it is error-prone, and it requires labor-intensive resources.

We view the periodic energy data in terms of a *mass*, i.e., the area under the periodic energy curve, representing the periodic quality of the signal by integrating quantities from both duration and power; the two inseparable dimensions of all acoustic signals. In that sense, we choose to use a summing approach to represent the quantity of sonority. 
Summing takes measurements from the whole duration of a unit, and, in contrast to averaging, it 
allows the accumulation of the individual measurements, retaining the contribution of the duration component in the final calculation. Summing is therefore essentially different from averaging (as well as from peak extraction) in that it is capable of uncovering the quantitative difference between two periodic sounds that have the exact same amplitude envelope yet differ in duration.

The contribution of duration to sonority was strongly supported in the seminal work of @price1980sonority, whereby the perception of disyllabic English words like *polite* /pəlʌɪt/ was shown to take place when the duration of the sonorant /l/ in the superficially related monosyllabic word *plight* /plʌɪt/ was manipulated, essentially leading to the perception of another syllable when the duration of the sonorant was increased. Importantly, the periodic energy mass, which is the integral of power and duration, is the only measurement of the three alternatives for continuous curve measurement---sum, average or peak---that is capable of accounting for the data in @price1980sonority.

It is therefore of interest to locate the *center of mass* within regions of interest as a measurement that is sensitive to the two axes of the periodic energy mass---duration (x-axis) and power (y-axis). The center of mass can be viewed as the point in time in which the area under the curve is split into two equal parts. The location of the center of mass in time (x-axis) is attracted to the peak of the curve (on the y-axis), where it is expected to be found with a perfectly symmetrical shape. However, the center of mass most often diverges from the peak of rise-fall curves so as to reflect asymmetries in the overall distribution of the mass, either leftward or rightward. The center of mass of the periodic energy curve (henceforth CoM) follows a methodology that was introduced with the Tonal Center of Gravity [@barnes2012tonal] by calculating a weighted average time point that uses a continuous time series as the weighting term. Thus, we use the equation in \@ref(eq:com) to locate the average point in time (*t*), weighted by continuous periodic energy (*per*) at discrete time points (every 10 ms):

\begin{equation}
\frac{\sum_i per_i t_i}{\sum_i per_i}  \label{eq:com}
\end{equation}

The location of the center of periodic energy mass of the entire syllable (henceforth *CoM~syllable~*) guides us to the point in time where the periodic mass of all the competing forces within that syllable are split into two equal parts. Once we obtain this reference point we can repeat this process within the resulting left-side portion, i.e., from the beginning of the syllable up to *CoM~syllable~*, to focus on the onset position (henceforth *CoM~onset~*).
We therefore measure the center of mass twice---first for the entire syllable (resulting in *CoM~syllable~*) and then for the left portion of the first measurement (resulting in *CoM~onset~*). 
The distance between *CoM~syllable~* and *CoM~onset~* is indicative of the amount of displacement of energy away from the center of the syllable, which we interpret as reflective of the degree of nucleus competition (see Figure \@ref(fig:com-4examples-1)).
<!-- [^cf-loess] -->

The center of mass is capable of capturing both components of a two-dimensional mass, considering the non-linear shape of the periodic energy curve. 
The leftward displacement of *CoM~onset~* relative to *CoM~syllable~* is affected by the distance, the amplitude, and the amount of discontinuity between the periodic energy at the onset and the center of mass of the entire syllable.
Any increase in the above results in a larger distance between the two centers of mass as Figure \@ref(fig:com-4examples-1) demonstrates.

#		Experimental assessments {#sec:experiments}

In what follows we present two experiments: (i) an exploratory study with 12 subjects and no repetitions and; (ii) a confirmatory study with 51 subjects and four repetitions
<!-- [^cf-expconf]  -->
[see @nicenboim2018exploratorysk on dividing up experimental endeavors into exploratory and confirmatory studies].
In both cases, informed consent from the participants was obtained before each experimental session. The experiment complied with the June 1964 Declaration of Helsinki (carried out by the World Medical Association and entitled “Ethical Principles for Medical Research Involving Human Subjects”), as last revised.[^cf-helsinki] 

<!-- Note that all the media and original code that was used in this study are available at the following link on *Open Science Framework*: https://tinyurl.com/y3l4avap. -->
<!-- (anonymized link): https://tinyurl.com/y4gvobyp  -->

##		Rationale {#sec:rationale}

To test the different predictions of the six sonority models (2$\times$NAP, 2$\times$SSP, 2$\times$MSD), we designed a perception task that prompts meta-linguistic syllable count judgement with 29 experimental target items. 
Participants were presented with a collection of speech items that were produced with one or two vowels, systematically for each combination of consonants in our set. 
Only the single-vowel productions were considered as targets, and an accurate response to our targets is always the monosyllabic option, yet we use the term "accuracy" to describe participants' responses only in a technical sense, without any assumption about correctness of responses. 
By focusing on the response time of "correct" responses to the target words we in fact measure the time it took participants to decide that a given single-vowel stimulus is monosyllabic. We thus interpret the response times of monosyllabic responses as reflective of the processing cost of assigning one nucleus to a given target stimulus with one vowel.

A NAP-based model assumes that this processing cost is tightly related to the nucleus competition between different portions of a syllable, such that response times will reflect the degree of nucleus competition within syllables (more competition = slower responses = worse-formed sequence). Traditional sonority models interpret the processing cost as related to well-formedness in terms of sonority slopes, such that worse-formed clusters are more likely to be misperceived and take longer to process [e.g., @berent2007we; @berent2008language; @berent2009listeners; @berent2012language; @lennertz2010people; @maionchi2015sonority; @sung2016perceptionsk; @young2017markednesssk]. The SSP derives a 3-level ordinal hierarchy of complex onset well-formedness---onset rise $>$ onset plateau $>$ onset fall---essentially predicting that response times will pattern into three groups, in line with the sonority slope of the onset clusters. MSD models derive a slightly more elaborate ordinal hierarchy, where onset rises with small sonority distance pattern below onset rises with a larger sonority distance. The latter are predicted to evoke the fastest responses in MSD models.

Note that since the bottom-up predictions of NAP are derived via measurements of acoustic signals of particular productions rather than from fixed symbolic predictions, the assumption that all things other than the controlled variable are equal in the experimental stimuli should hold also for a large degree of variation that occurs in natural speech. Thus, if a certain segment in one item is slightly longer, shorter, louder or softer than in other comparable tokens, bottom-up NAP is designed to directly account for this variation, while the other symbol-based ordinal models need to assume that such variation is mostly negligible. This allows us to opt for a slightly more ecologically valid experimental paradigm, by using natural speech recordings that were designed and selected to sound as similar as possible, rather than using synthesized speech.

##		Materials {#sec:materials}

The experimental design is focused on onset consonantal clusters with two members. These CC combinations are composed from a set of consonants with *coronal* and *labial* places of articulation to avoid articulatory effects that may arise from *homorganic* sequences (i.e., adjacent consonants that share the same place of articulation) by exploiting both directions of each combination---coronal-labial (back-to-front) and labial-coronal (front-to-back). The consonantal classes in this experiment include *stops*, *fricatives*, *nasals*, and *liquids* to reflect the main classes in traditional sonority hierarchies, with the exclusion of *glides*.[^cf-glides2] 
See Appendix \@ref(appendix:a) for a list of considerations and criteria that we used in constructing the experimental stimulus set, and see the full stimulus set in Table \@ref(tab:targetlist).

[^cf-glides2]: Although glides (namely /j/ and /w/) were originally included in the exploratory study, we eventually excluded glides from the analysis of the exploratory study, and we completely excluded glides from the ensuing confirmatory study. The main reason being that glides have a complex status, which is not only structurally-determined (a glide in the nucleus is considered a vowel), but it is also theory-dependent: a glide immediately adjacent to a nuclear vowel may be analyzed as a vowel (i.e., syllabified within the nucleus as part of a diphthong) or as a consonant (i.e., syllabified in the onset or coda positions), depending on language and analysis.

(ref:targetlist-caption) (\#tab:targetlist) Experimental stimulus set: Onset cluster types in the experiment
(ref:targetlist-caption2) cor = coronal; lab = labial; * = voicing disagreement between obstruents; ** = no labial liquid; *** = dorsal stop /k/ to avoid homorganic coronal cluster /lt/.
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:targetlist-caption)}
\begin{tabular}{cclcclcclcclcclcclcclcclccl}
\toprule
\multicolumn{1}{r}{\textbf{C1}} & \multicolumn{2}{c}{\textbf{Voiceless}} & \multicolumn{2}{c}{\textbf{Voiced}} & \multicolumn{2}{c}{\textbf{Nasals}} & \multicolumn{2}{c}{\textbf{Liquids}}\\
\multicolumn{1}{l}{\textbf{}} & \multicolumn{2}{c}{\textbf{Fricatives}} & \multicolumn{2}{c}{\textbf{Fricatives}} & & & &\\
\multicolumn{1}{l}{\textbf{C2}} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor} & \multicolumn{1}{c}{cor-lab} & \multicolumn{1}{c}{lab-cor}\\
\midrule
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sp}, \textbf{ʃp}} & \multicolumn{1}{c}{\textbf{ft}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{np}} & \multicolumn{1}{c}{\textbf{mt}} & \multicolumn{1}{c}{\textbf{lp}} & \multicolumn{1}{c}{\textbf{lk}***}\\
\multicolumn{1}{l}{\textbf{Stops}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiceless}} & \multicolumn{1}{c}{\textbf{sf}, \textbf{ʃf}} & \multicolumn{1}{c}{\textbf{fs}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{nf}} & \multicolumn{1}{c}{\textbf{ms}} & \multicolumn{1}{c}{\textbf{lf}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Voiced}} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{*} & \multicolumn{1}{c}{\textbf{zv}} & \multicolumn{1}{c}{\textbf{vz}} & \multicolumn{1}{c}{\textbf{nv}} & \multicolumn{1}{c}{\textbf{mz}} & \multicolumn{1}{c}{\textbf{lv}} & \multicolumn{1}{c}{**}\\
\multicolumn{1}{l}{\textbf{Fricatives}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{}} & & & & & & & &\\
\multicolumn{1}{l}{\textbf{Nasals}} & \multicolumn{1}{c}{\textbf{sm}, \textbf{ʃm}} & \multicolumn{1}{c}{\textbf{fn}} & \multicolumn{1}{c}{\textbf{zm}} & \multicolumn{1}{c}{\textbf{vn}} & \multicolumn{1}{c}{\textbf{nm}} & \multicolumn{1}{c}{\textbf{mn}} & \multicolumn{1}{c}{\textbf{lm}} & \multicolumn{1}{c}{**}\\
& & & & & & & &\\
& & & & & & & &\\
\multicolumn{1}{l}{\textbf{Liquids}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{fl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{vl}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{\textbf{ml}} & \multicolumn{1}{c}{**} & \multicolumn{1}{c}{**}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:targetlist-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

Table \@ref(tab:targetlist) presents 29 CC sequence types that reflect 16 different combinations of classes (16 unique cells in Table \@ref(tab:targetlist), excluding differences in place of articulation), of which 7-8 are considered onset falls, 3-4 are considered onset plateaus (11 total), and 5 are considered onset rises.[^cf-plateaufall] Of the 29 different clusters, only 3 clusters regularly occur in German words, /ʃp, ʃm, fl/, 6 clusters are attested to some degree in German loanwords, /sp, sf, sm, vl, zv, ml/
<!-- ,[^cf-source],  -->
[@van2012sonority],
and one cluster, /ʃf/, may be considered as similar to German licit clusters with a voiced obstruent following a voiceless one (i.e., /ʃv/ and /t͡sv/). 
Thus, the experimental set contains 19 cluster types that are unattested in German words. These unattested CC types appear in 13 of the 16 unique combinations, excluding the three rising sonority clusters with a liquid in *C*~2~, /fl, vl, ml/, that are all attested in German complex onsets to some degree (yet only marginally so in the case of /vl/ and /ml/).

The different CC sequences were embedded within a /CCal/ word-like frame, with the recurring *-al* rime. These /CCal/ tokens were produced with a single vowel, intended to yield monosyllabic items. We also prepared two disyllabic counterparts for each CC type---one with an epenthetic vowel, /CəCal/, and another with a prothetic vowel, /əCCal/ (with or without an initial glottal stop).[^cf-hebschwa] We thus obtained 29 single-vowel target types and 58 associated bi-vocalic filler types.

##		Predictions {#sec:predictions}

The full set of predictions for the 29 experimental targets is presented for all the symbol-based ordinal models (2$\times$SSP, 2$\times$MSD, and *NAP~td~*) in Table \@ref(tab:OrdinalTargetPreds), and for the signal-based continuous model (*NAP~bu~*) in Figure \@ref(fig:com-monosyl). Note that the scores of *NAP~bu~* are presented on a continuous ratio scale, with specific predictions for each token and consequential intervals between scores. The scores in *NAP~bu~* are not a generalization, rather, they are extracted from the specific set of recordings we measured, and they are expected to vary to some extents when measuring different tokens. In contrast, all the other models perform computations on discrete symbolic phonemes and derive scores in an ordinal scale, where the size of interval between scores is unknown.

(ref:OrdinalTargetPreds-caption) (\#tab:OrdinalTargetPreds) Well-formedness scores for the 29 experimental items using the five ordinal models that are based on symbolic phonemes: SSP~col/exp~, MSD~col/exp~, and NAP~td~
(ref:OrdinalTargetPreds-caption2) Higher values predict better-formed onset clusters in an ordinal scale (i.e., magnitude of differences between values cannot be inferred from these models).
\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{(ref:OrdinalTargetPreds-caption)}
\begin{tabular}{cclcclcclcclcclccl}
\toprule
\multicolumn{1}{l}{Onset cluster types} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{SSP\textsubscript{exp}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{col}}}} & \multicolumn{1}{l}{\textbf{\emph{MSD\textsubscript{exp}}}} & \multicolumn{1}{c}{\textbf{\emph{NAP\textsubscript{td}}}}\\
\midrule
\multicolumn{1}{l}{\textbf{fl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{4 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{sm}, \textbf{ʃm}, \textbf{fn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{3 (\emph{rise})} & \multicolumn{1}{c}{5}\\

\multicolumn{1}{l}{\textbf{vl}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{l}{2 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zm}, \textbf{vn}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{ml}} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{l}{1 (\emph{rise})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sf}, \textbf{ʃf}, \textbf{fs}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{zv}, \textbf{vz}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{2}\\

\multicolumn{1}{l}{\textbf{nm}, \textbf{mn}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{sp}, \textbf{ʃp}, \textbf{ft}} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{0 (\emph{plateau})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{3}\\

\multicolumn{1}{l}{\textbf{lm}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{1}\\

\multicolumn{1}{l}{\textbf{mz}, \textbf{nv}, \textbf{lv}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{0}\\

\multicolumn{1}{l}{\textbf{ms}, \textbf{nf}, \textbf{np}, \textbf{mt}, \textbf{lf}, \textbf{lp}, \textbf{lk}} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{l}{-1 (\emph{fall})} & \multicolumn{1}{c}{-1}\\
\bottomrule
\addlinespace
\end{tabular}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} (ref:OrdinalTargetPreds-caption2)}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

(ref:com-monosyl) Well-formedness scores in the continuous *NAP~bu~* model shown in terms of the distance betweeen the center of mass of the entire syllable, *CoM~syllable~* (red vertical lines), and the center of mass of the left portion, *CoM~onset~* (blue vertical lines), whereby the periodic energy curve is shown with a black line. Grey dotted vertical lines and annotated text denote segmental intervals by manual segmentation (for exposition purposes only). Items are oredered by score (better-formed given smaller distance), from left-to-right and from top-to-bottom.

```{r com-monosyl, fig.cap = "(ref:com-monosyl)", fig.width=7, fig.asp=.8, warning=FALSE, dev="cairo_pdf"}
# pdf.options(encoding = 'ISOLatin2')
ordered_syl_abs <- unique(monosyl_info[order(monosyl_info$NAP_bu),]$syl)
monosyl_info$syl <- factor(monosyl_info$syl, levels=ordered_syl_abs)
monosyl_plot_abs <- 
  ggplot(monosyl_info, aes(x=t)) + ylim(-5.5,20) +
  xlab("time (ms)") + ylab("Periodic energy (relative scale)") +
  geom_line(aes(y=smog_per), color="black", alpha=1, size=1) +
  #
  geom_segment(aes(x=x_com_ons, xend=x_com_ons, y=-2, yend=13), color="royalblue1", size=1.5, alpha=.7, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=x_com_syl, y=-2, yend=13),color="red", size=1.5, alpha=.6, linetype = "solid", lineend = "round") +
  geom_segment(aes(x=x_com_syl, xend=com_onset, y=-2, yend=-2), color="black", size=.5, alpha=1, linetype = "solid", arrow = arrow(length = unit(0.07, "npc"))) +
  # geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, check_overlap=T) + 
  geom_text(aes(x=(x_com_syl+com_onset)/2, y=-5, label=paste0(round(NAP_bu)," ms")), size=3, family = "Charis SIL", check_overlap=T) +
  #
  geom_segment(aes(x=pos_end, xend=pos_end, y=0, yend=20), color="grey", size=.5, alpha=.2, linetype = "dotted") +
  # geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, check_overlap=T) +
  geom_text(aes(x=pos_mid,y=17.5,label=text), size=5, family = "Charis SIL", check_overlap=T) + #family = "Charis SIL",
  #
  facet_wrap(~syl, ncol=5) +
  theme(text = element_text(family = "Charis SIL"), panel.background = element_blank(), axis.title = element_text(size = 12), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8), strip.text = element_blank())
  # theme(panel.background = element_blank(), axis.title = element_text(size = 12, family="sans"), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.text.x = element_text(size = 8, family="sans"), strip.text = element_blank())
print(monosyl_plot_abs)

```

```{r, include=FALSE, message = FALSE}
syl_t <- filter(syl_info) %>%
  # Aviad: no idea why we are subsetting by "a"
    filter(text=="a" ) %>%
    group_by(NAP_bu, syl, speaker) %>%
    summarize(tmin = min(t))

other_scores <- read_tsv(file="data_tables/CCal_model_predictions_fix.tsv", 
                           col_types = cols(
                               syl = col_character(),
                               SSP = col_integer(),
                               SSP_obs = col_integer(),
                               MSD = col_integer(),
                               MSD_obs = col_integer(),
                               NAP_td = col_integer()
                           ))

scores_full <- left_join(syl_t, other_scores) %>% ungroup() %>%
    mutate(NAP_bu = ifelse(nchar(syl)<5,NAP_bu,NA_real_))
```

## Data analysis {#sec:datanlysis}

```{r, include=FALSE}

read_list_opensesame <- function(list_files, speaker = "AA", scores_tbl= scores_full){

    GLIDES <- c("wlal","wnal","wzal","wsal","wtal","jmal","jval","jfal","jpal",               "welal","wenal","wezal","wesal","wetal","jemal","jeval","jefal","jepal")

    scores_speaker <- scores_tbl[scores_tbl$speaker == speaker,]

    map_dfr(list_files, ~{
        message(.x)
       suppressMessages (read_csv(.x)) %>%
            filter(practice =="no") %>%
           mutate(subj = str_match(logfile, '-([0-9]*)\\.csv')[,2],
                   RT = response_time /1000) %>%
            select(subj, stimulus,RT,correct) })%>%
        mutate(stimulus = str_replace_all(stimulus, 'ʃ','c')) %>% # ʃ
        filter(!stimulus %in% GLIDES) %>% left_join(scores_speaker,by=c("stimulus"="syl")) %>%
        mutate(corrRT = (RT - tmin/1000) * 1000, #in milliseconds
               type = factor(ifelse(nchar(stimulus)==4,"CCal",
                             ifelse(str_detect(stimulus,"^e.*" ),"eCCal","CeCal")),
                             levels=c("CeCal","eCCal","CCal")),
               response = case_when(RT>=3 ~ 0,
                                    correct ==1 & type == "CCal"  | correct ==0 & type != "CCal" ~ 1,
                                    TRUE ~ 2))
}
```

```{r, include=FALSE}

opensesame_pilot <- "data_tables/exploratry_results"

files_pilot <-
    c(list.files(path=paste0(opensesame_pilot,"/list1"), pattern="*.csv",full.names = TRUE),
      list.files(path=paste0(opensesame_pilot,"/list2"), pattern="*.csv",full.names = TRUE))

data_pilot_all <- read_list_opensesame(files_pilot, speaker="AA")

data_pilot <- data_pilot_all %>%
    filter(corrRT > 100)

N_below_100_pilot <- data_pilot_all %>%
    filter(corrRT < 100)
#0

# Sanity checks
N_trials_pilot <- 58
all(summarize(group_by(data_pilot,subj),N=n()) %>% pull(N)== N_trials_pilot)
summarize(group_by(data_pilot,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_pilot %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracy <- summarize(group_by(data_pilot,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracy %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracy %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)
```

```{r, include=FALSE}
library(brms)
run_brms <- function(data, chains = 4, iter = 3000, warmup=1000){
    data <- data %>% filter(type=="CCal", response ==1) %>%
        mutate_at(c("SSP","SSP_obs","MSD","MSD_obs","NAP_td"), ~ factor(., ordered = TRUE)) %>%
        mutate(sNAP_bu = NAP_bu -.5)
null_priors <- c(prior(normal(6, 2), class = Intercept),
                         prior(normal(.5, .2), class = sigma))
effect_priors <-  c(null_priors, prior(normal(0,1), class = b),
                    prior(normal(0,1), class = sd),
                    prior(lkj(2), class = cor))

    message("NAP_bu...")
    NAP_bu <- brm(corrRT ~ 1 +  sNAP_bu + (NAP_bu|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
               chains =chains, iter =iter, warmup = warmup)

    message("null...")
    null <- brm(corrRT ~ 1 + (1|subj), data=data,
               prior = null_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
    chains =chains, iter =iter, warmup = warmup)

message("SSP...")
    SSP <- brm(corrRT ~ 1 +  mo(SSP) +(mo(SSP)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("SSP_obs...")
    SSP_obs <- brm(corrRT ~ 1 +  mo(SSP_obs) +(mo(SSP_obs)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD...")
    MSD <- brm(corrRT ~ 1 +  mo(MSD) +(mo(MSD)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("MSD_obs...")
    MSD_obs <- brm(corrRT ~ 1 +  mo(MSD_obs) +(mo(MSD_obs)|subj), data=data,
                   prior = effect_priors,
                   family =  lognormal(), 
               control = list(adapt_delta=.9995,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

message("NAP_td...")
    NAP_td <- brm(corrRT ~ 1 +  mo(NAP_td) +(mo(NAP_td)|subj), data=data,
               prior = effect_priors,
               family =  lognormal(), 
               control = list(adapt_delta=.999,max_treedepth =12),
chains =chains, iter =iter, warmup = warmup)

list(data = data, models = list(NAP_bu = NAP_bu, null = null, SSP=SSP, SSP_obs =SSP_obs, MSD = MSD, MSD_obs =MSD_obs, NAP_td = NAP_td))
}
```

```{r, mpilot, include=FALSE}

 if(file.exists("data_tables/RDS/m_pilot.RDS")){
     m_pilot <- readRDS("data_tables/RDS/m_pilot.RDS")
 } else {
     m_pilot <- run_brms(data_pilot)
     saveRDS(m_pilot, file = "data_tables/RDS/m_pilot.RDS")
 }
 if(file.exists("data_tables/RDS/kfold_pilot.RDS")){
     
     kfold_pilot <- readRDS("data_tables/RDS/kfold_pilot.RDS")
 } else {
     kfold_pilot <- map(m_pilot$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_pilot, file = "data_tables/RDS/kfold_pilot.RDS")
 }

## loo::compare(x=kfold_pilot)

```

<!-- real data -->

```{r, include=FALSE}
opensesame_german <- "data_tables/confirmatory_results"

files_german <-
    list.files(path=opensesame_german,pattern="*.csv",full.names = TRUE)

data_german_all <- read_list_opensesame(files_german, speaker="AA")

data_german <- data_german_all %>%
    filter(corrRT > 100)

N_below_100 <- data_german_all %>%
    filter(corrRT < 100)

## Sanity checks
N_trials_german <- 232 
all(summarize(group_by(data_german,subj),N=n()) %>% pull(N)== N_trials_german)
summarize(group_by(data_german,subj,type),N=n()) %>% ungroup %>% distinct(type, N)

data_german %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracy <- summarize(group_by(data_german,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracy %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracy %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracy %>% filter(nchar==4)
bi_acc <- subj_accuracy %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracy %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# 1 bad subject

data_german <- data_german %>% filter(!subj %in% bad_subj)

```

```{r brms-models, include=FALSE}

if(file.exists("data_tables/RDS/m_german.RDS")){
    m_german <- readRDS("data_tables/RDS//m_german.RDS")
 } else {
     m_german <- run_brms(data_german, iter=4000, warmup=2000)
     saveRDS(m_german, file = "data_tables/RDS//m_german.RDS")
 }
# 
```
```{r loo-models, include=FALSE}
if(file.exists("data_tables/RDS/kfold_german.RDS")){
    kfold_german <- readRDS("data_tables/RDS/kfold_german.RDS")
} else {
     kfold_german <- map(m_german$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_german, file = "data_tables/RDS/kfold_german.RDS")
    }
```

```{r weigths-models, include=FALSE}
    
## ## loo::compare(x=loo_pilot)
## loo::compare(x=kfold_german)
## loo::compare(x=kfold_german[-1])
## loo::compare(x=kfold_german[-1][-6])
## loo::compare(x=kfold_german[c("MSD_obs","SSP_obs","null")])


## loo::loo_model_weights(x=loo_german)
## compare(loo_german$NAP_td, loo_german$SSP_obs)
## if(file.exists("data_tables/RDS/weights.RDS")){
##     weights <- readRDS("data_tables/RDS/weights.RDS")
## } else {
    ## weights <-loo_model_weights(,kfold_german$MSD_obs)

        ## xx <- ll_matrix[,c(2,3,4,5,6)]
    ## wxx <- loo::stacking_weights(xx)
    ## names(wxx) <-  colnames(xx)

    ## saveRDS(weights, file = "data_tables/RDS/weights.RDS")
## }
```
```{r loo-proc, include=FALSE}
## w_a <- model_weights(m_german$models$MSD,
##                    m_german$models$MSD_obs,
##                    m_german$models$SSP,
##                   ##  m_german$models$SSP_obs,
##                   ## m_german$models$NAP_bu,
##                   ## m_german$models$NAP_td,
##                   ## m_german$models$null,
##                    weights = "loo")


data_german_s <- m_german$data


## loos <- loo_german %>% map_dfc( ~
##     .x$pointwise[,"elpd_loo"]
##     ) %>% {setNames(.,paste0("elpd_",colnames(.)))}

## data_german_s <-  data_german_s %>% bind_cols(loos)


## data_g_summary <- data_german_s %>%
##     group_by(stimulus, NAP_bu, NAP_td) %>%
##     summarize_at(vars(starts_with("elpd")), mean)


## loo_model_weights(loo_german)

predictions <- m_german$models %>% map(~
                     predict(.x,summary=FALSE) %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_german_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots, eval =TRUE, warning=FALSE, include=FALSE}
data_RT <- data_german_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```

We use a Bayesian data analysis approach implemented in the probabilistic programming language  *Stan* [@Stan2018] using the model wrapper package *brms* [@R-brms_a; @R-brms_b] in *R* [@R-base].^[The complete list of *R* packages that we used is:  `r cite_r("bibs/r-references.bib")`.] An important motivation for using the Bayesian approach is that it    is easy to fit fully hierarchical models with the so-called "maximal random effect structure", which provide the most conservative estimates of uncertainty [@SchielzethForstmeier2009]. In all our models, we use regularizing priors, which we detail below. These priors are minimally informative and have the objective of yielding stable inferences [@chung2013weakly; @gelman2008weakly; @GelmanEtAl2017]. @NicenboimVasishth2016 and @VasishthEtAl2017EDAPS discuss the Bayesian approach in detail in the context of psycholinguistic and phonetic sciences research. We fit the models with four chains and 4000 iterations each, of which 1000 iterations were the burn-in or warm-up phase. In order to assess convergence, we verify that there are no divergent transitions, that all the $\hat{R}$  (the between- to within-chain variances) are close to one,  that the number of effective sample size are at least 10\% of the number of post-warmup samples, and we visually inspect the chains.

For the statistical models, we take into account that the traditional sonority models and the top-down version of NAP (i.e., *SSP~col~* , *SSP~exp~*, *MSD~col~*, *MSD~exp~*, and *NAP~td~*) are ordinal models, while the bottom-up version of NAP (*NAP~bu~*) is a continuous model. The ordinal models predict that certain groups of onset clusters will be better or worse-formed than other group depending on an ordinal score, but they do  not assume that the score will be equidistant with respect to its effect on the response variable, log-transformed response times. For this reason, the discrete scores of these models are assumed to have a monotonic effect on the log-response time in our task, that is, having a monotonically increasing or decreasing relationship with the log-response time, while  the distance between groups are estimated from the data [@burknerModelingMonotonicEffects2018]. 

In contrast,  *NAP~bu~* is modeled with a continuous predictor which is assumed to have a linear relationship with the log-response times. Finally, as baseline, we fitted a "null" model which assumes no relationship between the stimuli and the response times. 

All the models, included a random intercept and slope by subjects (except for the null model that included only a random intercept) and the following weakly regularizing priors: $Normal(6, 2)$ for the intercept, $Normal(0, 1)$ for the slope, $Normal_+(0,1)$ for the variance components, and $lkj(2)$ for the correlation between by-participant adjustments. The ordinal models have as well a Dirichlet prior for the simplex vector that represents the distance between the categories set to one for each of its parameters. 

We evaluate the models in three different ways: (i) estimation, (ii) descriptive adequacy, and (iii) model comparison.

### (i) Estimation {-}
We report mean estimates and 95\% quantile-based Bayesian credible intervals. A 95\% Bayesian credible interval has the following interpretation: it is an interval containing the true value with 95% probability given the data and the model [see, for example, @Jaynes1976; @MoreyEtAl2015]. 

### (ii) Descriptive adequacy {-}
We use posterior predictive checking to examine the descriptive adequacy or "fit" of the models [@shiffrinSurveyModelEvaluation2008]: the observed data should look plausible under the posterior predictive distribution of the models. The posterior predictive distribution of each model is composed of simulated datasets generated based on the posterior distributions of its parameters. Given the posterior of the parameters of the model, the posterior predictive distribution shows how other data may look like. Achieving descriptive adequacy means that the current data could have been predicted with the model. It is important to notice that a good fit, that is, passing a test of descriptive adequacy, is not strong evidence in favor of a model; in contrast, a major failure in descriptive adequacy can be interpreted as strong evidence against a model [@shiffrinSurveyModelEvaluation2008]. Thus, we use posterior predictive checks to assess whether the model behavior is reasonable and in which situations is not [see @gelmanBayesianDataAnalysis2013  for further discussion].

### (iii) Model comparison {-}
For model comparison, we examine the out-of-sample predictive accuracy of the different models using k-fold (k=15) cross validation stratified by subjects.^[Pareto smoothed importance sampling approximation to leave-one-out cross validation  [implemented in the package `loo`; @vehtariPracticalBayesianModel2017; @vehtariParetoSmoothedImportance2015] failed to yield stable estimates.] Cross validation evaluates the different models with respect to their predictive accuracy, that is, how well the models generalize to new data.

## Experiment 1: Exploratory study {#sec:experiment1}

Given the various novelties in our proposal, the methodologies for data collection, data extraction, and model implementation were first tested on a small body of real data that we collected before finalizing our methodologies (namely model implementations in Subsection \@ref(sec:modelimp) and the various procedural details in 
Appendix \@ref(appendix:a)).
<!-- Section \@ref(sec:methods).  -->
We used this exploratory study to test our methodologies and to explore the possibilities for properly estimating nucleus competition in each of the NAP models. 

The exploratory study was administered in two versions, each with half of the fillers and all of the targets in one block, yielding a total of 58 data points per subject (29 fillers + 29 targets, no repetitions).[^cf-pilot] The two different versions were evenly split between participants (each version was presented to six participants).

The results of this exploratory study are presented below with the finalized parameters that we also applied to the confirmatory study that follows (Subsection \@ref(sec:experiment2)). Due to the small sample size and exploratory nature of the study, we present only the estimation values of Experiment 1, without going into the descriptive adequacy of the models, and without analysis of model comparison. 

```{r participants, fig.show="hide"}
library(ggplot2)
pie_theme <- theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.border = element_blank(), panel.grid=element_blank(), axis.ticks = element_blank(), plot.title=element_text(size=14, face="bold"))

## pilot
subjects_pilot <- read.csv("data_tables/subjects/subjects_pilot.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_pilot, aes(x="",y="",fill=subject_age)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_pilot, aes(x=subject_age)) +
  geom_histogram(binwidth = 1)

pilot_mean_age <- round(mean(subjects_pilot$subject_age))
pilot_min_age <- round(min(subjects_pilot$subject_age))
pilot_max_age <- round(max(subjects_pilot$subject_age))
pilot_N <- length(subjects_pilot$subject_nr)
pilot_male <- length(which(subjects_pilot$subject_gender=="male"))

## main
subjects_main <- read.csv("data_tables/subjects/subjects_main.csv") #%>% select(-X) %>% distinct(subject_nr, .keep_all = TRUE)

ggplot(subjects_main, aes(x="",y="",fill=subject_age)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme 

ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) + 
  geom_bar(width = 1,stat = "identity") + 
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_education)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_handedness)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void() #pie_theme

ggplot(subjects_main, aes(x="",y="",fill=subject_german_native)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y", start = 0) + theme_void()
#```
#```{r participants_plot, fig.cap = "(ref:participants)", out.width = c('50%', '50%'), fig.align = 'center', fig.show="hold"}
# fig.cap = "(ref:participants)", dev='png', fig.ext=c('png', 'large.png'), fig.height=c(2, 2), fig.width=c(2, 2), fig.show="hold"

# ggplot(subjects_main, aes(x="",y="",fill=subject_age)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) + pie_theme
# 
# ggplot(subjects_main, aes(x="",y="",fill=subject_gender)) +
#   geom_bar(width = 1,stat = "identity") +
#   coord_polar("y", start = 0) +
#   scale_fill_brewer(palette="Dark2") + pie_theme

```

###		Participants {#sec:participants1}

The exploratory study consisted of 12 subjects (two males and ten females), all native German-speaking students from the Technische Hochschule Köln, who volunteered to participate in the study. The experiment was administered in a quiet room at the institute's facility in Cologne. The mean age of participants in the exploratory study was 25 (21--30).

### Results of the exploratory study {#sec:results1}

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp_pil <- results_txt(m_pilot$models$SSP, "moSSP")
est_MSD_exp_pil <- results_txt(m_pilot$models$MSD, "moMSD")
est_SSP_col_pil <- results_txt(m_pilot$models$SSP_obs, "moSSP_obs")
est_MSD_col_pil <- results_txt(m_pilot$models$MSD_obs, "moMSD_obs")
est_NAP_td_pil <- results_txt(m_pilot$models$NAP_td, "moNAP_td")
est_NAP_bu_pil <- results_txt(m_pilot$models$NAP_bu, "sNAP_bu")

```
<!-- #### Estimation {#sec:estimation1} -->
For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

* For *SSP~col~*: `r est_SSP_col_pil`.
* For *SSP~exp~*: `r est_SSP_exp_pil`.
* For *MSD~col~*: `r est_MSD_col_pil`.
* For *MSD~exp~*: `r est_MSD_exp_pil`.
* For *NAP~td~*: `r est_NAP_td_pil`.
* For *NAP~bu~*: `r est_NAP_bu_pil`.

```{r}
scores <- data_pilot %>% filter(!is.na(NAP_bu)) %>% 
  distinct(stimulus, NAP_bu) %>% 
  arrange(NAP_bu)

lpal_nap <- scores %>% filter(stimulus == "lpal") %>% 
  pull(NAP_bu) %>% round(0)
lkal_nap <- scores %>% filter(stimulus == "lkal") %>% 
  pull(NAP_bu)%>% round(0)
spal_nap <- scores %>% filter(stimulus == "spal") %>% 
  pull(NAP_bu)%>% round(0)

```

Notice that the posterior of the effect of well-formedness, $\hat\beta$, is not comparable across models. For the ordinal models, it represents the distance between two adjacent categories had they been equidistant, or in other words, $\hat\beta$ multiplied by the number of categories minus one represents the increase in log-scale between the first and the last category. This means that it is highly affected by the number of categories. For the continuous bottom-up model, $NAP_{bu}$, $\beta$ represents, the increase in log-scale for one unit in the well-formedness scale. To make it concrete, between, /lpal/ and /lkal/ there are `r lkal_nap -lpal_nap` units (`r lkal_nap` and `r lpal_nap` respectively); and between /lkal/ and /spal/ there are `r spal_nap  -lkal_nap` units (since their NAP scores are `r spal_nap` and `r lkal_nap` respectively). However, for all the models, $\hat\beta$ is negative, indicating that well-formedness is associated with faster responses. See Appendix \@ref(appendix:b) for the complete output of the models.

### Discussion of exploratory study {#sec:discussion1}

The exploratory study was not designed to distinguish between the models, as it had a relatively small number of observations and it was administered while the various parameters of the models were still undergoing development and change.
The exploratory study was used to fine-tune and settle on our techniques and to finalize our models, with emphasis on our two novel NAP-based models.
The final versions of all the models in the exploratory study, which are presented here, did not fail in the estimation results, suggesting that they are all well-suited for the confirmatory task. Importantly, the results of the following confirmatory study (Experiment 2), which is statistically much more robust, remain consistent with those of Experiment 1.

## Experiment 2: Confirmatory study {#sec:experiment2}

Experiment 2 was the main confirmatory study conducted after finalizing our hypotheses and methodologies with the data from Experiment 1. Each experimental block in Experiment 2 consisted of two repetitions of the target words (2 $\times$ 29 $=$ 58) and one trial of each filler word (1 $\times$ 58). The experiment consisted of two blocks with randomized trials, generating altogether four repetitions of the target words (4 $\times$ 29 $=$ 116) and two repetitions of the filler words (2 $\times$ 58 $=$ 116), yielding a total of 232 data points per subject. 

### Participants {#sec:participants2}

Fifty-one native German speakers (29 female, 19 male, and 3 "other") participated in the experiment, of which 48 were monolingual (the 3 bilingual speakers had Polish, Low German, and Hebrew as their heritage language). The vast majority (30 out of 51) were between the ages 19-29. Eight participants were between 30-39, and 11 participants were between 40-59 years old. There were also two younger participants, between 13 and 18 years old. The vast majority (49 of 51 participants) were right-handed. Of the 51 participants, 34 were students at the University of Cologne who took part in the experiment at the sound attenuated booth of the phonetics laboratory. The other 17 participants took part in the experiment at three different locations---all small quiet rooms within private apartments. All subjects were paid five Euros for their participation. 

We excluded the responses from one participant who failed in our participant inclusion criterion requiring accuracy of at least 75% with bi-vocalic fillers. The bi-vocalic fillers of the forms /CəCal/ and /əCCal/ link correct responses to the disyllabic choice ("2"), and we expect relatively few monosyllabic choices ("1") in response to stimuli with two separate vowels. 
Indeed, the overall average accuracy of all the 51 participants, when responding to bi-vocalic filler stimuli, was 96%. The excluded participant achieved a much lower accuracy score for bi-vocalic fillers, nearing chance-level with 65%.

### Results of the confirmatory study {#sec:results2}

```{r}
results_txt <- function(fitbrms, pred) {
    paste0(
        "$\\hat\\beta = ", signif(fixef(fitbrms)[pred, "Estimate"], 2),
        "\\text{, }95\\% \\text{ CrI } = [", signif(fixef(fitbrms)[pred, "Q2.5"], 2), ",", signif(fixef(fitbrms)[pred, "Q97.5"], 2), "]$"
    )
}

est_SSP_exp <- results_txt(m_german$models$SSP, "moSSP")
est_MSD_exp <- results_txt(m_german$models$MSD, "moMSD")
est_SSP_col <- results_txt(m_german$models$SSP_obs, "moSSP_obs")
est_MSD_col <- results_txt(m_german$models$MSD_obs, "moMSD_obs")
est_NAP_td <- results_txt(m_german$models$NAP_td, "moNAP_td")
est_NAP_bu <- results_txt(m_german$models$NAP_bu, "sNAP_bu")

```

### (i) Estimation {-}
<!-- #### Estimation {#sec:estimation2} -->

For all the models, the well-formedness score shows a clear effect on response times, with lower scores yielding longer log-transformed response times:

* For *SSP~col~*: `r est_SSP_col`.
* For *SSP~exp~*: `r est_SSP_exp`.
* For *MSD~col~*: `r est_MSD_col`.
* For *MSD~exp~*: `r est_MSD_exp`.
* For *NAP~td~*: `r est_NAP_td`.
* For *NAP~bu~*: `r est_NAP_bu`.

See section \@ref(sec:results1) for the interpretation of $\hat\beta$, and Appendix \@ref(appendix:b) for the complete output of the models. 

### (ii) Descriptive adequacy {-}
<!-- ####  Descriptive adequacy {#sec:posterior2} -->

The null model is shown in Figure \@ref(fig:NullFit) as a baseline. The slight differences in predictions for different clusters are due to individual differences in the accuracy. Recall that  we subset the response times conditional on the monosyllabic response ('1') to the forced-choice task. This means that when participants gave more monosyllabic answers for a specific  cluster, their adjusted intercept will have a greater influence on the predictions of the model for that cluster. In addition, clusters with fewer monosyllabic responses show more variability in their predictions (e.g., /lf/ vs. /fl/).

The model fit of the six sonority models is shown in Figures \@ref(fig:SSPcolFit)--\@ref(fig:NAPbuFit). The plots in these figures present the dispersion of the average response time results, depicted as red points for related CC clusters, vis-à-vis each models' predictions in the form of distributions, depicted with blue violins. The order of the stimuli, from left to right, follows from the models' scores such that predictions for better-formed clusters appear further to the right. 

(ref:NullFit) Null model fit: Observed mean log-transformed response times are depicted with red points, distribution of simulated means based on the null model are depicted with blue violins. Stimuli ordered from left to right according to their ascending well-formedness score in *NAP~bu~* (here in forced-ordinal scale for exposition purposes).
```{r NullFit,fig.cap = "(ref:NullFit)", warning=FALSE, fig.width=7, fig.asp=.4, dev="cairo_pdf"}
predictions$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL", min.segment.length=.5) + 
    scale_x_discrete("Stimuli", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_blank()) #plot.title = element_text(size=14, family = "Charis SIL"))
    # ggtitle("Null model fit")
```

### (ii.a) Traditional sonority models fit {-}
<!-- #### Traditional sonority models fit {#sec:traditionalModelfit} -->

(ref:SSPcolFit) *SSP~col~* model fit: Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness.
```{r SSPcolFit,fig.cap = "(ref:SSPcolFit)", warning=FALSE, fig.width=8, fig.asp=.4, dev="cairo_pdf"}

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1.2) +
    xlab(bquote(italic(SSP[col])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL"))# +
        # ggtitle(bquote(italic(SSP[col])~fit))
   }
```
(ref:SSPexpFit) *SSP~exp~* model fit (plot details are same as above).
```{r SSPexpFit,fig.cap = "(ref:SSPexpFit)", warning=FALSE, fig.width=8, fig.asp=.4, dev="cairo_pdf"}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(SSP[exp])~scores)) +
       ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(SSP[exp])~fit))
     }
```
(ref:MSDcolFit) *MSD~col~* model fit (plot details are same as above).
```{r MSDcolFit,fig.cap = "(ref:MSDcolFit)", warning=FALSE, fig.width=8, fig.asp=.4, dev="cairo_pdf"}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[col])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[col])~fit))
     }
```
(ref:MSDexpFit) *MSD~exp~* model fit (plot details are same as above).
```{r MSDexpFit,fig.cap = "(ref:MSDexpFit)", warning=FALSE, fig.width=8, fig.asp=.4, dev="cairo_pdf"}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1.2, alpha = .7) +
    xlab(bquote(italic(MSD[exp])~scores)) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
        # ggtitle(bquote(italic(MSD[exp])~fit))
        }
```

We consider a good fit in the case of the ordinal models to be roughly characterized by the following three criteria:
(i) the data is contained within the predictions, i.e., the red points appear within the respective violins;
(ii) the data is consistent within each predicted level, i.e., the vertical dispersion of red points pattern together around the same area within each level (preferably in the middle of the distribution), and;
(iii) the model predictors are not redundant, i.e., the violins of the different model levels show little overlap between them.

A quick observation at the four Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) reveals a common failure of all the traditional sonority models to contain the nasal plateaus (/mn/ and more so /nm/) within their predicted distribution alongside all the other plateaus ('0' model score in all these figures). Furthermore, the data within the plateau level appears to be widely dispersed. 

A closer observation at the two left-most columns in Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit), which reflect the onset fall and onset plateau levels, presents a comparison between the two sonority hierarchies---*col* (*SSP/MSD~col~*) and *exp* (*SSP/MSD~exp~*). The difference between these two sonority hierarchies results in different allocation of the fricative-stop clusters /ʃp, sp, ft/, as plateaus in the *col* hierarchy ('0' in Figures \@ref(fig:SSPcolFit) and \@ref(fig:MSDcolFit)) and as falls in the *exp* hierarchy ('-1' in Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). The latter case of the *exp* hierarchy, whereby the fricative-stop clusters are treated alongside the most ill-formed onset falls, leads to two distinct patterns in the vertical dispersion of data at the left-most column of onset falls (Figures \@ref(fig:SSPexpFit) and \@ref(fig:MSDexpFit)). 
This suggests that the *col* hierarchy (where all obstruents are grouped into one class on the sonority hierarchy such that fricative-stop clusters are considered plateaus) is better than the *exp* hierarchy in treating fricative-stop clusters, as reflected in the better model fits for onset falls and plateaus when the *col* hierarchy is applied (*SSP/MSD~col~* vs. *SSP/MSD~exp~*). However, the two sonority hierarchies also lead to differences in the grouping of onset rises when the MSD models are taken into account, as we observe next.

The right side of these plots, i.e., the columns that reflect well-formed onset rises with positive model scores, present three types of grouping across the four models. The two SSP models (*SSP~col/exp~*) make identical predictions with respect to onset rises, lumping all rises into one category ('1' in Figures \@ref(fig:SSPcolFit)--\@ref(fig:SSPexpFit)). The vertical dispersion of data in the right-most column of the SSP models appears very wide, with voiced-initial clusters like /ml, vn, vl/ appearing to pattern separately from voiceless-initial clusters like /ʃm, sm, fl/.
In contrast, the MSD models present multiple levels of well-formedness for onset rises. *MSD~col~* exhibits two levels of rises ('1--2' in Figure \@ref(fig:MSDcolFit)) while *MSD~exp~* exhibits four levels of rises ('1--4' in Figure \@ref(fig:MSDexpFit)). The grouping in *MSD~col~* appears to result in a relatively high overlap between the distributions of the three right-most columns (including onset plateaus), suggesting redundancy in the model. In comparison, the grouping in *MSD~exp~* seems to capture the distinct patterns of onset rises more accurately, with more levels yet less overlap. Therefore, it appears that the combination of the *exp* hierarchy with the MSD (*MSD~exp~*) has the best fit with respect to onset rises.

To conclude, an observation of the model fit of the four traditional sonority models brings up a mixed picture: the *col* hierarchy (*SSP/MSD~col~*) appears to result in a better fit with onset falls and plateaus, while *MSD~exp~* seems to have the best fit with onset rises. The advantage of *col* with respect to onset falls and plateaus is related to the treatment of voiceless fricative-stop clusters as better-formed (onset plateau rather than of onset fall). The advantage of *MSD~exp~* with respect to onset rises is related to its ability to separate the voiceless-initial from the voiced-initial onset rises, such that voiceless-initial rises are better-formed. These selective and partial advantages, as we detail next, are built-in into the logic of the NAP models.

### (ii.b) NAP~td~ model fit {-}
<!-- #### NAP~td~ model fit {#sec:NAPtdModelfit} -->

(ref:NAPtdFit) *NAP~td~* model fit (plot details are same as above).
```{r NAPtdFit,fig.cap = "(ref:NAPtdFit)", warning=FALSE, fig.width=8, fig.asp=.4, dev="cairo_pdf"}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =5, family="Charis SIL") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", size =1.2)+
  xlab(bquote(italic(NAP[td])~scores)) + ylab("Response time (log scale)") +
      ylim(600,1200) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text = element_text(size = 10, family = "Charis SIL"), axis.ticks.x = element_blank(), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
      # ggtitle(bquote(italic(NAP[td])~fit))
  }

```

Although *NAP~td~* is an ordinal model like all the traditional sonority models, it follows a different rationale (see Subsection \@ref(sec:naptdmodel)), whereby the distinct categories of the model estimate nucleus competition to reflect well-formedness. Figure \@ref(fig:NAPtdFit) shows that *NAP~td~* succeeds in containing all the data (points) within the respective predictions (blue violins). 
A comparison with Figures \@ref(fig:SSPcolFit)--\@ref(fig:MSDexpFit) shows that *NAP~td~* is the only model to achieve this.

However, *NAP~td~* appears to exhibit some redundancy, as suggested by the large degree of overlap between some of the predictive distributions of the model. This is apparent from the overlap between violins in the two right-most columns (3, 5) as well as the three left-most columns (-1, 0, 1) in Figure \@ref(fig:NAPtdFit).

### (ii.c) NAP~bu~ model fit {-}
<!-- #### NAP~bu~ model fit {#sec:NAPbuModelfit} -->

(ref:NAPbuFit) *NAP~bu~* model fit (plot details are same as above).
```{r NAPbuFit,fig.cap = "(ref:NAPbuFit)", warning=FALSE, fig.width=7, fig.asp=.4, dev="cairo_pdf"}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Charis SIL") +
    scale_x_discrete(bquote(italic(NAP[bu])~scores), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_text(size=14, family = "Charis SIL")) #+
    # ggtitle(bquote(italic(NAP[bu])~fit))
```

*NAP~bu~* is different from all the other models in that it presents scores that are specific to each token in a continuous rather than ordinal scale (i.e., the distances between scores in the model are also predicted). See Figure \@ref(fig:NAPbuFit) where the expected negative correlation between response time and well-formedness appears to generally hold for both the predictions and the data of the model fit of *NAP~bu~*.
Our criteria for goodness of fit based on the plots (see opening Subsection 
\@ref(sec:results2)) (iia)
<!-- \@ref(sec:traditionalModelfit))  -->
are not all valid when evaluating *NAP~bu~* since we have no classes and no vertical dispersion of data (points) within levels, and since the horizontal overlap of predictions (violins) between levels requires different interpretation. However, the criterion for inclusion of data within the model's predictions naturally also holds for the *NAP~bu~* fit, which fails to include the data for the nasal plateaus /nm/ and /mn/ within their respective predictive distribution (a failure that is shared by the traditional models; Subsection 
\@ref(sec:results2)) (iia).
<!-- \@ref(sec:traditionalModelfit)).  -->
Furthermore, *NAP~bu~* also fails to include the /z/-initial clusters---/zm/ and /zv/---within their respective predictive distribution.

(ref:SonFitAll) Combined sonority models fit (small reduced versions, see detailed versions above). Observed mean log-transformed response times are depicted with red points; distribution of simulated means based on the model are depicted with blue violins. Stimuli ordered from left to right according to their score in the model in ascending well-formedness. 
```{r SonFitAll,fig.cap = "(ref:SonFitAll)", warning=FALSE, out.width=c("50%","50%","50%","50%","50%","50%"), fig.width=c(3.5,3.5,3.5,3.5,3.2,3.2), fig.asp=c(.5,.5,.5,.5,.5,.5), fig.show="hold", fig.align = "default"}
# fig.ncol=2

predictions$SSP_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP_obs)) %>%
    arrange(SSP_obs) %>%
    mutate(SSP_obs = factor(SSP_obs, levels= unique(SSP_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP_obs), aes(y = corrRT), color = "red3", alpha = .7, size =1) +
    xlab(bquote(italic(SSP[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[col])~fit))}

predictions$MSD_obs %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD_obs)) %>%
    arrange(MSD_obs) %>%
    mutate(MSD_obs = factor(MSD_obs, levels= unique(MSD_obs)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD_obs, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD_obs), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[col]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[col])~fit))}

predictions$SSP %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(SSP)) %>%
    arrange(SSP) %>%
    mutate(SSP = factor(SSP, levels= unique(SSP)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = SSP, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,SSP), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(SSP[exp]))) +
       ylab("Response time (log scale)") +
        ylim(600,1200) +
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(SSP[exp])~fit))}

predictions$MSD %>%
  left_join(data_RT) %>%
    ungroup() %>%
  filter(!is.na(MSD)) %>%
    arrange(MSD) %>%
    mutate(MSD = factor(MSD, levels= unique(MSD)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
   { ggplot(., aes(x = MSD, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    #geom_text_repel(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,MSD), aes(y = corrRT), color = "red3", size =1, alpha = .7) +
    xlab(bquote(italic(MSD[exp]))) + ylab("Response time (log scale)") +
        ylim(600,1200) + 
        coord_trans(y="log") +
        theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
        ggtitle(bquote(italic(MSD[exp])~fit))}

predictions$NAP_td %>% 
  left_join(data_RT) %>%
    ungroup() %>%
    arrange(NAP_td) %>%
    mutate(NAP_td = factor(NAP_td, levels= unique(NAP_td)),
           stimulus = factor(stimulus, levels = unique(stimulus))) %>%
  { ggplot(., aes(x = NAP_td, y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    # geom_text_repel(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT, label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", size =5, family="Times") +
    geom_point(data=distinct(.,corrRT,stimulus,NAP_td), aes(y = corrRT), color = "red3", alpha = .7, size =1)+
  xlab(bquote(italic(NAP[td]))) + ylab("Response time (log scale)") +
      ylim(600,1200) + 
      coord_trans(y="log") +
      theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
      ggtitle(bquote(italic(NAP[td])~fit))}

predictions$NAP_bu %>% 
  mutate(NAP= round(NAP_bu,7)) %>%
    bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1) +
    # geom_text_repel(data= data_RT, aes(y = corrRT, x=factor(NAP),label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, min.segment.length=.5, family="Times") +
    scale_x_discrete(bquote(italic(NAP[bu])), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(size=11, family = "Times"), plot.title = element_blank()) +
    ggtitle(bquote(italic(NAP[bu])~fit))

```

The failures in the fit of the *NAP~bu~* model can be split into two types:
(i) nasal-initial clusters---*nval*, *nmal*, and *mnal*---which received results on a par with the slowest responses in the data, reflecting an overestimation of well-formedness by the model, and;
(ii) syllables beginning with a voiced sibilant---*zval* and *zmal*---which received results that pattern with faster responses, reflecting well-formedness underestimation by the model. 
These results may be taken to suggest that the distinctive high-frequency aperiodic energy of sibilants has a top-down repeller effect on the nucleus, despite additional periodic energy, while nasals, that can attract the nucleus in German (syllabic nasals occur in German), have a top-down attraction effect on the nucleus.[^cf-td]  

### (iii) Model comparison {-}
<!-- #### Model comparison {#sec:ModelComparison2} -->

While the model fits give us an insight into the behavior of each model with respect to the data, they are not well-suited to compare the different models against a consistent criterion. To do that, we run out-of-sample predictions using cross-validation to test each model's ability to predict unseen items.
Results of the model comparisons (see Table \@ref(tab:resultsmodels)) reveal a very clear advantage of *NAP~td~*
over all the other models. 
The difference in elpd scores from the next three models---*SSP/MSD~col~* and *NAP~bu~*---is around -90, which is about 6 times larger in absolute terms than the difference in standard errors of these three models, which is about 15. This is interpreted as a very robust lead for *NAP~td~*. 
The small differences between the next three models (*SSP/MSD~col~* and *NAP~bu~*) make them indistinguishable in second place.
The two traditional models that are based on the *exp* hierarchy---*SSP/MSD~exp~*---are similar to each other at the third place, only marginally better than the null model.

```{r resultsmodels, results = "asis"}

comparison <- loo::loo_compare(x=kfold_german)
ll_matrix <- map2_dfc(kfold_german, names(kfold_german), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_german)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison",note= "The table is ordered by the expected log-predictive density (elpd) score of the models, with a higher score indicating better predictive accuracy. The highest scored model is used as a baseline for the difference in elpd and the difference standard error (SE). The column weight represents the weights of the individual models that maximize the total elpd score of all the models.")
```

The right-most column in Table \@ref(tab:resultsmodels), *weight*, shows model averaging via stacking of predictive distributions. Stacking maximizes the potential elpd score by pulling the predictions of all the different models together. The values under the *weight* column represent the relative contribution of each model to this combined optimal model.
*NAP~td~* alone contributes the lion's share with 65% and *NAP~bu~* comes second with 14%.
All the other traditional models together contribute only 11% to this picture.

Model weights can be informative with respect to complementarity vs. redundancy because the ability to contribute to the maximized score entails some uniqueness. This can explain why the two *exp*-based models, *SSP/MSD~exp~*, did not contribute at all with zero weight. Looking at the model fits in Subsection 
\@ref(sec:results2) (ii), 
<!-- \@ref(sec:posterior2),  -->
it stands to reason that the main apparent advantage of the *exp* sonority hierarchy over the *col* sonority hierarchy in predicting onset rises is already subsumed by *NAP~td~*'s predictions, such that *SSP/MSD~exp~* had no unique contribution to add when stacked alongside superior *SSP/MSD~col~* and *NAP~td~* together.

The fact that the highest ranked NAP model in this comparison, *NAP~td~*, did not completely neutralize the contribution of *NAP~bu~* is telling, especially given that *NAP~td~* subsumed more of the traditional models, although it is more similar to *NAP~bu~* in principle. This is evident from Table \@ref(tab:resultsmodelsohnenapbu), in which we took *NAP~bu~* out of the equation, resulting in 88% contribution by *NAP~td~* and only 3%  contribution by *SSP~col~*.


```{r resultsmodelsohnenapbu, results="asis"}
## NAPtd vs. trad
kfold_ordinal <- kfold_german[!names(kfold_german) %in% c("NAP_bu")]
comparison <- loo::loo_compare(x=kfold_ordinal)
ll_matrix <- map2_dfc(kfold_ordinal, names(kfold_ordinal), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_ordinal)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{td}$ and traditional models comparison",note= NULL)
```

Note that when we take *NAP~td~* out of the equation, as in Table \@ref(tab:resultsmodelsohnenaptd), the relative contribution of *NAP~bu~* rises to 50%, with *SSP~col~* rising to 47%. Again, this suggests that the contribution of *NAP~bu~* remains robust and complementary with respect to all the symbol-based models, including the one that is based on the same principle, *NAP~td~*. 

Lastly, recall that the two NAP models are assumed to be active at the same time, as they were not conceived as competing models, but as two complementary models. When taken together, the combined contribution of the two NAP models in Table \@ref(tab:resultsmodels) covers almost 80% of the maximized elpd score.

```{r resultsmodelsohnenaptd, results="asis", collapse=FALSE}
## NAPbu vs. trad
kfold_classic <- kfold_german[!names(kfold_german) %in% c("NAP_td")]
comparison <- loo::loo_compare(x=kfold_classic)
ll_matrix <- map2_dfc(kfold_classic, names(kfold_classic), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_classic)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{bu}$ and traditional models comparison",note= NULL)
```


### Hebrew study

> NEW

```{r, include=FALSE}
opensesame_hebrew <- "data_tables/confirmatory_results_Heb"

files_hebrew <-
    list.files(path=opensesame_hebrew,pattern="*.csv",full.names = TRUE)

data_hebrew_all <- read_list_opensesame(files_hebrew, speaker="HN")

data_hebrew <- data_hebrew_all %>%
    filter(corrRT > 100)

N_below_100 <- data_hebrew_all %>%
    filter(corrRT < 100)

## Sanity checks

data_hebrew %>% group_by(type,correct) %>% summarize(mean(corrRT))

subj_accuracy <- summarize(group_by(data_hebrew,subj,nchar = nchar(stimulus)),n(),accuracy = mean(correct))
subj_accuracy %>% group_by(nchar) %>% summarize(mean(accuracy))

bad_subj <- subj_accuracy %>% filter(nchar==5) %>% summarize(acc=mean(accuracy)) %>%
    filter(acc < .75)

mono_acc <- subj_accuracy %>% filter(nchar==4)
bi_acc <- subj_accuracy %>% filter(nchar==5)
bi_acc_excluded <- subj_accuracy %>% filter(nchar==5) %>% filter(accuracy > .74)

mean_mono_acc <- mean(mono_acc$accuracy)
mean_bi_acc <- mean(bi_acc$accuracy)
mean_bi_acc_excluded <- mean(bi_acc_excluded$accuracy)

# NO bad subject!

data_hebrew <- data_hebrew %>% filter(!subj %in% bad_subj)
# saveRDS(data_hebrew, "data_hebrew.RDS")
```

```{r brms-models-heb, include=FALSE}

if(file.exists("data_tables/RDS/m_hebrew.RDS")){
    m_hebrew <- readRDS("data_tables/RDS//m_hebrew.RDS")
 } else {
     m_hebrew <- run_brms(data_hebrew, iter=4000, warmup=2000)
     saveRDS(m_hebrew, file = "data_tables/RDS//m_hebrew.RDS")
 }
# 
```
```{r loo-models-heb, include=FALSE}
if(file.exists("data_tables/RDS/kfold_hebrew.RDS")){
    kfold_hebrew <- readRDS("data_tables/RDS/kfold_hebrew.RDS")
} else {
     kfold_hebrew <- map(m_hebrew$models, kfold, folds = "stratified", group = "subj", K = 15)
     saveRDS(kfold_hebrew, file = "data_tables/RDS/kfold_hebrew.RDS")
    }
```



```{r}
estHeb_SSP_exp <- results_txt(m_hebrew$models$SSP, "moSSP")
estHeb_MSD_exp <- results_txt(m_hebrew$models$MSD, "moMSD")
estHeb_SSP_col <- results_txt(m_hebrew$models$SSP_obs, "moSSP_obs")
estHeb_MSD_col <- results_txt(m_hebrew$models$MSD_obs, "moMSD_obs")
estHeb_NAP_td <- results_txt(m_hebrew$models$NAP_td, "moNAP_td")
estHeb_NAP_bu <- results_txt(m_hebrew$models$NAP_bu, "sNAP_bu")
```


* For *SSP~col~*: `r estHeb_SSP_col`.
* For *SSP~exp~*: `r estHeb_SSP_exp`.
* For *MSD~col~*: `r estHeb_MSD_col`.
* For *MSD~exp~*: `r estHeb_MSD_exp`.
* For *NAP~td~*: `r estHeb_NAP_td`.
* For *NAP~bu~*: `r estHeb_NAP_bu`.


```{r resultsmodelshebrew, results = "asis"}

comparison <- loo::loo_compare(x=kfold_hebrew)
ll_matrix <- map2_dfc(kfold_hebrew, names(kfold_hebrew), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_hebrew)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%
    
    select(model, elpd = elpd_kfold, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff)%>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%
 
     apa_table(escape = FALSE, caption ="(\\#tab:modelstacking) All models comparison",note= "The table is ordered by the expected log-predictive density (elpd) score of the models, with a higher score indicating better predictive accuracy. The highest scored model is used as a baseline for the difference in elpd and the difference standard error (SE). The column weight represents the weights of the individual models that maximize the total elpd score of all the models.")
```

```{r resultsmodelsohnenapbu_heb, results="asis"}
## NAPtd vs. trad
kfold_ordinal <- kfold_hebrew[!names(kfold_hebrew) %in% c("NAP_bu")]
comparison <- loo::loo_compare(x=kfold_ordinal)
ll_matrix <- map2_dfc(kfold_ordinal, names(kfold_ordinal), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_ordinal)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{td}$ and traditional models comparison",note= NULL)
```

```{r resultsmodelsohnenaptd_heb, results="asis", collapse=FALSE}
## NAPbu vs. trad
kfold_classic <- kfold_hebrew[!names(kfold_hebrew) %in% c("NAP_td")]
comparison <- loo::loo_compare(x=kfold_classic)
ll_matrix <- map2_dfc(kfold_classic, names(kfold_classic), ~ data.frame( .x$pointwise) %>% set_names(.y)) %>% as.matrix()
weights <- loo::stacking_weights(ll_matrix)
names(weights) <- names(kfold_classic)

comparison%>%
    {bind_cols(model = rownames(.), as_tibble(.))} %>%
    mutate(elpd_kfold = round(elpd_kfold) %>% as.character,
           elpd_diff = as.numeric(elpd_diff), se_diff = as.numeric(se_diff)) %>%

    select(model, `Difference in elpd`=elpd_diff ,`Difference SE` = se_diff) %>%
    left_join(weights %>% tibble::enframe() %>%
              setNames(c("model", "weight")) %>%
              arrange(-weight) %>%
              mutate(weight = round(weight,2) %>% {ifelse(.==0,"$\\approx$ 0", as.character(.))})) %>%
        mutate(model = recode(model, "NAP_bu" = "$NAP_{bu}$", "NAP_td"= "$NAP_{td}$", "SSP_obs" = "$SSP_{col}$", "MSD_obs"= "$MSD_{col}$" , "SSP"= "$SSP_{exp}$", "null" = "Null", "MSD"="$MSD_{exp}$")) %>%

     apa_table(escape = FALSE, caption ="$NAP_{bu}$ and traditional models comparison",note= NULL)
```



```{r loo-proc_heb, include=FALSE}

data_hebrew_s <- m_hebrew$data


predictions_heb <- m_hebrew$models %>% map(~
                     predict(.x,summary=FALSE) %>%
                     array_branch(margin = 1) %>%
                     map_dfr( ~ {
                         data_hebrew_s %>%
                             mutate(pred= .x) %>%
                             group_by(stimulus, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
                             summarize(pred = mean(pred))
                     } )
                 )

```

```{r fitplots_heb, eval =TRUE, warning=FALSE, include=FALSE}

data_RT_heb <- data_hebrew_s %>%
    mutate(NAP = round(NAP_bu,7)) %>%
    group_by(stimulus,NAP, NAP_bu , SSP, SSP_obs , MSD , MSD_obs , NAP_td) %>%
    summarize(corrRT = mean(corrRT))  %>%
    ## summarize(corrRT = mean(log(corrRT * 1000)))  %>%
    bind_rows(tibble(NAP = seq(30,220,10)))  %>%
#    bind_rows(tibble(NAP = seq(0.367355,.4825569,0.01))) %>%
    filter(!is.na(stimulus))
    
```

(ref:NullHebFit) Null model fit: Observed mean log-transformed response times are depicted with red points, distribution of simulated means based on the null model are depicted with blue violins. Stimuli ordered from left to right according to their ascending well-formedness score in *NAP~bu~* (here in forced-ordinal scale for exposition purposes).
```{r NullHebFit,fig.cap = "(ref:NullHebFit)", warning=FALSE, fig.width=7, fig.asp=.4, dev="cairo_pdf"}
predictions_heb$null %>%
  mutate(NAP= round(NAP_bu,7)) %>%
    # bind_rows(tibble(NAP = seq(-220,-30,1)))  %>%
    bind_rows(tibble(pred = seq(30,220,200)))  %>%
    arrange(NAP) %>%
    ggplot(aes(x = factor(NAP), y= pred) ) +
    geom_violin(draw_quantiles = TRUE, alpha=.8, colour = "cornflowerblue", fill = "azure2") +
    geom_point(data= data_RT_heb, aes(y = corrRT, x= factor(NAP)), color = "red3", alpha = .7, size =1.2) +
    geom_text_repel(data= data_RT_heb, aes(y = corrRT, x=factor(NAP), label = {gsub("al", "", stimulus)} %>% {gsub("c", "ʃ", .)}), color = "red3", alpha = .9, size =5, family="Charis SIL", min.segment.length=.5) + 
    scale_x_discrete("Stimuli", breaks=NULL) +
    # scale_x_discrete(bquote(italic(NAP[bu])~scores~(ordinal)), breaks=NULL) +
    ylab("Response time (log scale)") +
    ylim(600,1200) + 
    coord_trans(y="log") +
    theme(panel.background = element_blank(), axis.text.y = element_text(size = 10, family = "Charis SIL"), axis.title = element_text(size=12, family = "Charis SIL"), plot.title = element_blank()) #plot.title = element_text(size=14, family = "Charis SIL"))
    # ggtitle("Null model fit")
```



---

### Discussion of confirmatory study {#sec:discussion2}

The results of the confirmatory study (Experiment 2) can be summarized as follows: 
(i) all of the sonority models we tested are capable of explaining the response time data for different consonant clusters to some extent; 
(ii) the symbolic top-down NAP model, *NAP~td~*, outperforms all the the other models;
(iii) some interesting differences between *col* and *exp* sonority hierarchies were observed, where the advantages of the minimal *col* sonority hierarchy proved to be more effective; 
(iv) the traditional sonority models appear to be largely subsumed by the symbolic *NAP~td~* model; 
(v) the combined contribution of the top-down and bottom-up NAP-based models (*NAP~td/bu~*) appears to be complementary to some degree.

The success of our NAP models relative to the traditional models in predicting the data can be mainly attributed to the following traits of NAP:
(i) all the voiceless-initial onset clusters, including onset falls and plateaus (e.g., /sp/ and /sf/), are relatively well-formed in NAP, correctly predicting the patterning together of such data at the low-right parts of the plots (faster response times);
(ii) onset rises like /ml/, nasal plateaus (/nm/ and /mn/), and onset falls like /lm/ pattern together as similar and relatively ill-formed in NAP, correctly predicting the data, whereby sonorant-initial plateaus and rises do not pattern with (better-formed) obstruent-initial plateaus and rises.

A simplified generalization that may illustrate this state of affairs in symbolic terms would be to claim that the sonority intercept of onset clusters appears to be more impactful than the sonority slope in determining syllabic well-formedness (i.e., the starting level of the cluster is more predictive of well-formedness than the angle of the cluster's slope).

#		General discussion {#sec:genDiscussion}

Our experimental results provide strong support for our synergy of proposals, including our choice of sonority's perceptual basis and acoustic correlate, the incorporation of continuous entities and dynamic procedures in phonological models, and the dual-route modeling strategy that accounts for both top-down and bottom-up inferences with separate compatible machinery. The following subsections discuss two important implications that are borne out of our interpretation of the results: 
(i) in the following subsection (\@ref(sec:dichotomies)) we discuss the complementarity of discrete and continuous modes in cognitive modeling, suggesting that the top-down--bottom-up distinction exhibits a better fit with the discrete--continuous dichotomy than the classic phonetics--phonology dichotomy;
(ii) in Subsection \@ref(sec:division) we discuss a potential phonotactic division of labor, demonstrated with a more holistic account of /s/-stop clusters.

##		Reshuffling dichotomies  {#sec:dichotomies}

This work rejects the classic dichotomy between phonetics and phonology, whereby continuous phenomena are considered phonetic, while phonology is exclusively modeled with discrete terms. This problematic dichotomy implies that perception and articulation are continuous while cognition is discrete, a perspective that, indeed, has characterized many areas of the cognitive sciences from the second half of the twentieth century.
The antithesis of this discrete view of the mind [often dubbed "the computer metaphor of the mind"; see @searle1990cognitive] motivated a shift in cognitive sciences towards more continuous and dynamic models, which has already amassed a compelling body of evidence to support it [e.g., @barlow1972single; @rosen1992temporal; @laks1995connectionistsk; @port1995mind; @case1995evaluation; @kelso1997dynamic; @pouget2003inference; @greenberg2003temporal; @spivey2007continuity; @ghitza2011linking; @giraud2012cortical; @lancia2013interaction; @poeppel2014current].

We assume here that perception and cognition are continuous processes that can activate categorical
representations [see @case1995evaluation; @gafos2006dynamics; @lancia2013interaction; @roessig2019dynamics for derivation of linguistic categories from continuous inputs].
These categorical representations are the learned symbols of the system in abstract terms, readily equated with co-activation of neural populations in the brain in physical terms [see, e.g., @friederici2011brain; @mesgarani2014phonetic; @tang2017intonational].
Once learned and established, symbols can feed top-down inferences that play a role alongside bottom-up inferences in perception [see e.g., @connor2004visual; @pinto2013bottom; @shuai2014temporal].

@tuller1994nonlinear, @gafos2006dynamics, @gafosbenus2006dynamics, and @roessig2019dynamics serve as good examples for the useful application of *attractor landscape* models in order to link linguistic categorical reflexes with continuous variables. They show that the incorporation of continuity in phonology is not only possible but also advantageous for our understanding of the nature of discrete categories, as they tend to be manifested in fuzzy distributions given multiple (redundant) cues and individual differences. 
It is therefore the case that previous work has already convincingly shown that a phonetics--phonology dichotomy does not fit well with a classic continuous--discrete dichotomy, as we have good reasons to incorporate continuous entities alongside discrete units within phonology.

In essence, works like those in @tuller1994nonlinear, @gafos2006dynamics, @gafosbenus2006dynamics and @roessig2019dynamics present a single model that incorporates continuous and symbolic aspects via the unifying mathematics of attractor landspaes. This is a necessary component in a system where continuous and symbolic entities are assumed to interact, modeling the manner in which the two types of entities can relate to each other. However, this cannot be an exhaustive description of a language system as it lacks the ability to explain the states of the system within which interactions occur (e.g., it cannot explain or predict the shape and behaviour of the attrctor landscape or address the limitations on dynamic events that the system can reliably detect).

Our task here is therefore different, yet related and complementary from a holistic system-wide perspective as we are interested in studying the effects of signal-based and symbol-based processes outside of their interaction.
In this paper we model the effects of processes that respond to signal-based continuous stimuli as bottom-up processes, and we separately model the effects of processes that are initiated by symbol-based discrete units, which we consider as top-down. 
Our work claims that the continuous--discrete dichotomy in phonology should be linked to the bottom-up--top-down dichotomy, such that both of these two distinct processes can *coexist* in a single language system.
It is therefore important to highlight the difference between them.
Bottom-up routes in perception are based on continuous stimuli and they are functional, in the sense that they adhere to the laws of physics and to the limitations of the sensorimotor system of the agents. 
Bottom-up processes that seem to systematically characteraize language processing may be taken to imply an evolutionary benefit that they embody with respect to reliable communication.
In contrast, top-down inferences in perception are based on the history of symbolic representations that speakers can learn from experience. 
This learning ability has its own universal functional limitations (e.g., memory-related capacities) but the learned links between the dynamic and symbolic modes are largely arbitrary as they rely on the superficial history presented by a given language system.
These symbols and their probabilistic distributions are constantly updated, reflecting knowledge about the distribution of categorically analyzable units of speech.

In this study we modeled the notion of sonority and its contribution to linguistic sound systems with the assumption that the two different routes---bottom-up and top-down---are both active when speech inferences take place. Our bottom-up model uses continuous data (periodic energy), dynamic principles (competition), and functional motivation (syllables carry pitch information) to model sonority. Our top-down model is based on generalizations over the discrete segmental units in the system and their distribution given bottom-up sonority restrictions (note that our top-down model is not a true statistical learner for reasons that we explain in Subsection \@ref(sec:complementary)). 
The results of model stacking based on our experimental data (see Table \@ref(tab:modelstacking)) support this move, showing that the combined contribution of the two NAP models is relatively complementary (65% + 14%), and, when taken together, they make 79% of the combined contribution of all the six sonority models to a maximized elpd score, reflecting the combined ability of the models to predict unseen forms (see Subsection 
\@ref(sec:results2)) (iii).
<!-- \@ref(sec:ModelComparison2)). -->

##		Phonotactic division of labor: A holistic account of /s/-stop clusters {#sec:division}

Our NAP account of /s/-stop clusters does not suffice to explain this phonotactic phenomenon since there is nothing in NAP that is specific to sibilants or stops that would make the combination of a sibilant and a stop consonant stand out from other comparable obstruent cluster combinations.[^cf-sstop] 
In fact, all the voiceless elements are practically invisible to NAP as it is only sensitive to portions of the speech signal that contain sufficient periodic energy. Indeed, the predictions of NAP, which were corroborated in the experiment, expect non-sibilant counterparts of /s/, like /f/ in the cluster *ftV*, to pattern with *spV* and *ʃpV*. Furthermore, NAP successfully predicts that all the voiceless-initial clusters in the experiment (including the /s/-stop clusters) generally pattern together as well-formed from NAP's point of view. This may suffice to explain why /s/-stop clusters are tolerated, but not why they are so often preferred over other obstruent combinations. The complete phonotactic story of /s/-stop clusters thus requires an integrative explanation, in which sonority only plays a limited role. 

One complementary explanation for /s/-stop clusters can be found in @wright2004review, where the notion of "cue robustness" serves to explain why sibilants, with their salient and distinctive high frequency aperiodic energy, can stand out more than other fricatives and allow effective recoverability from relatively weak marginal positions (i.e., distant from the vocalic nucleus). Although this explanation is also based on perception, cue robustness does not require the notion of the syllable and can be based on simpler adjacency relations. 

Furthermore, obstruent combinations such as a stop-stop clusters (e.g., *tpV*, *ptV*) entail different articulatory gestural coordination patterns than those entailed by fricative-stop clusters (e.g., *spV*, *ftV*). The former, stop-stop clusters, are expected to be dispreferred due to articulatory similarity effects in the production of two consecutive stops, echoing the formal OCP constraint in many traditional phonological accounts [@leben1973suprasegmental; @mccarthy1979formalsk], as a dissimilatory requirement banning two consecutive units of the same type.

These three phonotactic perspectives are complementary, and although they do not represent an exhaustive list of phonotactic pressures, we need at least these three---*sonority*, *cue robustness*, and *articulatory dissimilation*---in order to properly appreciate the phonotactic phenomenon of /s/-stop clusters.

# Conclusions  {#sec:conclusions}

This project suggests a synergy of novel theoretical and methodological approaches in an attempt to shed new light on old problems in linguistics. Naturally, the paradigm shift that we propose here for models of sonority, and for phonological models in general, will need to accumulate more supporting evidence from multiple sources in order for it to be widely considered and consequently developed further. 
In this paper we therefore lay the foundation for such potential long-term contribution. We demonstrated how our set of proposals results in a model of sonority that can account for some of the most persistent problems in phonological theory. 
Importantly, our NAP-based models not only present clear advantages over traditional sonority models in terms of empirical coverage, they also provide functional explanations as to the source and cause of sonority phenomena, linking sonority to pitch intelligibility at the level of the syllable. 
Furthermore, our dual-route strategy to modeling (see Subsection \@ref(sec:dichotomies)) makes the important distinction between (bottom-up) signal-based inferences and (top-down) symbol-based inferences, thus appropriately predicting the complementary contribution of these two essentially different inference routes of the same linguistic phenomena.

Lastly, this study also provides compelling motivations and strong evidence that support our proposal to link the notion of sonority with periodic energy in the acoustic signal. This proposal entails that the role of periodic energy is of great importance to linguistic analysis of speech, beyond the scope of the current proposal---a partial list of relevant topics includes:
(i) acoustic descriptions of prosodic *weight* and prosodic *prominence* in terms of periodic energy mass;
(ii) automatic syllabification procedures based on the smoothed fluctuation rates of the periodic energy curve [useful for a myriad of tasks, including speech rhythmicity studies; see, e.g., @galves2002sonoritysk; @tilsen2013speech; @rasanen2018pre; @lin2020hit], and;
(iii) improved visualization and analysis of pitch contours in intonation research based on the interaction of the periodic energy and the F0 trajectories 
(see Anonymous 2018; Anonymous 2019; and Anonymous 2020 for continuous visualization and quantification procedures in intonational phonology using periodic energy).
<!-- [see @albert2018using; @cangemi2019modellingsk; and @albert2020propersk for continuous visualization and quantification procedures in intonational phonology using periodic energy]. -->

<!-- # Methods {#sec:methods} -->

<!-- ## Stimulus list considerations {#sec:considerationlist} -->

<!-- The following list summarizes concerns that were taken into consideration when constructing the stimulus set (see the full set in Table \@ref(tab:targetlist)): -->

<!-- *	For the class of liquids we consider only the lateral /l/, disregarding the sub-class of rhotics that are phonetically varied and inconsistent across different languages. We therefore present no liquid plateau in the experimental set. -->

<!-- *	We use the alveolar /s/ for the class of voiceless sibilants (voiceless coronal fricatives). In *C*~1~ positions we also use the post-alveolar /ʃ/ to monitor language-specific effects that may appear due to restrictions in German, in which /ʃC/ onset clusters can be licit, while /sC/ onset clusters occur only marginally in loanwords. -->
<!-- <!-- should not be allowed. --> 

<!-- *	We use stops only in *C*~2~ position. We avoid stops in *C*~1~ position since it is also the phrase-initial position of the stimuli, which is practically devoid of acoustic cues for the closure phase of the stop. Furthermore, we use only voiceless stops in order to keep the amount of stimuli reasonably low. -->

<!-- * We use one instance of the dorsal consonant /k/ instead of the coronal /t/ as an alternative to a labial-coronal cluster with a liquid in *C*~1~, thus retaining the same direction of a labial-coronal cluster (i.e., both labial-coronal and coronal-dorsal are front-to-back in terms of their places of articulation). We do not use other dorsals for fricative, nasal or liquid consonants, as these tend to be relatively more marked and more inconsistent between languages. -->

<!-- *	Lastly, we avoid sequences of obstruents that differ in voicing due to the cross-linguistic tendency of obstruent clusters to agree in voicing [see, e.g., @cho1990typology], although note that German allows /ʃv/ and /t͡sv/ clusters while banning /ʃf/. -->

<!-- ## Audio recordings {#sec:audio} -->

<!-- Audio stimuli for the experiment were recorded by a phonetically trained native Hebrew speaker (the first author) in a sound attenuated booth at the phonetics laboratory of the University of Cologne. Speech was recorded via head-mounted headset condenser microphone (AKG C420), capturing mono digital files at a resolution of 44.1kHz sample-rate and 24 bit depth with a Metric Halo MIO 2882 audio interface. Selected audio takes were treated in the original high resolution for DC offset correction and compression of ultra low frequencies under 52Hz (to compensate for some room reverberation effects). Audio was then downgraded from 24 to 16 bit depth with Goodhertz Good Dither dithering to be used in the perception task running on OpenSesame 3.1.9 [@mathot2012opensesame]. The audio that was submitted to analyses by the APP Detector (see Subsection \@ref(sec:obtaining)) was also downgraded in sample-rate to 16kHz. Finally all audio takes, at all resolutions, were normalized to the same RMS target of -20dBFS. -->

<!-- \begin{exe} -->
<!-- \ex \textbf{/ze maʁˈgiʃ \emph{CCal} kaˈʁe.ga/} (ʼit feels (like) \emph{CCal} at the momentʼ) \label{ex:sentence} -->
<!-- \end{exe} -->

<!-- To record the stimuli, the combined 87 word-like tokens were embedded within carrier sentences, appearing in a non-final position with -->
<!-- default declarative intonation -->
<!-- in order to maintain consistent prosody. Carrier sentences were also designed to minimize potential effects of resyllabification as well as co-articulation by controlling the segmental makeup immediately preceding and following target words, see example in (\@ref(ex:sentence)) -->
<!-- (the original sentence elicitation lists are available at the OSF repository in mixed Hebrew and phonemic transcripts in the form of slide presentations, see link in the opening of Section \@ref(sec:experiments)). -->

<!-- ## Obtaining periodic energy data {#sec:obtaining} -->

<!-- We extract continuous measurements of periodic energy from acoustic signals using the *APP Detector*, a computer code that was introduced in @deshmukh2003detectionsk and developed in subsequent publications [@deshmukh2005use; @vishnubhotla2007detection], with the ability to measure the spectral distribution of periodic energy from digital audio files with a 16kHz sample-rate, effectively measuring periodic energy up to 8kHz [consider that our pitch perception range is limited to periods up to approximately 4kHz, reflecting a ceiling effect of the phase-locking of neural firing rates; @wever1937perception; @attneave1971pitch; @pressnitzer2001lower; @moore2013anintro 46]. We export the periodic energy data from the APP Detector's analysis into R [@R-base], for further data manipulation, visualization, modelling, and statistics. -->

<!-- We sum over the different frequencies that the APP Detector measures to obtain a time series  with the sum of periodic energy over different frequencies at 10 ms intervals. -->
<!-- We fit a smoothed curve to the periodic energy time series, to eliminate small-scale fluctuations in the curve's trajectory, and, finally, the sum of periodic energy is log transformed, where it is divided by a value that reflects the threshold of effective voicing periodicity, thus setting a meaningful zero for the periodic energy floor (we apply Tukey's Running Median Smoothing "3RS3R" to the periodic energy data. The threshold of the log transform is calculated as the maximal value obtained for voiceless portions in the set). -->
<!-- <!-- [^cf-smog] --> 

<!-- The periodic energy curve is fitted with Local Polynomial Regression Fitting (*loess*) in the figures. This is only used for visual clarity---all the data points and calculations are based on the periodic energy curve before this smoothing and after initial smoothing and log transform (as detailed above). -->

<!-- ## Perception task details {#sec:ptask} -->

<!-- Recall that the experiments were designed as a forced-choice 2-alternative perception task, where we collected accuracy and response time information. -->
<!-- We determined the zero time for valid response times at the the mid point of transition from *C*~2~ to /a/, individually for each one of the 87 stimuli (given that all stimuli share the rime *al*, which is fully predictable in the context of the experiment, in contrast to the preceding material, which is unpredictable). We used manual segmentation to determine this point for each target. We then excluded response times shorter than 100ms (i.e., 100ms after the mid point of transition from *C*~2~ to /a/) as too fast to be considered as valid. This led to no exclusions from Experiment 1 and to only one observation being excluded from Experiment 2. -->

<!-- Participants were seated in a quiet room in front of a laptop computer (a MacBook Air 13-inch, Early 2014) running the experiment on OpenSesame 3.1.9 [@mathot2012opensesame], where they listened to the stimuli through a set of closed headphones (Sennhieser HD 201), fed directly from the laptop's internal audio interface. After verifying that participants share a standard understanding of the notion of the syllable with a few German examples of words with one and two syllables (*See*, *Spaß*, *Quark*, and *Angst* vs. *Schu-le*, *Kin-der*, *Bre-zel*, *Pflau-men*), they were instructed to listen to nonce words in an "unknown" foreign language (the use of a speaker with a Hebrew accent in the stimuli that were presented to German listeners was intended to further support a "foreign" type of perception). Participants were instructed to respond quickly and accurately on whether they hear one or two syllables by clicking '1' or '2' on the computer keyboard (participants used their left index finger to choose '1' and their right index finger to choose '2').<!-- [^cf-indexfinger]  --> 
<!-- A training session of ten trials preceded the experimental trials, allowing the participants to familiarize with the task, and allowing the experimenter to adjust listening volume and monitor potential problems and misunderstandings regarding the task. -->


<!-- # CRediT authorship contribution statement {-} -->
<!-- **AA**: Conceptualization, Methodology, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision. **BN**: Conceptualization, Methodology, Formal Analysis, Writing - Review & Editing, Visualization.  -->

<!-- # Acknowledgements {-} -->
<!-- This work was supported by the DAAD (German Academic Exchange Service) and SFB 1252 Collaborative Research Center “Prominence in Language". -->

<!-- # Note {-} -->
<!-- All materials associated with the current article are available on *Open Science Framework* (OSF) at -->
<!-- https://tinyurl.com/5jxmjntx -->

<!-- footnotes -->

<!-- [^cf-strength]: Note that a related notion of *strength hierarchies* makes similar distinctions, yet in the opposite direction (stronger = less sonorant). Strength hierarchies are mostly evoked in relation with lenition processes rather than syllabic phenomena.  -->

[^cf-sanskrit]: @Donegan1978onthenatural notes that Pāṇini and the Sanskrit grammarians used the term svara to imply some kind of harmonic musical quality which applies mainly to vowels. @parker2002quantifying [p. 58] notes further that the Sanskrit grammarians observed natural classes for speech sounds that are "grouped according to their degree of 'opening' (vivāra)".

[^cf-liquid]: The group of *liquids* is the most loosely defined, as it includes both *lateral approximants* (namely /l/) and various types of rhotics such as *trills* (/r,ʀ,ʁ/), *taps* (namely /ɾ/), and alveolar and retroflex *approximants* (/ɹ,ɻ/). In this paper we only consider the lateral approximant /l/ as the relatively more stable and common token of this varied liquid type.

<!-- [^cf-vowels]: The class of vowels is often also divided into (in rising sonority order) high, mid, and low vowels [sometimes also with distinctions between central and peripheral vowels; see @gordon2012sonority], but these distinctions will be irrelevant in the context of this paper. -->

<!-- [^cf-sequencingclassics]: Prominent examples include @zwicky1972notesk; @hankamer1974sonority; @hooper1976introduction; @kiparsky1979metrical; @lowenstamm1981maximal; @steriade1982greek; @cairns1982markedness; @selkirk1984majorsk; @harris1983syllable; @mohanan1986theory, and @clements1990role. -->

[^cf-sdp]: The *Sonority Dispersion Principle* [SDP; @clements1990role; @clements1992sonority] is a slightly different yet related principle that prefers onset rises with large distance and equal dispersion of sonority index values across the consonantal sequence and the following vowel. The results of the SDP are highly contingent on the given sonority hierarchy and it is not very clear how to apply the SDP with onset sonority falls [among other problems listed in @parker2002quantifying 22--24]. 

[^cf-nobasis]: There are also perspectives, such as @foley1977foundations, @rice1992onderiving, and @harris2006phonology [see @blaho2008syntax for an overview], where an abstract definition of sonority is not grounded in the nature of the speech signal, and is therefore not expected/required to have a phonetic basis.

[^cf-list]: A partial list of some prominent examples includes @sigurd1955rank; @jakobson1956fundamentals; @chomsky1968spesk; @foley1972rule; @ladefoged1971preliminaries; @allen1973accentsk; @fujimura1975syllable; @Donegan1978onthenatural; @ultan1978typological; @price1980sonority; @lindblom1983production; @anderson1986suprasegmental; @vennemann1988preferencesk; @levitt1991syllable; @pierrehumbert1992lenition; @fujimura1997acoustic; @stemberger1997handbook; @boersma1998functional; @kingston1998class; @zhang2001effects; @howe2004harmonic; @clements2009does; @sharma2018significance

[^cf-mora]: Moras are used to represent quantitative differences between light and heavy syllables (weight sensitivity), such that light syllables contain one mora while heavier syllables contain two (and sometimes even three) moras [see @hyman1984atheory; @mccarthy1990footsk; @hayes1989compensatory; @ito1989prosodic; @zec1995sonority; @zec2003prosodic].

[^cf-perception]: This view of sonority is explicitly perceptual. However, it does not contradict and it should be considered as complementary to syllabic descriptions from an articulatory point of view, such as timing coordination and phase relations that characterize syllabic organization in Articulatory Phonology (@browman1992articulatory; @macneilage1998frame; @goldstein2007syllablesk; @hermes2013phonologysk; @gafos2014stochastic).

<!-- [^cf-statlearn]: See, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing; @roettger2019evidential. -->
<!-- [^cf-phonlearn]: For *statistical learners* in general see, e.g., @christiansen1999power; @frisch2001psychologicalsk; @tremblay2013processing; @roettger2019evidential. For *phonotactic learners* see, e.g., @coleman1997stochastic; @vitevitch2004webbasedsk; @bailey2001determinants; @hayes2008maximum; @hayes2011interpreting; @albright2009feature; @daland2011explaining; @futrell2017generative; @jarosz2017inputsk; @mayer2019phonotactic; @mirea2019usingsk. -->

<!-- [^cf-stress]: It is important to note that weight sensitivity is not a universal process, as stress assignment patterns vary from language to language, and not all languages even have stress to begin with. However, it is one of the naturally occurring stress assignment patterns that many unrelated languages exhibit, e.g., Arabic, Tibetan (Lhasa), Wolof, Finnish, and Latin [see @goedemans2013weight; and @gordon2006syllableweight 23 for more exhaustive lists]. -->

<!-- [^cf-glides]: We expect only vocoids to be able to compete for the nucleus from this vowel-adjacent *C*~2~ position. Note that a vocoid in V position is considered a vowel while in C position it would be commonly considered a glide. To avoid such circular definitions, we completely excluded glides from this study. -->

<!-- [^cf-10]: There is, in fact, a more intricate text-tune interaction that can also lead to local "heaviness" of stressed syllables (i.e., on-line prosodic enhancements of nuclei) in order to accommodate certain tonal events in post-lexical intonation via extended duration and/or increased acoustic energy [@roettger2019tune]. -->

<!-- [^cf-smog]: We apply Tukey's (Running Median) Smoothing "3RS3R" to the periodic energy data. The threshold of the log transform is calculated as the maximal value obtained for voiceless portions in the set, less than 5% of the scale before transformation. -->

<!-- [^cf-loess]: The periodic energy curve is fitted with Local Polynomial Regression Fitting (*loess*) in the figures. This is only used for visual clarity---all the data points and calculations are based on the periodic energy curve before this smoothing and after initial smoothing and log transform (see details in Subsection \@ref(sec:obtaining)). -->

<!-- [^cf-expconf]: See @nicenboim2018exploratorysk on dividing up experimental endeavors into exploratory and confirmatory studies. -->

<!-- [^cf-hierarchynotes]: Note that the addition of the articulatory contact distinction to the sonority hierarchy in *NAP~td~* does not have any implications on the following predictions of the model. It is also worth noting that some rhotics, which are traditionally considered liquids, may in fact belong with the vocoid consonants (e.g., most of the English rhotics). However, we excluded rhotics from this study. -->

[^cf-reconciliation]: The symbolic sonority hierarchy in NAP reconciles perceptual and articulatory approaches to sonority by modelling their mutual contribution to enhancing pitch intelligibility (or periodic energy mass in acoustic terms). This hierarchy is similar to a few proposals for sonority hierarchies that combined levels of voicing/periodicity with degree of vocal tract opening [e.g., @lass1988phonology; @miller2012sonority; and @sharma2018significance]. Such hierarchies may be also seen as compatible with source-filter models of speech [e.g., @fant2000source], where the source controls voicing and the filter controls opening.

[^cf-angle]: A slightly similar calculation can be found in Fullwood's [-@fullwood2014perceptual] *Sonority Angle*.
<!-- , although on different grounds, for different purposes, and with different outcomes. -->

<!-- [^cf-peak]: Note that peak measurements can be advantageous in that they tend to appear at the center of segmental units [minima in consonants and maxima in vowels; see @parker2008sound], overcoming some of the problems related to segmentation, given that peaks are less affected by determination of segmental boundaries. -->

[^cf-helsinki]: In accordance with German Research Foundation (DFG) guidelines for experiments with unimpaired adult populations, the ethics approval is required by the Principal Investigator (in this case, Prof. Dr. Martine Grice).

[^cf-plateaufall]: Depending on whether fricatives are considered higher or similar in sonority to stops, clusters of the type fricative-stop may be considered as either an onset fall or an onset plateau.

<!-- [^cf-source]: Based primarily on @van2012sonority. -->

[^cf-hebschwa]: The schwa in this case is produced as a weak (unstressed) /e/ vowel from the 5-vowel inventory of Modern Hebrew.

<!-- [^cf-indexfinger]: Participants used their left index finger to choose '1' and their right index finger to choose '2', using ‘F' and ‘J' buttons respectively. The respective buttons were covered with salient red-on-white ‘1' and ‘2' stickers (the rest of the laptop's keys are all white-on-black by design). -->

[^cf-pilot]: The exploratory study included also nine clusters with glides, which we later discarded from our confirmatory study. The actual number of data points per subject was therefore higher, with 76 data points per subject (adding nine targets and nine fillers that we later discarded from analysis).

[^cf-sstop]: The term */s/-stop cluster* is used here to refer to voiceless sequences of a sibilant, namely /s/ or /ʃ/, that precedes an oral stop, e.g., /p,t,k/.

[^cf-td]: Note that language-specific top-down biases, such as the question of syllabic consonants in a given language, are not covered by our top-down model, *NAP~td~*, which is limited to the universal aspects of NAP. That said, *NAP~td~* succeeded in fitting the data for /z/-initial and nasal-initial clusters within its predictions.


\newpage

# References
```{r create_r-references}
r_refs(file = "bibs/r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_a.Rmd') 
```

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('./appendix_b.Rmd')
```