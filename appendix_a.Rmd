<!-- ## Complete output of the Bayesian models  {-} -->
<!-- # Methods {#sec:methods} -->
\label{appendix:a}

## Stimulus considerations {-} 
<!-- {#sec:considerationlist} -->
The following list summarizes concerns that were taken into consideration when constructing the stimulus set (see the full set in Table \@ref(tab:targetlist)):

*	For the class of liquids we consider only the lateral /l/, disregarding the sub-class of rhotics that are phonetically varied and inconsistent across different languages. We therefore present no liquid plateau in the experimental set.

*	We use the alveolar /s/ for the class of voiceless sibilants (voiceless coronal fricatives). In *C*~1~ positions we also use the post-alveolar /ʃ/ to monitor language-specific effects that may appear due to restrictions in German, in which /ʃC/ onset clusters can be licit, while /sC/ onset clusters occur only marginally in loanwords.
<!-- should not be allowed. -->

*	We use stops only in *C*~2~ position. We avoid stops in *C*~1~ position since it is also the phrase-initial position of the stimuli, which is practically devoid of acoustic cues for the closure phase of the stop. Furthermore, we use only voiceless stops in order to keep the amount of stimuli reasonably low.

* We use one instance of the dorsal consonant /k/ instead of the coronal /t/ as an alternative to a labial-coronal cluster with a liquid in *C*~1~, thus retaining the same direction of a labial-coronal cluster (i.e., both labial-coronal and coronal-dorsal are front-to-back in terms of their places of articulation). We do not use other dorsals for fricative, nasal or liquid consonants, as these tend to be relatively more marked and more inconsistent between languages.

*	Lastly, we avoid sequences of obstruents that differ in voicing due to the cross-linguistic tendency of obstruent clusters to agree in voicing [see, e.g., @cho1990typology], although note that German allows /ʃv/ and /t͡sv/ clusters while banning /ʃf/.

## Audio recordings {-}
<!-- {#sec:audio} -->
Audio stimuli for the experiment were recorded by a phonetically trained native Hebrew speaker (the first author) in a sound attenuated booth at the phonetics laboratory of the University of Cologne. Speech was recorded via head-mounted headset condenser microphone (AKG C420), capturing mono digital files at a resolution of 44.1kHz sample-rate and 24 bit depth with a Metric Halo MIO 2882 audio interface. Selected audio takes were treated in the original high resolution for DC offset correction and compression of ultra low frequencies under 52Hz (to compensate for some room reverberation effects). Audio was then downgraded from 24 to 16 bit depth with Goodhertz Good Dither dithering to be used in the perception task running on OpenSesame 3.1.9 [@mathot2012opensesame]. The audio that was submitted to analyses by the APP Detector (see Subsection \@ref(sec:obtaining)) was also downgraded in sample-rate to 16kHz. Finally all audio takes, at all resolutions, were normalized to the same RMS target of -20dBFS.

\begin{exe}
\ex \textbf{/ze maʁˈgiʃ \emph{CCal} kaˈʁe.ga/} (ʼit feels (like) \emph{CCal} at the momentʼ) \label{ex:sentence}
\end{exe}

To record the stimuli, the combined 87 word-like tokens were embedded within carrier sentences, appearing in a non-final position with
default declarative intonation
in order to maintain consistent prosody. Carrier sentences were also designed to minimize potential effects of resyllabification as well as co-articulation by controlling the segmental makeup immediately preceding and following target words, see example in (\@ref(ex:sentence))
(the original sentence elicitation lists are available at the OSF repository in mixed Hebrew and phonemic transcripts in the form of slide presentations, see link in the opening of Section \@ref(sec:experiments)).

## Obtaining periodic energy data {-}
<!-- {#sec:obtaining} -->
We extract continuous measurements of periodic energy from acoustic signals using the *APP Detector*, a computer code that was introduced in @deshmukh2003detectionsk and developed in subsequent publications [@deshmukh2005use; @vishnubhotla2007detection], with the ability to measure the spectral distribution of periodic energy from digital audio files with a 16kHz sample-rate, effectively measuring periodic energy up to 8kHz [consider that our pitch perception range is limited to periods up to approximately 4kHz, reflecting a ceiling effect of the phase-locking of neural firing rates; @wever1937perception; @attneave1971pitch; @pressnitzer2001lower; @moore2013anintro 46]. We export the periodic energy data from the APP Detector's analysis into R [@R-base], for further data manipulation, visualization, modelling, and statistics.

We sum over the different frequencies that the APP Detector measures to obtain a time series  with the sum of periodic energy over different frequencies at 10 ms intervals.
We fit a smoothed curve to the periodic energy time series, to eliminate small-scale fluctuations in the curve's trajectory, and, finally, the sum of periodic energy is log transformed, where it is divided by a value that reflects the threshold of effective voicing periodicity, thus setting a meaningful zero for the periodic energy floor (we apply Tukey's Running Median Smoothing "3RS3R" to the periodic energy data. The threshold of the log transform is calculated as the maximal value obtained for voiceless portions in the set).
<!-- [^cf-smog] -->

The periodic energy curve is fitted with Local Polynomial Regression Fitting (*loess*) in the figures. This is only used for visual clarity---all the data points and calculations are based on the periodic energy curve before this smoothing and after initial smoothing and log transform (as detailed above).

## Perception task details {-}
<!-- {#sec:ptask} -->
Recall that the experiments were designed as a forced-choice 2-alternative perception task, where we collected accuracy and response time information.
We determined the zero time for valid response times at the the mid point of transition from *C*~2~ to /a/, individually for each one of the 87 stimuli (given that all stimuli share the rime *al*, which is fully predictable in the context of the experiment, in contrast to the preceding material, which is unpredictable). We used manual segmentation to determine this point for each target. We then excluded response times shorter than 100ms (i.e., 100ms after the mid point of transition from *C*~2~ to /a/) as too fast to be considered as valid. This led to no exclusions from Experiment 1 and to only one observation being excluded from Experiment 2.

Participants were seated in a quiet room in front of a laptop computer (a MacBook Air 13-inch, Early 2014) running the experiment on OpenSesame 3.1.9 [@mathot2012opensesame], where they listened to the stimuli through a set of closed headphones (Sennhieser HD 201), fed directly from the laptop's internal audio interface. After verifying that participants share a standard understanding of the notion of the syllable with a few German examples of words with one and two syllables (*See*, *Spaß*, *Quark*, and *Angst* vs. *Schu-le*, *Kin-der*, *Bre-zel*, *Pflau-men*), they were instructed to listen to nonce words in an "unknown" foreign language (the use of a speaker with a Hebrew accent in the stimuli that were presented to German listeners was intended to further support a "foreign" type of perception). Participants were instructed to respond quickly and accurately on whether they hear one or two syllables by clicking '1' or '2' on the computer keyboard (participants used their left index finger to choose '1' and their right index finger to choose '2').<!-- [^cf-indexfinger]  -->
A training session of ten trials preceded the experimental trials, allowing the participants to familiarize with the task, and allowing the experimenter to adjust listening volume and monitor potential problems and misunderstandings regarding the task.

## References {-}
